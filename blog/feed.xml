<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://www.pradeepgadde.com/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.pradeepgadde.com/blog/" rel="alternate" type="text/html" /><updated>2022-03-22T15:28:13+05:30</updated><id>https://www.pradeepgadde.com/blog/feed.xml</id><title type="html">Pradeep Gadde</title><subtitle>NetDevOps Engineer + Tech Educator</subtitle><entry><title type="html">Kubernetes DNS</title><link href="https://www.pradeepgadde.com/blog/kubernetes/2022/03/21/kubernetes-dns.html" rel="alternate" type="text/html" title="Kubernetes DNS" /><published>2022-03-21T10:55:04+05:30</published><updated>2022-03-21T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/kubernetes/2022/03/21/kubernetes-dns</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/kubernetes/2022/03/21/kubernetes-dns.html"><![CDATA[<h1 id="kubernetes-dns">Kubernetes DNS</h1>

<p>When we setup our minikube cluster, it is configured to use the CoreDNS addon or its precursor, kube-dns.</p>

<p>We can verify the same.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-n</span> kube-system
NAME                                       READY   STATUS    RESTARTS        AGE
calico-kube-controllers-8594699699-dztlm   1/1     Running   2 <span class="o">(</span>120m ago<span class="o">)</span>    47h
calico-node-gqvw6                          1/1     Running   2 <span class="o">(</span>119m ago<span class="o">)</span>    47h
calico-node-qdbcf                          1/1     Running   1 <span class="o">(</span>120m ago<span class="o">)</span>    47h
calico-node-sw74l                          1/1     Running   1 <span class="o">(</span>118m ago<span class="o">)</span>    47h
coredns-64897985d-58btq                    1/1     Running   2 <span class="o">(</span>120m ago<span class="o">)</span>    47h
etcd-minikube                              1/1     Running   1 <span class="o">(</span>120m ago<span class="o">)</span>    47h
kube-apiserver-minikube                    1/1     Running   3 <span class="o">(</span>120m ago<span class="o">)</span>    47h
kube-controller-manager-minikube           1/1     Running   3 <span class="o">(</span>120m ago<span class="o">)</span>    47h
kube-proxy-7k4lb                           1/1     Running   1 <span class="o">(</span>118m ago<span class="o">)</span>    47h
kube-proxy-gm2dh                           1/1     Running   1 <span class="o">(</span>119m ago<span class="o">)</span>    47h
kube-proxy-hvkqd                           1/1     Running   1 <span class="o">(</span>120m ago<span class="o">)</span>    47h
kube-scheduler-minikube                    1/1     Running   2 <span class="o">(</span>120m ago<span class="o">)</span>    47h
storage-provisioner                        1/1     Running   10 <span class="o">(</span>107m ago<span class="o">)</span>   47h
</code></pre></div></div>
<p>Describe the coredns pod, to get additional information.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pods coredns-64897985d-58btq <span class="nt">-n</span> kube-system
Name:                 coredns-64897985d-58btq
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 minikube/172.16.30.6
Start Time:           Sat, 19 Mar 2022 23:48:56 +0530
Labels:               k8s-app<span class="o">=</span>kube-dns
                      pod-template-hash<span class="o">=</span>64897985d
Annotations:          &lt;none&gt;
Status:               Running
IP:                   10.88.0.4
IPs:
  IP:           10.88.0.4
Controlled By:  ReplicaSet/coredns-64897985d
Containers:
  coredns:
    Container ID:  docker://c866d7b12f5f086407e04f3fd4e500889f860bfd55790e1e2a4870fad1052330
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      docker-pullable://k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      <span class="nt">-conf</span>
      /etc/coredns/Corefile
    State:          Running
      Started:      Mon, 21 Mar 2022 21:07:03 +0530
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Sun, 20 Mar 2022 19:25:02 +0530
      Finished:     Mon, 21 Mar 2022 21:06:40 +0530
    Ready:          True
    Restart Count:  2
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health <span class="nv">delay</span><span class="o">=</span>60s <span class="nb">timeout</span><span class="o">=</span>5s <span class="nv">period</span><span class="o">=</span>10s <span class="c">#success=1 #failure=5</span>
    Readiness:    http-get http://:8181/ready <span class="nv">delay</span><span class="o">=</span>0s <span class="nb">timeout</span><span class="o">=</span>1s <span class="nv">period</span><span class="o">=</span>10s <span class="c">#success=1 #failure=3</span>
    Environment:  &lt;none&gt;
    Mounts:
      /etc/coredns from config-volume <span class="o">(</span>ro<span class="o">)</span>
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rhcln <span class="o">(</span>ro<span class="o">)</span>
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  config-volume:
    Type:      ConfigMap <span class="o">(</span>a volume populated by a ConfigMap<span class="o">)</span>
    Name:      coredns
    Optional:  <span class="nb">false
  </span>kube-api-access-rhcln:
    Type:                    Projected <span class="o">(</span>a volume that contains injected data from multiple sources<span class="o">)</span>
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             <span class="nb">true
</span>QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os<span class="o">=</span>linux
Tolerations:                 CriticalAddonsOnly <span class="nv">op</span><span class="o">=</span>Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute <span class="nv">op</span><span class="o">=</span>Exists <span class="k">for </span>300s
                             node.kubernetes.io/unreachable:NoExecute <span class="nv">op</span><span class="o">=</span>Exists <span class="k">for </span>300s
Events:
  Type     Reason     Age                  From     Message
  <span class="nt">----</span>     <span class="nt">------</span>     <span class="nt">----</span>                 <span class="nt">----</span>     <span class="nt">-------</span>
  Warning  Unhealthy  107m                 kubelet  Liveness probe failed: Get <span class="s2">"http://10.88.0.4:8080/health"</span>: context deadline exceeded <span class="o">(</span>Client.Timeout exceeded <span class="k">while </span>awaiting headers<span class="o">)</span>
  Warning  Unhealthy  107m <span class="o">(</span>x2 over 107m<span class="o">)</span>  kubelet  Readiness probe failed: Get <span class="s2">"http://10.88.0.4:8181/ready"</span>: context deadline exceeded <span class="o">(</span>Client.Timeout exceeded <span class="k">while </span>awaiting headers<span class="o">)</span>
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<h2 id="create-a-simple-pod-to-use-as-a-test-environment">Create a simple Pod to use as a test environment</h2>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat dnsutils.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">dnsutils</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">dnsutils</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3</span>
    <span class="na">command</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">sleep</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">3600"</span>
    <span class="na">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
  <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">Always</span>

<span class="s">pradeep@learnk8s$</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> dnsutils.yaml
pod/dnsutils created
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Once that Pod is running, you can exec <code class="language-plaintext highlighter-rouge">nslookup</code> in that environment.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-i</span> <span class="nt">-t</span> dnsutils <span class="nt">--</span> nslookup kubernetes.default
<span class="p">;;</span> connection timed out<span class="p">;</span> no servers could be reached

<span class="nb">command </span>terminated with <span class="nb">exit </span>code 1
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>As the nslookup command failed, check the following:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          47h
web          NodePort    10.105.47.136   &lt;none&gt;        8080:31270/TCP   111m
web2         NodePort    10.106.73.151   &lt;none&gt;        8080:31691/TCP   102m
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">kubernetes</code> service is running the <code class="language-plaintext highlighter-rouge">default</code> namespace.
Check the <code class="language-plaintext highlighter-rouge">kube-dns</code> service in the <code class="language-plaintext highlighter-rouge">kube-system</code> namespace.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get svc <span class="nt">-n</span> kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>                  AGE
kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   47h
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>It is running fine. Let us check what <code class="language-plaintext highlighter-rouge">nameserver</code> is being used by this Pod.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-ti</span> dnsutils <span class="nt">--</span> <span class="nb">cat</span> /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
</code></pre></div></div>

<p>This confirms that the <code class="language-plaintext highlighter-rouge">dnsutils</code> pod is using the correct nameserver.</p>

<p>Are DNS endpoints exposed?
You can verify that DNS endpoints are exposed by using the <code class="language-plaintext highlighter-rouge">kubectl get endpoints</code> command.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get ep kube-dns <span class="nt">-n</span> kube-system
NAME       ENDPOINTS                                  AGE
kube-dns   10.88.0.4:53,10.88.0.4:53,10.88.0.4:9153   47h
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Hmm, the endpoints are coming from the Podman CNI but not from the Calico CNI that we are using in this cluster. You remember the race condition issue  between Calico and Podman CNIs?!</p>

<p>Let us get the replicaset ID for the coredns service and delete it.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get rs <span class="nt">-n</span> kube-system
NAME                                 DESIRED   CURRENT   READY   AGE
calico-kube-controllers-8594699699   1         1         1       47h
coredns-64897985d                    1         1         1       47h
</code></pre></div></div>
<p>Deleting the replicaset</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl delete rs coredns-64897985d <span class="nt">-n</span> kube-system
replicaset.apps <span class="s2">"coredns-64897985d"</span> deleted
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Immediately, the replicaset got re-created. Check the AGE!</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get rs <span class="nt">-n</span> kube-system
NAME                                 DESIRED   CURRENT   READY   AGE
calico-kube-controllers-8594699699   1         1         1       47h
coredns-64897985d                    1         1         0       2s
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Let us check the endpoints again.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get ep kube-dns <span class="nt">-n</span> kube-system
NAME       ENDPOINTS                                                 AGE
kube-dns   10.244.205.196:53,10.244.205.196:53,10.244.205.196:9153   47h
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Now that the endpoints are correctly pointing to the Calico CNI IP addresses, verify the nslookup again.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-ti</span> dnsutils <span class="nt">--</span> <span class="nb">cat</span> /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-i</span> <span class="nt">-t</span> dnsutils <span class="nt">--</span> nslookup kubernetes.default
Server:		10.96.0.10
Address:	10.96.0.10#53

Name:	kubernetes.default.svc.cluster.local
Address: 10.96.0.1

pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Just to verify the coredns status.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-n</span> kube-system
NAME                                       READY   STATUS    RESTARTS        AGE
calico-kube-controllers-8594699699-dztlm   1/1     Running   2 <span class="o">(</span>132m ago<span class="o">)</span>    47h
calico-node-gqvw6                          1/1     Running   2 <span class="o">(</span>131m ago<span class="o">)</span>    47h
calico-node-qdbcf                          1/1     Running   1 <span class="o">(</span>132m ago<span class="o">)</span>    47h
calico-node-sw74l                          1/1     Running   1 <span class="o">(</span>130m ago<span class="o">)</span>    47h
coredns-64897985d-kwp95                    1/1     Running   0               74s
etcd-minikube                              1/1     Running   1 <span class="o">(</span>132m ago<span class="o">)</span>    47h
kube-apiserver-minikube                    1/1     Running   3 <span class="o">(</span>132m ago<span class="o">)</span>    47h
kube-controller-manager-minikube           1/1     Running   3 <span class="o">(</span>132m ago<span class="o">)</span>    47h
kube-proxy-7k4lb                           1/1     Running   1 <span class="o">(</span>130m ago<span class="o">)</span>    47h
kube-proxy-gm2dh                           1/1     Running   1 <span class="o">(</span>131m ago<span class="o">)</span>    47h
kube-proxy-hvkqd                           1/1     Running   1 <span class="o">(</span>132m ago<span class="o">)</span>    47h
kube-scheduler-minikube                    1/1     Running   2 <span class="o">(</span>132m ago<span class="o">)</span>    47h
storage-provisioner                        1/1     Running   10 <span class="o">(</span>118m ago<span class="o">)</span>   47h
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Are DNS queries being received/processed?
You can verify if queries are being received by CoreDNS by adding the <code class="language-plaintext highlighter-rouge">log</code> plugin to the CoreDNS configuration (aka Corefile). The CoreDNS Corefile is held in a ConfigMap named <code class="language-plaintext highlighter-rouge">coredns</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get cm <span class="nt">-n</span> kube-system
NAME                                 DATA   AGE
calico-config                        4      47h
coredns                              1      47h
extension-apiserver-authentication   6      47h
kube-proxy                           2      47h
kube-root-ca.crt                     1      47h
kubeadm-config                       1      47h
kubelet-config-1.23                  1      47h
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Describe the config map.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe cm coredns <span class="nt">-n</span> kube-system
Name:         coredns
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
<span class="o">====</span>
Corefile:
<span class="nt">----</span>
.:53 <span class="o">{</span>
    errors
    health <span class="o">{</span>
       lameduck 5s
    <span class="o">}</span>
    ready
    kubernetes cluster.local <span class="k">in</span><span class="nt">-addr</span>.arpa ip6.arpa <span class="o">{</span>
       pods insecure
       fallthrough <span class="k">in</span><span class="nt">-addr</span>.arpa ip6.arpa
       ttl 30
    <span class="o">}</span>
    prometheus :9153
    hosts <span class="o">{</span>
       192.168.64.1 host.minikube.internal
       fallthrough
    <span class="o">}</span>
    forward <span class="nb">.</span> /etc/resolv.conf <span class="o">{</span>
       max_concurrent 1000
    <span class="o">}</span>
    cache 30
    loop
    reload
    loadbalance
<span class="o">}</span>


BinaryData
<span class="o">====</span>

Events:  &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Edit this configmap and add the <code class="language-plaintext highlighter-rouge">log</code> plugin.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl edit cm coredns <span class="nt">-n</span> kube-system
configmap/coredns edited
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Describe it again and verify the change</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe cm coredns <span class="nt">-n</span> kube-system
Name:         coredns
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
<span class="o">====</span>
Corefile:
<span class="nt">----</span>
.:53 <span class="o">{</span>
    log
    errors
    health <span class="o">{</span>
       lameduck 5s
    <span class="o">}</span>
    ready
    kubernetes cluster.local <span class="k">in</span><span class="nt">-addr</span>.arpa ip6.arpa <span class="o">{</span>
       pods insecure
       fallthrough <span class="k">in</span><span class="nt">-addr</span>.arpa ip6.arpa
       ttl 30
    <span class="o">}</span>
    prometheus :9153
    hosts <span class="o">{</span>
       192.168.64.1 host.minikube.internal
       fallthrough
    <span class="o">}</span>
    forward <span class="nb">.</span> /etc/resolv.conf <span class="o">{</span>
       max_concurrent 1000
    <span class="o">}</span>
    cache 30
    loop
    reload
    loadbalance
<span class="o">}</span>


BinaryData
<span class="o">====</span>

Events:  &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods.</p>

<p>Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system logs coredns-64897985d-kwp95
.:53
<span class="o">[</span>INFO] plugin/reload: Running configuration MD5 <span class="o">=</span> 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
<span class="o">[</span>INFO] Reloading
<span class="o">[</span>INFO] plugin/health: Going into lameduck mode <span class="k">for </span>5s
pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-i</span> <span class="nt">-t</span> dnsutils <span class="nt">--</span> nslookup kubernetes.default
Server:		10.96.0.10
Address:	10.96.0.10#53

Name:	kubernetes.default.svc.cluster.local
Address: 10.96.0.1

pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system logs coredns-64897985d-kwp95
.:53
<span class="o">[</span>INFO] plugin/reload: Running configuration MD5 <span class="o">=</span> 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
<span class="o">[</span>INFO] Reloading
<span class="o">[</span>INFO] plugin/health: Going into lameduck mode <span class="k">for </span>5s
<span class="o">[</span>INFO] plugin/reload: Running configuration MD5 <span class="o">=</span> 07d7c5ad4525bf2c472eaef020d0184d
<span class="o">[</span>INFO] Reloading <span class="nb">complete</span>
<span class="o">[</span>INFO] 127.0.0.1:39552 - 1628 <span class="s2">"HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 132 1.058936054s
<span class="o">[</span>INFO] 10.244.151.4:41947 - 42256 <span class="s2">"A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000469847s
<span class="o">[</span>INFO] 10.244.151.4:55247 - 43436 <span class="s2">"A IN kubernetes.default.svc.cluster.local. udp 54 false 512"</span> NOERROR qr,aa,rd 106 0.000357148s
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Let us make some more queries,</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-i</span> <span class="nt">-t</span> dnsutils <span class="nt">--</span> nslookup web.default
Server:		10.96.0.10
Address:	10.96.0.10#53

Name:	web.default.svc.cluster.local
Address: 10.105.47.136

pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-i</span> <span class="nt">-t</span> dnsutils <span class="nt">--</span> nslookup web2.default
Server:		10.96.0.10
Address:	10.96.0.10#53

Name:	web2.default.svc.cluster.local
Address: 10.106.73.151

pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-i</span> <span class="nt">-t</span> dnsutils <span class="nt">--</span> nslookup web3.default
Server:		10.96.0.10
Address:	10.96.0.10#53

<span class="k">**</span> server can<span class="s1">'t find web3.default: NXDOMAIN

command terminated with exit code 1
pradeep@learnk8s$
</span></code></pre></div></div>

<p>All the queries worked as expected. We did not have any service named <code class="language-plaintext highlighter-rouge">web3</code> in the <code class="language-plaintext highlighter-rouge">default</code> namespace, so it failed.</p>

<p>Let us review the coredns logs again.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system logs coredns-64897985d-kwp95
.:53
<span class="o">[</span>INFO] plugin/reload: Running configuration MD5 <span class="o">=</span> 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
<span class="o">[</span>INFO] Reloading
<span class="o">[</span>INFO] plugin/health: Going into lameduck mode <span class="k">for </span>5s
<span class="o">[</span>INFO] plugin/reload: Running configuration MD5 <span class="o">=</span> 07d7c5ad4525bf2c472eaef020d0184d
<span class="o">[</span>INFO] Reloading <span class="nb">complete</span>
<span class="o">[</span>INFO] 127.0.0.1:39552 - 1628 <span class="s2">"HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 132 1.058936054s
<span class="o">[</span>INFO] 10.244.151.4:41947 - 42256 <span class="s2">"A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000469847s
<span class="o">[</span>INFO] 10.244.151.4:55247 - 43436 <span class="s2">"A IN kubernetes.default.svc.cluster.local. udp 54 false 512"</span> NOERROR qr,aa,rd 106 0.000357148s
<span class="o">[</span>INFO] 10.244.151.4:46146 - 25028 <span class="s2">"A IN web.default.default.svc.cluster.local. udp 55 false 512"</span> NXDOMAIN qr,aa,rd 148 0.000302078s
<span class="o">[</span>INFO] 10.244.151.4:43513 - 15523 <span class="s2">"A IN web.default.svc.cluster.local. udp 47 false 512"</span> NOERROR qr,aa,rd 92 0.000412597s
<span class="o">[</span>INFO] 10.244.151.4:41650 - 22470 <span class="s2">"A IN web2.default.default.svc.cluster.local. udp 56 false 512"</span> NXDOMAIN qr,aa,rd 149 0.000398918s
<span class="o">[</span>INFO] 10.244.151.4:33214 - 54758 <span class="s2">"A IN web2.default.svc.cluster.local. udp 48 false 512"</span> NOERROR qr,aa,rd 94 0.00031088s
<span class="o">[</span>INFO] 10.244.151.4:34263 - 58079 <span class="s2">"A IN web3.default.default.svc.cluster.local. udp 56 false 512"</span> NXDOMAIN qr,aa,rd 149 0.000433005s
<span class="o">[</span>INFO] 10.244.151.4:52587 - 56008 <span class="s2">"A IN web3.default.svc.cluster.local. udp 48 false 512"</span> NXDOMAIN qr,aa,rd 141 0.000420513s
<span class="o">[</span>INFO] 10.244.151.4:49172 - 35844 <span class="s2">"A IN web3.default.cluster.local. udp 44 false 512"</span> NXDOMAIN qr,aa,rd 137 0.000302094s
<span class="o">[</span>INFO] 10.244.151.4:44833 - 3203 <span class="s2">"A IN web3.default. udp 30 false 512"</span> NXDOMAIN qr,rd,ra 105 0.18944251s
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Are you in the right namespace for the service?
DNS queries that donâ€™t specify a namespace are limited to the podâ€™s namespace.</p>

<p>If the namespace of the pod and service differ, the DNS query must include the namespace of the service.</p>

<p>This query is limited to the podâ€™s namespace:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-i</span> <span class="nt">-t</span> dnsutils <span class="nt">--</span> nslookup ingress-nginx-controller
Server:		10.96.0.10
Address:	10.96.0.10#53

<span class="k">**</span> server can<span class="s1">'t find ingress-nginx-controller: NXDOMAIN

command terminated with exit code 1
pradeep@learnk8s$
</span></code></pre></div></div>
<p>This failed because, <code class="language-plaintext highlighter-rouge">ingress-nginx-controller</code> svc is not present in the <code class="language-plaintext highlighter-rouge">default</code> namespace.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system logs coredns-64897985d-kwp95
.:53
<span class="o">[</span>INFO] plugin/reload: Running configuration MD5 <span class="o">=</span> 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
<span class="o">[</span>INFO] Reloading
<span class="o">[</span>INFO] plugin/health: Going into lameduck mode <span class="k">for </span>5s
<span class="o">[</span>INFO] plugin/reload: Running configuration MD5 <span class="o">=</span> 07d7c5ad4525bf2c472eaef020d0184d
<span class="o">[</span>INFO] Reloading <span class="nb">complete</span>
<span class="o">[</span>INFO] 127.0.0.1:39552 - 1628 <span class="s2">"HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512"</span> NXDOMAIN qr,rd,ra 132 1.058936054s
<span class="o">[</span>INFO] 10.244.151.4:41947 - 42256 <span class="s2">"A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512"</span> NXDOMAIN qr,aa,rd 155 0.000469847s
<span class="o">[</span>INFO] 10.244.151.4:55247 - 43436 <span class="s2">"A IN kubernetes.default.svc.cluster.local. udp 54 false 512"</span> NOERROR qr,aa,rd 106 0.000357148s
<span class="o">[</span>INFO] 10.244.151.4:46146 - 25028 <span class="s2">"A IN web.default.default.svc.cluster.local. udp 55 false 512"</span> NXDOMAIN qr,aa,rd 148 0.000302078s
<span class="o">[</span>INFO] 10.244.151.4:43513 - 15523 <span class="s2">"A IN web.default.svc.cluster.local. udp 47 false 512"</span> NOERROR qr,aa,rd 92 0.000412597s
<span class="o">[</span>INFO] 10.244.151.4:41650 - 22470 <span class="s2">"A IN web2.default.default.svc.cluster.local. udp 56 false 512"</span> NXDOMAIN qr,aa,rd 149 0.000398918s
<span class="o">[</span>INFO] 10.244.151.4:33214 - 54758 <span class="s2">"A IN web2.default.svc.cluster.local. udp 48 false 512"</span> NOERROR qr,aa,rd 94 0.00031088s
<span class="o">[</span>INFO] 10.244.151.4:34263 - 58079 <span class="s2">"A IN web3.default.default.svc.cluster.local. udp 56 false 512"</span> NXDOMAIN qr,aa,rd 149 0.000433005s
<span class="o">[</span>INFO] 10.244.151.4:52587 - 56008 <span class="s2">"A IN web3.default.svc.cluster.local. udp 48 false 512"</span> NXDOMAIN qr,aa,rd 141 0.000420513s
<span class="o">[</span>INFO] 10.244.151.4:49172 - 35844 <span class="s2">"A IN web3.default.cluster.local. udp 44 false 512"</span> NXDOMAIN qr,aa,rd 137 0.000302094s
<span class="o">[</span>INFO] 10.244.151.4:44833 - 3203 <span class="s2">"A IN web3.default. udp 30 false 512"</span> NXDOMAIN qr,rd,ra 105 0.18944251s
<span class="o">[</span>INFO] 10.244.151.4:50251 - 50181 <span class="s2">"A IN ingress-nginx-controller.default.svc.cluster.local. udp 68 false 512"</span> NXDOMAIN qr,aa,rd 161 0.000381672s
<span class="o">[</span>INFO] 10.244.151.4:52033 - 12007 <span class="s2">"A IN ingress-nginx-controller.svc.cluster.local. udp 60 false 512"</span> NXDOMAIN qr,aa,rd 153 0.000844354s
<span class="o">[</span>INFO] 10.244.151.4:50003 - 30556 <span class="s2">"A IN ingress-nginx-controller.cluster.local. udp 56 false 512"</span> NXDOMAIN qr,aa,rd 149 0.000453492s
<span class="o">[</span>INFO] 10.244.151.4:41810 - 19431 <span class="s2">"A IN ingress-nginx-controller. udp 42 false 512"</span> NXDOMAIN qr,rd,ra 117 0.057777871s
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>We can see that the query is done with the <code class="language-plaintext highlighter-rouge">default</code> namespace: <code class="language-plaintext highlighter-rouge">ingress-nginx-controller.default.svc.cluster.local.</code></p>

<p>Let us list all active services in all namespaces.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get svc <span class="nt">-A</span>
NAMESPACE       NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>                      AGE
default         kubernetes                           ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                      47h
default         web                                  NodePort    10.105.47.136    &lt;none&gt;        8080:31270/TCP               137m
default         web2                                 NodePort    10.106.73.151    &lt;none&gt;        8080:31691/TCP               127m
ingress-nginx   ingress-nginx-controller             NodePort    10.101.85.4      &lt;none&gt;        80:30419/TCP,443:30336/TCP   147m
ingress-nginx   ingress-nginx-controller-admission   ClusterIP   10.104.163.136   &lt;none&gt;        443/TCP                      147m
kube-system     kube-dns                             ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP       47h
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Now that we see <code class="language-plaintext highlighter-rouge">ingress-nginx-controller</code> in the <code class="language-plaintext highlighter-rouge">ingress-nginx</code> namespace, let us query for the correct name.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-i</span> <span class="nt">-t</span> dnsutils <span class="nt">--</span> nslookup ingress-nginx-controller.ingress-nginx
Server:		10.96.0.10
Address:	10.96.0.10#53

Name:	ingress-nginx-controller.ingress-nginx.svc.cluster.local
Address: 10.101.85.4
</code></pre></div></div>

<p>We can see the name resolution is successful this time.</p>]]></content><author><name>Kubernetes</name></author><category term="Kubernetes" /><category term="minikube" /><summary type="html"><![CDATA[Kubernetes DNS]]></summary></entry><entry><title type="html">Kubernetes Ingress Controller</title><link href="https://www.pradeepgadde.com/blog/kubernetes/2022/03/21/kubernetes-ingress.html" rel="alternate" type="text/html" title="Kubernetes Ingress Controller" /><published>2022-03-21T10:55:04+05:30</published><updated>2022-03-21T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/kubernetes/2022/03/21/kubernetes-ingress</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/kubernetes/2022/03/21/kubernetes-ingress.html"><![CDATA[<h1 id="kubernetes-ingress-controller">Kubernetes Ingress Controller</h1>

<h2 id="enable-ingress-addon">Enable Ingress Addon</h2>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube addons list
|-----------------------------|----------|--------------|--------------------------------|
|         ADDON NAME          | PROFILE  |    STATUS    |           MAINTAINER           |
|-----------------------------|----------|--------------|--------------------------------|
| ambassador                  | minikube | disabled     | third-party <span class="o">(</span>ambassador<span class="o">)</span>       |
| auto-pause                  | minikube | disabled     | google                         |
| csi-hostpath-driver         | minikube | disabled     | kubernetes                     |
| dashboard                   | minikube | disabled     | kubernetes                     |
| default-storageclass        | minikube | enabled âœ…   | kubernetes                     |
| efk                         | minikube | disabled     | third-party <span class="o">(</span>elastic<span class="o">)</span>          |
| freshpod                    | minikube | disabled     | google                         |
| gcp-auth                    | minikube | disabled     | google                         |
| gvisor                      | minikube | disabled     | google                         |
| helm-tiller                 | minikube | disabled     | third-party <span class="o">(</span>helm<span class="o">)</span>             |
| ingress                     | minikube | disabled     | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| ingress-dns                 | minikube | disabled     | google                         |
| istio                       | minikube | disabled     | third-party <span class="o">(</span>istio<span class="o">)</span>            |
| istio-provisioner           | minikube | disabled     | third-party <span class="o">(</span>istio<span class="o">)</span>            |
| kong                        | minikube | disabled     | third-party <span class="o">(</span>Kong HQ<span class="o">)</span>          |
| kubevirt                    | minikube | disabled     | third-party <span class="o">(</span>kubevirt<span class="o">)</span>         |
| logviewer                   | minikube | disabled     | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| metallb                     | minikube | disabled     | third-party <span class="o">(</span>metallb<span class="o">)</span>          |
| metrics-server              | minikube | disabled     | kubernetes                     |
| nvidia-driver-installer     | minikube | disabled     | google                         |
| nvidia-gpu-device-plugin    | minikube | disabled     | third-party <span class="o">(</span>nvidia<span class="o">)</span>           |
| olm                         | minikube | disabled     | third-party <span class="o">(</span>operator          |
|                             |          |              | framework<span class="o">)</span>                     |
| pod-security-policy         | minikube | disabled     | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| portainer                   | minikube | disabled     | portainer.io                   |
| registry                    | minikube | disabled     | google                         |
| registry-aliases            | minikube | disabled     | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| registry-creds              | minikube | disabled     | third-party <span class="o">(</span>upmc enterprises<span class="o">)</span> |
| storage-provisioner         | minikube | enabled âœ…   | google                         |
| storage-provisioner-gluster | minikube | disabled     | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| volumesnapshots             | minikube | disabled     | kubernetes                     |
|-----------------------------|----------|--------------|--------------------------------|
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Enable the <code class="language-plaintext highlighter-rouge">ingress</code> addon which is currently disabled in this environment.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube addons <span class="nb">enable </span>ingress
    â–ª Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
    â–ª Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
    â–ª Using image k8s.gcr.io/ingress-nginx/controller:v1.1.1
ðŸ”Ž  Verifying ingress addon...
ðŸŒŸ  The <span class="s1">'ingress'</span> addon is enabled
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Verify the addons list again, and confirm that the <code class="language-plaintext highlighter-rouge">ingress</code> is enabled.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube addons list
|-----------------------------|----------|--------------|--------------------------------|
|         ADDON NAME          | PROFILE  |    STATUS    |           MAINTAINER           |
|-----------------------------|----------|--------------|--------------------------------|
| ambassador                  | minikube | disabled     | third-party <span class="o">(</span>ambassador<span class="o">)</span>       |
| auto-pause                  | minikube | disabled     | google                         |
| csi-hostpath-driver         | minikube | disabled     | kubernetes                     |
| dashboard                   | minikube | disabled     | kubernetes                     |
| default-storageclass        | minikube | enabled âœ…   | kubernetes                     |
| efk                         | minikube | disabled     | third-party <span class="o">(</span>elastic<span class="o">)</span>          |
| freshpod                    | minikube | disabled     | google                         |
| gcp-auth                    | minikube | disabled     | google                         |
| gvisor                      | minikube | disabled     | google                         |
| helm-tiller                 | minikube | disabled     | third-party <span class="o">(</span>helm<span class="o">)</span>             |
| ingress                     | minikube | enabled âœ…   | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| ingress-dns                 | minikube | disabled     | google                         |
| istio                       | minikube | disabled     | third-party <span class="o">(</span>istio<span class="o">)</span>            |
| istio-provisioner           | minikube | disabled     | third-party <span class="o">(</span>istio<span class="o">)</span>            |
| kong                        | minikube | disabled     | third-party <span class="o">(</span>Kong HQ<span class="o">)</span>          |
| kubevirt                    | minikube | disabled     | third-party <span class="o">(</span>kubevirt<span class="o">)</span>         |
| logviewer                   | minikube | disabled     | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| metallb                     | minikube | disabled     | third-party <span class="o">(</span>metallb<span class="o">)</span>          |
| metrics-server              | minikube | disabled     | kubernetes                     |
| nvidia-driver-installer     | minikube | disabled     | google                         |
| nvidia-gpu-device-plugin    | minikube | disabled     | third-party <span class="o">(</span>nvidia<span class="o">)</span>           |
| olm                         | minikube | disabled     | third-party <span class="o">(</span>operator          |
|                             |          |              | framework<span class="o">)</span>                     |
| pod-security-policy         | minikube | disabled     | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| portainer                   | minikube | disabled     | portainer.io                   |
| registry                    | minikube | disabled     | google                         |
| registry-aliases            | minikube | disabled     | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| registry-creds              | minikube | disabled     | third-party <span class="o">(</span>upmc enterprises<span class="o">)</span> |
| storage-provisioner         | minikube | enabled âœ…   | google                         |
| storage-provisioner-gluster | minikube | disabled     | unknown <span class="o">(</span>third-party<span class="o">)</span>          |
| volumesnapshots             | minikube | disabled     | kubernetes                     |
|-----------------------------|----------|--------------|--------------------------------|
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Verify all resources that got created as part of this addon installation.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-n</span> ingress-nginx
NAME                                       READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-cvwv7       0/1     Completed   0          65s
ingress-nginx-admission-patch-zsrq4        0/1     Completed   1          65s
ingress-nginx-controller-cc8496874-6zr4h   1/1     Running     0          65s
pradeep@learnk8s<span class="nv">$ </span>kubectl get all <span class="nt">-n</span> ingress-nginx
NAME                                           READY   STATUS      RESTARTS   AGE
pod/ingress-nginx-admission-create-cvwv7       0/1     Completed   0          83s
pod/ingress-nginx-admission-patch-zsrq4        0/1     Completed   1          83s
pod/ingress-nginx-controller-cc8496874-6zr4h   1/1     Running     0          83s

NAME                                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>                      AGE
service/ingress-nginx-controller             NodePort    10.101.85.4      &lt;none&gt;        80:30419/TCP,443:30336/TCP   83s
service/ingress-nginx-controller-admission   ClusterIP   10.104.163.136   &lt;none&gt;        443/TCP                      83s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ingress-nginx-controller   1/1     1            1           83s

NAME                                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/ingress-nginx-controller-cc8496874   1         1         1       83s

NAME                                       COMPLETIONS   DURATION   AGE
job.batch/ingress-nginx-admission-create   1/1           10s        83s
job.batch/ingress-nginx-admission-patch    1/1           11s        83s
pradeep@learnk8s<span class="nv">$ </span>
</code></pre></div></div>
<h2 id="deploy-a-hello-world-app">Deploy a Hello, World App!</h2>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create deployment web <span class="nt">--image</span><span class="o">=</span>gcr.io/google-samples/hello-app:1.0
deployment.apps/web created
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Expose the deployment as  a NodePort Service on port number 8080.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl expose deployment web <span class="nt">--type</span><span class="o">=</span>NodePort -
<span class="nt">-port</span><span class="o">=</span>8080
service/web exposed
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Verify that the service is created properly.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get service web
NAME   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE
web    NodePort   10.105.47.136   &lt;none&gt;        8080:31270/TCP   11s
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Get the URL for this newly created service.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube service web <span class="nt">--url</span>
http://172.16.30.6:31270
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Access the URL</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>curl http://172.16.30.6:31270
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<h2 id="create-an-ingress">Create an Ingress</h2>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat example-ingress.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-ingress</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">nginx.ingress.kubernetes.io/rewrite-target</span><span class="pi">:</span> <span class="s">/$1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">hello-world.info</span>
      <span class="na">http</span><span class="pi">:</span>
        <span class="na">paths</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
            <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
            <span class="na">backend</span><span class="pi">:</span>
              <span class="na">service</span><span class="pi">:</span>
                <span class="na">name</span><span class="pi">:</span> <span class="s">web</span>
                <span class="na">port</span><span class="pi">:</span>
                  <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
<span class="s">pradeep@learnk8s$</span>
</code></pre></div></div>
<p>Create the Ingress using the above definition.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$kubectl</span> apply <span class="nt">-f</span> example-ingress.yaml
ingress.networking.k8s.io/example-ingress created
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Verify that the Ingress is created.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get ingress
NAME              CLASS   HOSTS              ADDRESS       PORTS   AGE
example-ingress   nginx   hello-world.info   172.16.30.6   80      87s
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Add the <code class="language-plaintext highlighter-rouge">172.16.30.6	hello-world.info</code> to the bottom of the /etc/hosts file on your computer (you will need administrator access):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span><span class="nb">cat</span> /etc/hosts
<span class="c">##</span>
<span class="c"># Host Database</span>
<span class="c">#</span>
<span class="c"># localhost is used to configure the loopback interface</span>
<span class="c"># when the system is booting.  Do not change this entry.</span>
<span class="c">##</span>
127.0.0.1	localhost
255.255.255.255	broadcasthost
::1             localhost
<span class="c"># Added by Docker Desktop</span>
<span class="c"># To allow the same kube context to work on the host and the container:</span>
127.0.0.1 kubernetes.docker.internal
<span class="c"># End of section</span>
172.16.30.6	hello-world.info
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>curl hello-world.info
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<h2 id="create-a-second-deployment">Create a second Deployment</h2>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create deployment web2 <span class="nt">--image</span><span class="o">=</span>gcr.io/google-samples/hello-app:2.0
deployment.apps/web2 created
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Expose the deployment.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl expose deployment web2 <span class="nt">--port</span><span class="o">=</span>8080 <span class="nt">--type</span><span class="o">=</span>NodePort
service/web2 exposed
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Edit the existing <code class="language-plaintext highlighter-rouge">example-ingress.yaml</code> manifest, and add the following lines at the end:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat example-ingress.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-ingress</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">nginx.ingress.kubernetes.io/rewrite-target</span><span class="pi">:</span> <span class="s">/$1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">hello-world.info</span>
      <span class="na">http</span><span class="pi">:</span>
        <span class="na">paths</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
            <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
            <span class="na">backend</span><span class="pi">:</span>
              <span class="na">service</span><span class="pi">:</span>
                <span class="na">name</span><span class="pi">:</span> <span class="s">web</span>
                <span class="na">port</span><span class="pi">:</span>
                  <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
          <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/v2</span>
            <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
            <span class="na">backend</span><span class="pi">:</span>
              <span class="na">service</span><span class="pi">:</span>
                <span class="na">name</span><span class="pi">:</span> <span class="s">web2</span>
                <span class="na">port</span><span class="pi">:</span>
                  <span class="na">number</span><span class="pi">:</span> <span class="m">8080</span>
<span class="s">pradeep@learnk8s$</span>
</code></pre></div></div>
<p>Apply the changes.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> example-ingress.yaml
ingress.networking.k8s.io/example-ingress configured
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<h2 id="test-your-ingress">Test your Ingress</h2>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>curl hello-world.info
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Access the 2nd version of the Hello World app.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>curl hello-world.info/v2
Hello, world!
Version: 2.0.0
Hostname: web2-5858b4c7c5-krt4l
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s$ kubectl get ingress
NAME              CLASS   HOSTS              ADDRESS       PORTS   AGE
example-ingress   nginx   hello-world.info   172.16.30.6   80      11m
pradeep@learnk8s$
</code></pre></div></div>
<p>Let us describe the ingress to see how it looks like.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe ingress
Name:             example-ingress
Namespace:        default
Address:          172.16.30.6
Default backend:  default-http-backend:80 <span class="o">(</span>&lt;error: endpoints <span class="s2">"default-http-backend"</span> not found&gt;<span class="o">)</span>
Rules:
  Host              Path  Backends
  <span class="nt">----</span>              <span class="nt">----</span>  <span class="nt">--------</span>
  hello-world.info
                    /     web:8080 <span class="o">(</span>10.244.205.195:8080<span class="o">)</span>
                    /v2   web2:8080 <span class="o">(</span>10.244.151.3:8080<span class="o">)</span>
Annotations:        nginx.ingress.kubernetes.io/rewrite-target: /<span class="nv">$1</span>
Events:
  Type    Reason  Age                From                      Message
  <span class="nt">----</span>    <span class="nt">------</span>  <span class="nt">----</span>               <span class="nt">----</span>                      <span class="nt">-------</span>
  Normal  Sync    12m <span class="o">(</span>x3 over 20m<span class="o">)</span>  nginx-ingress-controller  Scheduled <span class="k">for </span><span class="nb">sync
</span>pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>We can see one host (<code class="language-plaintext highlighter-rouge">hello-world.info</code>) and two paths (<code class="language-plaintext highlighter-rouge">/</code> and <code class="language-plaintext highlighter-rouge">/v2</code>), two backends (<code class="language-plaintext highlighter-rouge">web:8080</code> and <code class="language-plaintext highlighter-rouge">web2:8080</code>). These two services point to the respective Pods.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME                    READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
web-746c8679d4-nfkcw    1/1     Running   0          75m   10.244.205.195   minikube-m02   &lt;none&gt;           &lt;none&gt;
web2-5858b4c7c5-krt4l   1/1     Running   0          66m   10.244.151.3     minikube-m03   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>What happens to the requests sent to a path different from both of these (<code class="language-plaintext highlighter-rouge">/</code> and <code class="language-plaintext highlighter-rouge">/v2</code>)?
Those requests are handled by the default backend.</p>

<p>From the above description, we can see there is a default backend <code class="language-plaintext highlighter-rouge">default-http-backend:80</code> which is not defined yet, so there is an error.
Let us test few undefined paths like (<code class="language-plaintext highlighter-rouge">/v3</code>, <code class="language-plaintext highlighter-rouge">/v5</code> etc)</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>curl hello-world.info/v3
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
pradeep@learnk8s<span class="nv">$ </span>curl hello-world.info/v5
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
</code></pre></div></div>

<p>It looks like all these undefined paths are handled by the <code class="language-plaintext highlighter-rouge">web:8080</code> backend only. According to Kubernetes documentation, If <code class="language-plaintext highlighter-rouge">defaultBackend</code> is not set, the handling of requests that do not match any of the rules will be up to the ingress controller (consult the documentation for your ingress controller to find out how it handles this case).</p>

<p>If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.</p>

<p>Let us edit the ingress, to add a defaultBackend, in this case , change it to <code class="language-plaintext highlighter-rouge">web2</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl edit ingress example-ingress
ingress.networking.k8s.io/example-ingress edited
</code></pre></div></div>
<p>Describe it again, to see the changes.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe ingress
Name:             example-ingress
Namespace:        default
Address:          172.16.30.6
Default backend:  web2:8080 <span class="o">(</span>10.244.151.3:8080<span class="o">)</span>
Rules:
  Host              Path  Backends
  <span class="nt">----</span>              <span class="nt">----</span>  <span class="nt">--------</span>
  hello-world.info
                    /     web:8080 <span class="o">(</span>10.244.205.195:8080<span class="o">)</span>
                    /v2   web2:8080 <span class="o">(</span>10.244.151.3:8080<span class="o">)</span>
Annotations:        nginx.ingress.kubernetes.io/rewrite-target: /<span class="nv">$1</span>
Events:
  Type    Reason  Age                From                      Message
  <span class="nt">----</span>    <span class="nt">------</span>  <span class="nt">----</span>               <span class="nt">----</span>                      <span class="nt">-------</span>
  Normal  Sync    11s <span class="o">(</span>x4 over 78m<span class="o">)</span>  nginx-ingress-controller  Scheduled <span class="k">for </span><span class="nb">sync
</span>pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>This line <code class="language-plaintext highlighter-rouge">Default backend:  web2:8080 (10.244.151.3:8080)</code> confirms our change.
List out all the endpoints.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get ep
NAME         ENDPOINTS             AGE
kubernetes   172.16.30.6:8443      47h
web          10.244.205.195:8080   87m
web2         10.244.151.3:8080     78m
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Let us look at the ingress controller logs.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nt">-n</span> ingress-nginx logs ingress-nginx-controller-cc8496874-6zr4h
<span class="nt">-------------------------------------------------------------------------------</span>
NGINX Ingress controller
  Release:       v1.1.1
  Build:         a17181e43ec85534a6fea968d95d019c5a4bc8cf
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.19.9

<span class="nt">-------------------------------------------------------------------------------</span>

W0321 15:42:29.028539       9 client_config.go:615] Neither <span class="nt">--kubeconfig</span> nor <span class="nt">--master</span> was specified.  Using the inClusterConfig.  This might not work.
I0321 15:42:29.028983       9 main.go:223] <span class="s2">"Creating API client"</span> <span class="nv">host</span><span class="o">=</span><span class="s2">"https://10.96.0.1:443"</span>
I0321 15:42:29.057278       9 main.go:267] <span class="s2">"Running in Kubernetes cluster"</span> <span class="nv">major</span><span class="o">=</span><span class="s2">"1"</span> <span class="nv">minor</span><span class="o">=</span><span class="s2">"23"</span> <span class="nv">git</span><span class="o">=</span><span class="s2">"v1.23.3"</span> <span class="nv">state</span><span class="o">=</span><span class="s2">"clean"</span> <span class="nv">commit</span><span class="o">=</span><span class="s2">"816c97ab8cff8a1c72eccca1026f7820e93e0d25"</span> <span class="nv">platform</span><span class="o">=</span><span class="s2">"linux/amd64"</span>
I0321 15:42:29.317187       9 main.go:104] <span class="s2">"SSL fake certificate created"</span> <span class="nv">file</span><span class="o">=</span><span class="s2">"/etc/ingress-controller/ssl/default-fake-certificate.pem"</span>
I0321 15:42:29.374201       9 ssl.go:531] <span class="s2">"loading tls certificate"</span> <span class="nv">path</span><span class="o">=</span><span class="s2">"/usr/local/certificates/cert"</span> <span class="nv">key</span><span class="o">=</span><span class="s2">"/usr/local/certificates/key"</span>
I0321 15:42:29.419869       9 nginx.go:255] <span class="s2">"Starting NGINX Ingress controller"</span>
I0321 15:42:29.437178       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"ConfigMap"</span>, Namespace:<span class="s2">"ingress-nginx"</span>, Name:<span class="s2">"ingress-nginx-controller"</span>, UID:<span class="s2">"d14f8541-76a0-4357-b9ad-38c7ce8f6586"</span>, APIVersion:<span class="s2">"v1"</span>, ResourceVersion:<span class="s2">"12223"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'CREATE'</span> ConfigMap ingress-nginx/ingress-nginx-controller
I0321 15:42:29.442938       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"ConfigMap"</span>, Namespace:<span class="s2">"ingress-nginx"</span>, Name:<span class="s2">"tcp-services"</span>, UID:<span class="s2">"9cffa0f1-6754-4bfd-bf78-6cae4febc393"</span>, APIVersion:<span class="s2">"v1"</span>, ResourceVersion:<span class="s2">"12224"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'CREATE'</span> ConfigMap ingress-nginx/tcp-services
I0321 15:42:29.443183       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"ConfigMap"</span>, Namespace:<span class="s2">"ingress-nginx"</span>, Name:<span class="s2">"udp-services"</span>, UID:<span class="s2">"3ac9b32a-17b9-4824-bc0b-51a609648e9c"</span>, APIVersion:<span class="s2">"v1"</span>, ResourceVersion:<span class="s2">"12225"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'CREATE'</span> ConfigMap ingress-nginx/udp-services
I0321 15:42:30.622870       9 nginx.go:297] <span class="s2">"Starting NGINX process"</span>
I0321 15:42:30.623750       9 leaderelection.go:248] attempting to acquire leader lease ingress-nginx/ingress-controller-leader...
I0321 15:42:30.624864       9 controller.go:155] <span class="s2">"Configuration changes detected, backend reload required"</span>
I0321 15:42:30.623967       9 nginx.go:317] <span class="s2">"Starting validation webhook"</span> <span class="nv">address</span><span class="o">=</span><span class="s2">":8443"</span> <span class="nv">certPath</span><span class="o">=</span><span class="s2">"/usr/local/certificates/cert"</span> <span class="nv">keyPath</span><span class="o">=</span><span class="s2">"/usr/local/certificates/key"</span>
I0321 15:42:30.666454       9 leaderelection.go:258] successfully acquired lease ingress-nginx/ingress-controller-leader
I0321 15:42:30.667675       9 status.go:84] <span class="s2">"New leader elected"</span> <span class="nv">identity</span><span class="o">=</span><span class="s2">"ingress-nginx-controller-cc8496874-6zr4h"</span>
I0321 15:42:30.700891       9 status.go:214] <span class="s2">"POD is not ready"</span> <span class="nv">pod</span><span class="o">=</span><span class="s2">"ingress-nginx/ingress-nginx-controller-cc8496874-6zr4h"</span> <span class="nv">node</span><span class="o">=</span><span class="s2">"minikube"</span>
I0321 15:42:30.884747       9 controller.go:172] <span class="s2">"Backend successfully reloaded"</span>
I0321 15:42:30.885032       9 controller.go:183] <span class="s2">"Initial sync, sleeping for 1 second"</span>
I0321 15:42:30.885722       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"Pod"</span>, Namespace:<span class="s2">"ingress-nginx"</span>, Name:<span class="s2">"ingress-nginx-controller-cc8496874-6zr4h"</span>, UID:<span class="s2">"de6796ef-94a6-499c-b9b5-418c39cc62c0"</span>, APIVersion:<span class="s2">"v1"</span>, ResourceVersion:<span class="s2">"12337"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'RELOAD'</span> NGINX reload triggered due to a change <span class="k">in </span>configuration
I0321 15:56:40.753346       9 admission.go:149] processed ingress via admission controller <span class="o">{</span>testedIngressLength:1 testedIngressTime:0.412s renderingIngressLength:1 renderingIngressTime:0.009s admissionTime:18.0kBs testedConfigurationSize:0.421<span class="o">}</span>
I0321 15:56:40.755944       9 main.go:101] <span class="s2">"successfully validated configuration, accepting"</span> <span class="nv">ingress</span><span class="o">=</span><span class="s2">"default/example-ingress"</span>
I0321 15:56:40.794396       9 store.go:424] <span class="s2">"Found valid IngressClass"</span> <span class="nv">ingress</span><span class="o">=</span><span class="s2">"default/example-ingress"</span> <span class="nv">ingressclass</span><span class="o">=</span><span class="s2">"nginx"</span>
I0321 15:56:40.796827       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"Ingress"</span>, Namespace:<span class="s2">"default"</span>, Name:<span class="s2">"example-ingress"</span>, UID:<span class="s2">"51471ff6-1982-47da-85ee-a9356165df9f"</span>, APIVersion:<span class="s2">"networking.k8s.io/v1"</span>, ResourceVersion:<span class="s2">"13112"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'Sync'</span> Scheduled <span class="k">for </span><span class="nb">sync
</span>I0321 15:56:40.802072       9 controller.go:155] <span class="s2">"Configuration changes detected, backend reload required"</span>
I0321 15:56:40.979390       9 controller.go:172] <span class="s2">"Backend successfully reloaded"</span>
I0321 15:56:40.988286       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"Pod"</span>, Namespace:<span class="s2">"ingress-nginx"</span>, Name:<span class="s2">"ingress-nginx-controller-cc8496874-6zr4h"</span>, UID:<span class="s2">"de6796ef-94a6-499c-b9b5-418c39cc62c0"</span>, APIVersion:<span class="s2">"v1"</span>, ResourceVersion:<span class="s2">"12337"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'RELOAD'</span> NGINX reload triggered due to a change <span class="k">in </span>configuration
I0321 15:57:28.297662       9 status.go:299] <span class="s2">"updating Ingress status"</span> <span class="nv">namespace</span><span class="o">=</span><span class="s2">"default"</span> <span class="nv">ingress</span><span class="o">=</span><span class="s2">"example-ingress"</span> <span class="nv">currentValue</span><span class="o">=[]</span> <span class="nv">newValue</span><span class="o">=[{</span>IP:172.16.30.6 Hostname: Ports:[]<span class="o">}]</span>
I0321 15:57:28.322904       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"Ingress"</span>, Namespace:<span class="s2">"default"</span>, Name:<span class="s2">"example-ingress"</span>, UID:<span class="s2">"51471ff6-1982-47da-85ee-a9356165df9f"</span>, APIVersion:<span class="s2">"networking.k8s.io/v1"</span>, ResourceVersion:<span class="s2">"13164"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'Sync'</span> Scheduled <span class="k">for </span><span class="nb">sync
</span>172.16.30.1 - - <span class="o">[</span>21/Mar/2022:16:00:09 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 80 0.009 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.009 200 766ea45be48fc8dcb53c425eb3800ab2
I0321 16:04:49.720933       9 admission.go:149] processed ingress via admission controller <span class="o">{</span>testedIngressLength:1 testedIngressTime:0.161s renderingIngressLength:1 renderingIngressTime:0s admissionTime:21.7kBs testedConfigurationSize:0.161<span class="o">}</span>
I0321 16:04:49.721038       9 main.go:101] <span class="s2">"successfully validated configuration, accepting"</span> <span class="nv">ingress</span><span class="o">=</span><span class="s2">"default/example-ingress"</span>
I0321 16:04:49.733433       9 controller.go:155] <span class="s2">"Configuration changes detected, backend reload required"</span>
I0321 16:04:49.734529       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"Ingress"</span>, Namespace:<span class="s2">"default"</span>, Name:<span class="s2">"example-ingress"</span>, UID:<span class="s2">"51471ff6-1982-47da-85ee-a9356165df9f"</span>, APIVersion:<span class="s2">"networking.k8s.io/v1"</span>, ResourceVersion:<span class="s2">"13649"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'Sync'</span> Scheduled <span class="k">for </span><span class="nb">sync
</span>I0321 16:04:49.911585       9 controller.go:172] <span class="s2">"Backend successfully reloaded"</span>
I0321 16:04:49.914092       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"Pod"</span>, Namespace:<span class="s2">"ingress-nginx"</span>, Name:<span class="s2">"ingress-nginx-controller-cc8496874-6zr4h"</span>, UID:<span class="s2">"de6796ef-94a6-499c-b9b5-418c39cc62c0"</span>, APIVersion:<span class="s2">"v1"</span>, ResourceVersion:<span class="s2">"12337"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'RELOAD'</span> NGINX reload triggered due to a change <span class="k">in </span>configuration
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:16:05:30 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 80 0.004 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.004 200 c3d48047b7ef7ac70326098b25f1e6c1
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:16:05:33 +0000] <span class="s2">"GET /v2 HTTP/1.1"</span> 200 61 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.013 <span class="o">[</span>default-web2-8080] <span class="o">[]</span> 10.244.151.3:8080 61 0.013 200 e90cf46a8385a64ac01c834734768488
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:16:32:44 +0000] <span class="s2">"GET /v2 HTTP/1.1"</span> 200 61 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.114 <span class="o">[</span>default-web2-8080] <span class="o">[]</span> 10.244.151.3:8080 61 0.114 200 cb87a3a8abd12099f7db64a2ec83c681
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:03:17 +0000] <span class="s2">"GET /v3 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.040 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.041 200 f794e2a783715ac73ff7177f60e9dd68
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:03:21 +0000] <span class="s2">"GET /v5 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.016 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.017 200 095cbb53ca04bdb4359fa7b745e5a809
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:03:29 +0000] <span class="s2">"GET /v2 HTTP/1.1"</span> 200 61 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web2-8080] <span class="o">[]</span> 10.244.151.3:8080 61 0.002 200 cbc0c654a89c5f98f18a5102ddf61b0e
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:03:33 +0000] <span class="s2">"GET /1 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 81 0.008 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.008 200 767955b2b4eed7791912b2a843363b28
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:05:26 +0000] <span class="s2">"GET /1 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 81 0.004 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.005 200 0ec584df780661bccfa10731bcc70524
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:05:29 +0000] <span class="s2">"GET / HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 80 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 5ecb91527d77741daf16c17a8d79a462
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:05:31 +0000] <span class="s2">"GET /as HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.006 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.005 200 034643529f3104fef19915593168b200
I0321 17:14:53.079003       9 admission.go:149] processed ingress via admission controller <span class="o">{</span>testedIngressLength:1 testedIngressTime:0.41s renderingIngressLength:1 renderingIngressTime:0.007s admissionTime:21.7kBs testedConfigurationSize:0.417<span class="o">}</span>
I0321 17:14:53.079746       9 main.go:101] <span class="s2">"successfully validated configuration, accepting"</span> <span class="nv">ingress</span><span class="o">=</span><span class="s2">"default/example-ingress"</span>
I0321 17:14:53.099157       9 controller.go:155] <span class="s2">"Configuration changes detected, backend reload required"</span>
I0321 17:14:53.099689       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"Ingress"</span>, Namespace:<span class="s2">"default"</span>, Name:<span class="s2">"example-ingress"</span>, UID:<span class="s2">"51471ff6-1982-47da-85ee-a9356165df9f"</span>, APIVersion:<span class="s2">"networking.k8s.io/v1"</span>, ResourceVersion:<span class="s2">"15050"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'Sync'</span> Scheduled <span class="k">for </span><span class="nb">sync
</span>I0321 17:14:53.225350       9 controller.go:172] <span class="s2">"Backend successfully reloaded"</span>
I0321 17:14:53.227604       9 event.go:282] Event<span class="o">(</span>v1.ObjectReference<span class="o">{</span>Kind:<span class="s2">"Pod"</span>, Namespace:<span class="s2">"ingress-nginx"</span>, Name:<span class="s2">"ingress-nginx-controller-cc8496874-6zr4h"</span>, UID:<span class="s2">"de6796ef-94a6-499c-b9b5-418c39cc62c0"</span>, APIVersion:<span class="s2">"v1"</span>, ResourceVersion:<span class="s2">"12337"</span>, FieldPath:<span class="s2">""</span><span class="o">})</span>: <span class="nb">type</span>: <span class="s1">'Normal'</span> reason: <span class="s1">'RELOAD'</span> NGINX reload triggered due to a change <span class="k">in </span>configuration
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:16:53 +0000] <span class="s2">"GET /v5 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.009 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.009 200 2e8ee25ad012ea3d5ac6fb4646f37d61
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:16:57 +0000] <span class="s2">"GET /v3 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.004 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.005 200 70d9d27891c2b97a6bcda5e315fa3691
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:17:01 +0000] <span class="s2">"GET /v2 HTTP/1.1"</span> 200 61 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.010 <span class="o">[</span>default-web2-8080] <span class="o">[]</span> 10.244.151.3:8080 61 0.009 200 297effa7825bef53d93e6dce4e5d9d62
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:17:02 +0000] <span class="s2">"GET /v1 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.003 200 421c9cb655e07183d9a7d1a89562db97
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:17:29 +0000] <span class="s2">"GET /v1 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 3bff305b8c1b782d08acf2de386076d9
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:17:31 +0000] <span class="s2">"GET /v3 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.001 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 e3dca522b7470345325f5dcda8adfc8e
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:17:34 +0000] <span class="s2">"GET /v5 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.003 200 207b1c9f4b52683061bb0807857c32f6
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:17:36 +0000] <span class="s2">"GET /v6 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 f2d0ae4264c26168246099ee90cc9f27
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:17:39 +0000] <span class="s2">"GET /vdfs HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 84 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.001 200 b08773ef6c074b4b10fee5626457df47
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:17:40 +0000] <span class="s2">"GET /vdfs HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 84 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 cd5a39f9d32b212bf97b41c57f7b429b
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:18:58 +0000] <span class="s2">"GET /vdfs HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 84 0.009 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.008 200 f413dd82a31c72df36114843c5054aad
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:18:59 +0000] <span class="s2">"GET /vdfs HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 84 0.003 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.004 200 09a92db2773acffcd23b31c856673a9b
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:19:12 +0000] <span class="s2">"GET /v1 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.001 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 c21e9af32944af9d9d70b39eaedbe11c
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:19:14 +0000] <span class="s2">"GET /v HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 81 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 5daf84fe1f8992ff8d9d0cdec4b751c1
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:19:17 +0000] <span class="s2">"GET /v2 HTTP/1.1"</span> 200 61 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.007 <span class="o">[</span>default-web2-8080] <span class="o">[]</span> 10.244.151.3:8080 61 0.008 200 52df2d3780d65d11e34a545583958a17
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:20:32 +0000] <span class="s2">"GET /v2 HTTP/1.1"</span> 200 61 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.003 <span class="o">[</span>default-web2-8080] <span class="o">[]</span> 10.244.151.3:8080 61 0.003 200 d05940e016c2b36ce79996c04065b5a8
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:20:34 +0000] <span class="s2">"GET /v2f HTTP/1.1"</span> 200 61 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 83 0.003 <span class="o">[</span>default-web2-8080] <span class="o">[]</span> 10.244.151.3:8080 61 0.002 200 c59de52e577935709573bf0515e999e2
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:20:36 +0000] <span class="s2">"GET /v3 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.003 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 3354dc569894182a6ee5a5f69892ca66
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:20:40 +0000] <span class="s2">"GET /v35 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 83 0.004 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.004 200 75cccfe798f3a6f2ee072c9233c8e3b7
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:20:44 +0000] <span class="s2">"GET /v2a HTTP/1.1"</span> 200 61 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 83 0.004 <span class="o">[</span>default-web2-8080] <span class="o">[]</span> 10.244.151.3:8080 61 0.004 200 8ae6170f4a95446d0615bfc5d4a16403
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:20:48 +0000] <span class="s2">"GET /dg HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.003 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.003 200 c4cea1304f930170d7a7148fc2ac04a7
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:21:49 +0000] <span class="s2">"GET /dg HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.004 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.003 200 b860ca473d1b622514dd4ebf194311f2
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:21:50 +0000] <span class="s2">"GET /dg HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.003 200 1f8bf150a942fb5272d01d9a82c6fd77
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:21:51 +0000] <span class="s2">"GET /dg HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.003 200 2a2c3eec3ebe85dd546ea35cf92f1357
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:21:52 +0000] <span class="s2">"GET /dg HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.005 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.005 200 fca07770b107dfb25126f60a1f4dabcc
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:21:52 +0000] <span class="s2">"GET /dg HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.003 200 421ddaffd339aafd3ac912c195035b7e
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:22:41 +0000] <span class="s2">"GET /dg HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 58a2dc6300d892b640bd26ab007daba8
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:22:42 +0000] <span class="s2">"GET /dg HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.003 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 402a0a5fd8037182ea8671d3d9347ee7
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:22:44 +0000] <span class="s2">"GET /df HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.003 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.004 200 4cb6f78cb734373510f98f669cdf253d
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:22:46 +0000] <span class="s2">"GET /dfdg HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 84 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.001 200 2fd8c843e0fac657bc80e2a6b23dcaf6
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:23:52 +0000] <span class="s2">"GET /v7 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.003 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 5aaabd3963132e95499b5be8b57368e6
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:23:54 +0000] <span class="s2">"GET /v9 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.002 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.002 200 452ce52c0ab19f2fdc37152f925887f7
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:24:28 +0000] <span class="s2">"GET /v9 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.003 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.004 200 faf0e7f8be1b66b38f6747c7df815525
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:24:30 +0000] <span class="s2">"GET /v9 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.004 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.003 200 c4526f5d5a9fab7245b1d77becf24bb7
172.16.30.1 - - <span class="o">[</span>21/Mar/2022:17:24:31 +0000] <span class="s2">"GET /v9 HTTP/1.1"</span> 200 60 <span class="s2">"-"</span> <span class="s2">"curl/7.77.0"</span> 82 0.003 <span class="o">[</span>default-web-8080] <span class="o">[]</span> 10.244.205.195:8080 60 0.003 200 47fd2dd3ed81e5fa3762687343ed06c0
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>]]></content><author><name>Kubernetes</name></author><category term="Kubernetes" /><category term="minikube" /><summary type="html"><![CDATA[Kubernetes Ingress Controller]]></summary></entry><entry><title type="html">Kubernetes Networking with Calico CNI</title><link href="https://www.pradeepgadde.com/blog/kubernetes/2022/03/19/kubernetes-networking-calico.html" rel="alternate" type="text/html" title="Kubernetes Networking with Calico CNI" /><published>2022-03-19T10:55:04+05:30</published><updated>2022-03-19T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/kubernetes/2022/03/19/kubernetes-networking-calico</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/kubernetes/2022/03/19/kubernetes-networking-calico.html"><![CDATA[<p>Kubernetes Networking with Calico CNI</p>

<p>Let us build another minikube cluster, this time with the Calico CNI. Minikube supports many CNIs.</p>

<p>From the <code class="language-plaintext highlighter-rouge">minikube start -h</code> output, we can see the supported CNI plug-ins list.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nt">--cni</span><span class="o">=</span><span class="s1">''</span>: CNI plug-in to use. Valid options: auto, bridge, calico, cilium, flannel, kindnet, or path to a CNI
manifest <span class="o">(</span>default: auto<span class="o">)</span>
</code></pre></div></div>
<p>Start the 3-node minikube cluster with <code class="language-plaintext highlighter-rouge">calico</code> CNI plugin.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube start <span class="nt">--nodes</span><span class="o">=</span>3 <span class="nt">--cni</span><span class="o">=</span>calico
ðŸ˜„  minikube v1.25.2 on Darwin 12.2.1
âœ¨  Automatically selected the hyperkit driver
ðŸ‘  Starting control plane node minikube <span class="k">in </span>cluster minikube
ðŸ”¥  Creating hyperkit VM <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB, <span class="nv">Disk</span><span class="o">=</span>20000MB<span class="o">)</span> ...
ðŸ³  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    â–ª kubelet.housekeeping-interval<span class="o">=</span>5m
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ðŸ”—  Configuring Calico <span class="o">(</span>Container Networking Interface<span class="o">)</span> ...
ðŸ”Ž  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ðŸŒŸ  Enabled addons: default-storageclass, storage-provisioner

ðŸ‘  Starting worker node minikube-m02 <span class="k">in </span>cluster minikube
ðŸ”¥  Creating hyperkit VM <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB, <span class="nv">Disk</span><span class="o">=</span>20000MB<span class="o">)</span> ...
ðŸŒ  Found network options:
    â–ª <span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.6
ðŸ³  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    â–ª <span class="nb">env </span><span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.6
ðŸ”Ž  Verifying Kubernetes components...

ðŸ‘  Starting worker node minikube-m03 <span class="k">in </span>cluster minikube
ðŸ”¥  Creating hyperkit VM <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB, <span class="nv">Disk</span><span class="o">=</span>20000MB<span class="o">)</span> ...
ðŸŒ  Found network options:
    â–ª <span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.6,172.16.30.7
ðŸ³  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    â–ª <span class="nb">env </span><span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.6
    â–ª <span class="nb">env </span><span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.6,172.16.30.7
ðŸ”Ž  Verifying Kubernetes components...
ðŸ„  Done! kubectl is now configured to use <span class="s2">"minikube"</span> cluster and <span class="s2">"default"</span> namespace by default
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Verify the cluster node IPs: <code class="language-plaintext highlighter-rouge">172.16.30.6</code>, <code class="language-plaintext highlighter-rouge">172.16.30.7</code>, and <code class="language-plaintext highlighter-rouge">172.16.30.8</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide
NAME           STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube       Ready    control-plane,master   22m   v1.23.3   172.16.30.6   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
minikube-m02   Ready    &lt;none&gt;                 20m   v1.23.3   172.16.30.7   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
minikube-m03   Ready    &lt;none&gt;                 17m   v1.23.3   172.16.30.8   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Verify the list of Pods in all namespaces.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-A</span> <span class="nt">-o</span> wide
NAMESPACE     NAME                                       READY   STATUS    RESTARTS        AGE     IP            NODE           NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-8594699699-dztlm   1/1     Running   0               6m50s   10.88.0.3     minikube       &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-gqvw6                          1/1     Running   1 <span class="o">(</span>2m54s ago<span class="o">)</span>   4m38s   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-qdbcf                          1/1     Running   0               6m50s   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-sw74l                          1/1     Running   0               114s    172.16.30.8   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-64897985d-58btq                    1/1     Running   0               6m50s   10.88.0.2     minikube       &lt;none&gt;           &lt;none&gt;
kube-system   etcd-minikube                              1/1     Running   0               7m1s    172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-minikube                    1/1     Running   0               7m1s    172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-minikube           1/1     Running   0               7m1s    172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-7k4lb                           1/1     Running   0               114s    172.16.30.8   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-gm2dh                           1/1     Running   0               4m38s   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-hvkqd                           1/1     Running   0               6m50s   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-minikube                    1/1     Running   0               7m1s    172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   storage-provisioner                        1/1     Running   1 <span class="o">(</span>2m26s ago<span class="o">)</span>   6m47s   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>There are two Pods using the 10.88.0.X subnet (<code class="language-plaintext highlighter-rouge">calico-kube-controllers</code> and <code class="language-plaintext highlighter-rouge">coredns</code> Pods ).</p>

<p>Login to the <code class="language-plaintext highlighter-rouge">controlplane</code> node and check the CNI directory.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ cat /etc/cni/net.d/
.keep                      10-calico.conflist         87-podman-bridge.conflist  calico-kubeconfig
$ cat /etc/cni/net.d/10-calico.conflist
{
  "name": "k8s-pod-network",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "calico",
      "log_level": "info",
      "log_file_path": "/var/log/calico/cni/cni.log",
      "datastore_type": "kubernetes",
      "nodename": "minikube",
      "mtu": 0,
      "ipam": {
          "type": "calico-ipam"
      },
      "policy": {
          "type": "k8s"
      },
      "kubernetes": {
          "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
      }
    },
    {
      "type": "portmap",
      "snat": true,
      "capabilities": {"portMappings": true}
    },
    {
      "type": "bandwidth",
      "capabilities": {"bandwidth": true}
    }
  ]
}$
</span></code></pre></div></div>
<p>Apart from the Calico , there is a config file for Podman as well. Let us check that as well.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /etc/cni/net.d/87-podman-bridge.conflist
<span class="o">{</span>
  <span class="s2">"cniVersion"</span>: <span class="s2">"0.4.0"</span>,
  <span class="s2">"name"</span>: <span class="s2">"podman"</span>,
  <span class="s2">"plugins"</span>: <span class="o">[</span>
    <span class="o">{</span>
      <span class="s2">"type"</span>: <span class="s2">"bridge"</span>,
      <span class="s2">"bridge"</span>: <span class="s2">"cni-podman0"</span>,
      <span class="s2">"isGateway"</span>: <span class="nb">true</span>,
      <span class="s2">"ipMasq"</span>: <span class="nb">true</span>,
      <span class="s2">"hairpinMode"</span>: <span class="nb">true</span>,
      <span class="s2">"ipam"</span>: <span class="o">{</span>
        <span class="s2">"type"</span>: <span class="s2">"host-local"</span>,
        <span class="s2">"routes"</span>: <span class="o">[{</span> <span class="s2">"dst"</span>: <span class="s2">"0.0.0.0/0"</span> <span class="o">}]</span>,
        <span class="s2">"ranges"</span>: <span class="o">[</span>
          <span class="o">[</span>
            <span class="o">{</span>
              <span class="s2">"subnet"</span>: <span class="s2">"10.88.0.0/16"</span>,
              <span class="s2">"gateway"</span>: <span class="s2">"10.88.0.1"</span>
            <span class="o">}</span>
          <span class="o">]</span>
        <span class="o">]</span>
      <span class="o">}</span>
    <span class="o">}</span>,
    <span class="o">{</span>
      <span class="s2">"type"</span>: <span class="s2">"portmap"</span>,
      <span class="s2">"capabilities"</span>: <span class="o">{</span>
        <span class="s2">"portMappings"</span>: <span class="nb">true</span>
      <span class="o">}</span>
    <span class="o">}</span>,
    <span class="o">{</span>
      <span class="s2">"type"</span>: <span class="s2">"firewall"</span>
    <span class="o">}</span>,
    <span class="o">{</span>
      <span class="s2">"type"</span>: <span class="s2">"tuning"</span>
    <span class="o">}</span>
  <span class="o">]</span>
<span class="o">}</span>
<span class="err">$</span>
</code></pre></div></div>

<p>From this, it is clear that while starting the minikube cluster, those two Pods got the IP from the Podman CNI, which seems to compete with the Calico CNI.</p>

<p>This issue is reported here: https://github.com/kubernetes/kubernetes/issues/107687</p>

<p>Let us create a new Pod in the default namespace and check which IP will be obtained.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl run nginx <span class="nt">--image</span><span class="o">=</span>nginx
pod/nginx created
</code></pre></div></div>
<p>Verify the IP of the new Pod and the node on which it got hosted.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME    READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          56s   10.244.205.193   minikube-m02   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>

<p>The new nginx pod got the IP address of <code class="language-plaintext highlighter-rouge">10.244.205.193</code> and is running on the <code class="language-plaintext highlighter-rouge">minikube-m02</code> node.</p>

<p>Let us log in to this <code class="language-plaintext highlighter-rouge">minikube-m02</code> node and verify the routing table and list of interfaces.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m02
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 22:a7:1a:d9:be:42 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.7/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 84337sec preferred_lft 84337sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:d3:aa:60:ec brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.205.192/32 scope global tunl0
       valid_lft forever preferred_lft forever
8: calic440f455693@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.7 metric 1024
10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink
10.244.151.0/26 via 172.16.30.8 dev tunl0 proto bird onlink
blackhole 10.244.205.192/26 proto bird
10.244.205.193 dev calic440f455693 scope link
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.7
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.7 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$
</span></code></pre></div></div>

<p>There are two special interfaces, <code class="language-plaintext highlighter-rouge">5: tunl0@NONE:</code> and <code class="language-plaintext highlighter-rouge">8: calic440f455693@if5:</code>. We can see that it is an <code class="language-plaintext highlighter-rouge">ipip</code> tunnel and tunnel interface (<code class="language-plaintext highlighter-rouge">tunl0</code>) has the IP address of <code class="language-plaintext highlighter-rouge">10.244.205.192/32</code>.</p>

<p>From the routing table, there are two /26 routes pointing to the <code class="language-plaintext highlighter-rouge">tunl0</code> interface. The <code class="language-plaintext highlighter-rouge">10.244.120.64/26 via 172.16.30.6</code> and <code class="language-plaintext highlighter-rouge">10.244.151.0/26 via 172.16.30.8</code> entries in the routing table, shows  the <code class="language-plaintext highlighter-rouge">Calico</code> assigned subnets on the other two nodes, and the routes for those remote subnets via the <code class="language-plaintext highlighter-rouge">IPIP</code> tunnel interface.</p>

<p>From within the <code class="language-plaintext highlighter-rouge">minikube-m02</code> node, issue the <code class="language-plaintext highlighter-rouge">docker ps</code> command the retrieve the container ID of the <code class="language-plaintext highlighter-rouge">nginx</code> container.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS     NAMES
0d1f5d390956   nginx                  <span class="s2">"/docker-entrypoint.â€¦"</span>   28 minutes ago   Up 28 minutes             k8s_nginx_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0
6b67d9586b86   k8s.gcr.io/pause:3.6   <span class="s2">"/pause"</span>                 28 minutes ago   Up 28 minutes             k8s_POD_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0
6f65ec9ee321   5ef66b403f4f           <span class="s2">"start_runit"</span>            39 minutes ago   Up 39 minutes             k8s_calico-node_calico-node-gqvw6_kube-system_6a835837-d36e-4366-aefe-cedc09d2f148_1
3d0be715b116   9b7cc9982109           <span class="s2">"/usr/local/bin/kubeâ€¦"</span>   41 minutes ago   Up 41 minutes             k8s_kube-proxy_kube-proxy-gm2dh_kube-system_770d601e-7b93-42fa-8019-5976ae95684e_0
9c077d1f8374   k8s.gcr.io/pause:3.6   <span class="s2">"/pause"</span>                 41 minutes ago   Up 41 minutes             k8s_POD_calico-node-gqvw6_kube-system_6a835837-d36e-4366-aefe-cedc09d2f148_0
4b2360ad79a9   k8s.gcr.io/pause:3.6   <span class="s2">"/pause"</span>                 41 minutes ago   Up 41 minutes             k8s_POD_kube-proxy-gm2dh_kube-system_770d601e-7b93-42fa-8019-5976ae95684e_0
<span class="err">$</span>
</code></pre></div></div>

<p>Get the <code class="language-plaintext highlighter-rouge">Pid</code> of the <code class="language-plaintext highlighter-rouge">nginx</code> container with the <code class="language-plaintext highlighter-rouge">docker inspect</code> command.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker inspect 0d1f5d390956 | <span class="nb">grep </span>Pid
            <span class="s2">"Pid"</span>: 11380,
            <span class="s2">"PidMode"</span>: <span class="s2">""</span>,
            <span class="s2">"PidsLimit"</span>: null,
<span class="err">$</span>
</code></pre></div></div>

<p>With the <code class="language-plaintext highlighter-rouge">nsenter</code> command, verify the list of interfaces inside the <code class="language-plaintext highlighter-rouge">nginx</code> container.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 11380 <span class="nt">-n</span> ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    <span class="nb">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    <span class="nb">link</span>/sit 0.0.0.0 brd 0.0.0.0
3: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    <span class="nb">link</span>/ipip 0.0.0.0 brd 0.0.0.0
5: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default
    <span class="nb">link</span>/ether e6:ea:af:be:ec:53 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.205.193/32 brd 10.244.205.193 scope global eth0
       valid_lft forever preferred_lft forever
<span class="err">$</span>
</code></pre></div></div>

<p>We can see that the Pod IP is configured on the <code class="language-plaintext highlighter-rouge">5: eth0@if8:</code> interface. From the <code class="language-plaintext highlighter-rouge">@if8</code>, we can see the link to the <code class="language-plaintext highlighter-rouge">8: calic440f455693@if5:</code> interface on the host (like the <code class="language-plaintext highlighter-rouge">veth</code> interfaces in the case of Kindnet CNI).</p>

<p>Look at the routing table of the <code class="language-plaintext highlighter-rouge">nginx</code> pod.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 11380 <span class="nt">-n</span> ip route
default via 169.254.1.1 dev eth0
169.254.1.1 dev eth0 scope <span class="nb">link</span>
<span class="err">$</span>
</code></pre></div></div>

<p>Exit the <code class="language-plaintext highlighter-rouge">minikube-m02</code> node and use the <code class="language-plaintext highlighter-rouge">kubectl describe nodes</code> command and filter for the PodCIDRs.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe nodes | <span class="nb">grep</span> <span class="nt">-e</span> Name <span class="nt">-e</span> PodCIDR
Name:               minikube
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
Name:               minikube-m02
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
  Namespace                   Name                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
Name:               minikube-m03
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
  Namespace                   Name                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Currently, there is only one pod (<code class="language-plaintext highlighter-rouge">nginx</code>) in the cluster. Similar to what we have verified on the <code class="language-plaintext highlighter-rouge">minikube-m02</code> node, do the same on the other two nodes as well.</p>

<p>From the <code class="language-plaintext highlighter-rouge">minikube</code> node:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 9e:55:51:7f:e1:98 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.6/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 82845sec preferred_lft 82845sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:92:37:51:98 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: cni-podman0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 36:85:e8:76:d2:1c brd ff:ff:ff:ff:ff:ff
    inet 10.88.0.1/16 brd 10.88.255.255 scope global cni-podman0
       valid_lft forever preferred_lft forever
6: veth5a561323@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cni-podman0 state UP group default
    link/ether f6:67:49:87:51:36 brd ff:ff:ff:ff:ff:ff link-netnsid 0
7: vethb6ed82ba@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cni-podman0 state UP group default
    link/ether 22:b5:fe:8a:37:59 brd ff:ff:ff:ff:ff:ff link-netnsid 1
8: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.120.64/32 scope global tunl0
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.6 metric 1024
10.88.0.0/16 dev cni-podman0 proto kernel scope link src 10.88.0.1
blackhole 10.244.120.64/26 proto bird
10.244.151.0/26 via 172.16.30.8 dev tunl0 proto bird onlink
10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.6
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.6 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$ exit
logout
pradeep@learnk8s$
</span></code></pre></div></div>
<p>On the <code class="language-plaintext highlighter-rouge">controlplane</code> node, there are extra interfaces coming from the <code class="language-plaintext highlighter-rouge">Podman CNI</code>.</p>

<p>From the <code class="language-plaintext highlighter-rouge">minikube-m03</code> node:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m03
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether de:b8:1d:5e:d9:c0 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.8/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 83040sec preferred_lft 83040sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:66:0b:71:73 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.151.0/32 scope global tunl0
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.8 metric 1024
10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink
blackhole 10.244.151.0/26 proto bird
10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.8
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.8 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$
</span></code></pre></div></div>
<p>On other nodes also, we do see similar <code class="language-plaintext highlighter-rouge">tunl0</code> interface and routes for Calico subnets (/26s) via the <code class="language-plaintext highlighter-rouge">IPIP</code> tunnel.
For the local subnet, there is a <code class="language-plaintext highlighter-rouge">blackhole</code> route: for example on the <code class="language-plaintext highlighter-rouge">minikube-m03</code> node <code class="language-plaintext highlighter-rouge">blackhole 10.244.151.0/26 proto bird</code>, and on <code class="language-plaintext highlighter-rouge">minikube-m02</code> node <code class="language-plaintext highlighter-rouge">blackhole 10.244.205.192/26 proto bird</code>, on the <code class="language-plaintext highlighter-rouge">minikube</code> node <code class="language-plaintext highlighter-rouge">blackhole 10.244.120.64/26 proto bird</code>.</p>

<p>Let us manually schedule a Pod on the <code class="language-plaintext highlighter-rouge">minikube-m03</code> node.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat my-pod.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-manual</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">minikube-m03</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> my-pod.yaml
pod/nginx-manual created
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME           READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx          1/1     Running   0          53m   10.244.205.193   minikube-m02   &lt;none&gt;           &lt;none&gt;
nginx-manual   1/1     Running   0          39s   10.244.151.1     minikube-m03   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>As expected, the <code class="language-plaintext highlighter-rouge">nginx-manual</code> pod got an IP from the <code class="language-plaintext highlighter-rouge">10.244.151.0/26</code> subnet.</p>

<p>Log back to the <code class="language-plaintext highlighter-rouge">minikube-m03</code> node and check the routing table and list of interfaces.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m03
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether de:b8:1d:5e:d9:c0 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.8/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 82414sec preferred_lft 82414sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:66:0b:71:73 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.151.0/32 scope global tunl0
       valid_lft forever preferred_lft forever
8: califba6dd09590@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.8 metric 1024
10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink
blackhole 10.244.151.0/26 proto bird
10.244.151.1 dev califba6dd09590 scope link
10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.8
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.8 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$
</span></code></pre></div></div>

<p>Compared to the previous output (when there were no pods on this node), there is a new interface <code class="language-plaintext highlighter-rouge">8: califba6dd09590@if5:</code> and new route entry for the Pod IP <code class="language-plaintext highlighter-rouge">10.244.151.1 dev califba6dd09590 scope link</code>.</p>

<p>Get the container ID and Pid of this <code class="language-plaintext highlighter-rouge">nginx-manual</code> container.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED             STATUS             PORTS     NAMES
244b50fad9d4   nginx                  <span class="s2">"/docker-entrypoint.â€¦"</span>   5 minutes ago       Up 5 minutes                 k8s_nginx_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0
e7117e519923   k8s.gcr.io/pause:3.6   <span class="s2">"/pause"</span>                 5 minutes ago       Up 5 minutes                 k8s_POD_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0
c4c993bc1f2e   calico/node            <span class="s2">"start_runit"</span>            About an hour ago   Up About an hour             k8s_calico-node_calico-node-sw74l_kube-system_0e30bbad-8370-4907-ab20-ce81450ad13c_0
a3482f106e3a   9b7cc9982109           <span class="s2">"/usr/local/bin/kubeâ€¦"</span>   About an hour ago   Up About an hour             k8s_kube-proxy_kube-proxy-7k4lb_kube-system_f284e305-e71b-40b0-a715-796d0733bc03_0
25a914d158da   k8s.gcr.io/pause:3.6   <span class="s2">"/pause"</span>                 About an hour ago   Up About an hour             k8s_POD_calico-node-sw74l_kube-system_0e30bbad-8370-4907-ab20-ce81450ad13c_0
abce35d2a0ae   k8s.gcr.io/pause:3.6   <span class="s2">"/pause"</span>                 About an hour ago   Up About an hour             k8s_POD_kube-proxy-7k4lb_kube-system_f284e305-e71b-40b0-a715-796d0733bc03_0
<span class="err">$</span>
</code></pre></div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker inspect 244b50fad9d4 | <span class="nb">grep </span>Pid
            <span class="s2">"Pid"</span>: 42125,
            <span class="s2">"PidMode"</span>: <span class="s2">""</span>,
            <span class="s2">"PidsLimit"</span>: null,
<span class="err">$</span>
</code></pre></div></div>

<p>Use the <code class="language-plaintext highlighter-rouge">nsenter</code> command and verify the Pod interfaces and route table.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nsenter <span class="nt">-t</span> 42125 <span class="nt">-n</span> ip a
nsenter: cannot open /proc/42125/ns/net: Permission denied
<span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 42125 <span class="nt">-n</span> ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    <span class="nb">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    <span class="nb">link</span>/sit 0.0.0.0 brd 0.0.0.0
3: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    <span class="nb">link</span>/ipip 0.0.0.0 brd 0.0.0.0
5: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default
    <span class="nb">link</span>/ether 9a:93:7b:77:8f:c7 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.151.1/32 brd 10.244.151.1 scope global eth0
       valid_lft forever preferred_lft forever
<span class="err">$</span>
</code></pre></div></div>
<p>The <code class="language-plaintext highlighter-rouge">eth0</code> interface (<code class="language-plaintext highlighter-rouge">5: eth0@if8</code>) of the Pod has the IP address <code class="language-plaintext highlighter-rouge">10.244.151.1/32</code> and is mapped to the  new <code class="language-plaintext highlighter-rouge">cali</code> interface <code class="language-plaintext highlighter-rouge">8: califba6dd09590@if5:</code> on the host.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 42125 <span class="nt">-n</span> ip route
default via 169.254.1.1 dev eth0
169.254.1.1 dev eth0 scope <span class="nb">link</span>
<span class="err">$</span>
</code></pre></div></div>
<p>Verify communication between the two Pods. <code class="language-plaintext highlighter-rouge">10.244.205.193</code> is the IP address of the <code class="language-plaintext highlighter-rouge">nginx</code> pod running on the other node <code class="language-plaintext highlighter-rouge">minikube-m02</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 42125 <span class="nt">-n</span> ping 10.244.205.193
PING 10.244.205.193 <span class="o">(</span>10.244.205.193<span class="o">)</span>: 56 data bytes
64 bytes from 10.244.205.193: <span class="nb">seq</span><span class="o">=</span>0 <span class="nv">ttl</span><span class="o">=</span>62 <span class="nb">time</span><span class="o">=</span>17.321 ms
64 bytes from 10.244.205.193: <span class="nb">seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>62 <span class="nb">time</span><span class="o">=</span>0.963 ms
64 bytes from 10.244.205.193: <span class="nb">seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>62 <span class="nb">time</span><span class="o">=</span>0.891 ms
64 bytes from 10.244.205.193: <span class="nb">seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>62 <span class="nb">time</span><span class="o">=</span>1.529 ms
^C
<span class="nt">---</span> 10.244.205.193 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max <span class="o">=</span> 0.891/5.176/17.321 ms
<span class="err">$</span>
</code></pre></div></div>
<p>Verify communication to the outside cluster hosts, like any internet host ( for example 8.8.8.8)</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 42125 <span class="nt">-n</span> ping 8.8.8.8
PING 8.8.8.8 <span class="o">(</span>8.8.8.8<span class="o">)</span>: 56 data bytes
64 bytes from 8.8.8.8: <span class="nb">seq</span><span class="o">=</span>0 <span class="nv">ttl</span><span class="o">=</span>115 <span class="nb">time</span><span class="o">=</span>20.260 ms
64 bytes from 8.8.8.8: <span class="nb">seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>115 <span class="nb">time</span><span class="o">=</span>15.457 ms
^C
<span class="nt">---</span> 8.8.8.8 ping statistics <span class="nt">---</span>
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max <span class="o">=</span> 15.457/17.858/20.260 ms
<span class="err">$</span>
</code></pre></div></div>
<p>Let us install <code class="language-plaintext highlighter-rouge">calicoctl</code> as a Pod itself.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> https://projectcalico.docs.tigera.io/manifests/calicoctl.yaml
serviceaccount/calicoctl created
pod/calicoctl created
clusterrole.rbac.authorization.k8s.io/calicoctl created
clusterrolebinding.rbac.authorization.k8s.io/calicoctl created
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Verify that the <code class="language-plaintext highlighter-rouge">calicoctl</code> pod is running.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-n</span> kube-system <span class="nt">-o</span> wide
NAME                                       READY   STATUS    RESTARTS      AGE   IP            NODE           NOMINATED NODE   READINESS GATES
calico-kube-controllers-8594699699-dztlm   1/1     Running   0             82m   10.88.0.3     minikube       &lt;none&gt;           &lt;none&gt;
calico-node-gqvw6                          1/1     Running   1 <span class="o">(</span>78m ago<span class="o">)</span>   80m   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
calico-node-qdbcf                          1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
calico-node-sw74l                          1/1     Running   0             77m   172.16.30.8   minikube-m03   &lt;none&gt;           &lt;none&gt;
calicoctl                                  1/1     Running   0             18s   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
coredns-64897985d-58btq                    1/1     Running   0             82m   10.88.0.2     minikube       &lt;none&gt;           &lt;none&gt;
etcd-minikube                              1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-apiserver-minikube                    1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-controller-manager-minikube           1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-proxy-7k4lb                           1/1     Running   0             77m   172.16.30.8   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-proxy-gm2dh                           1/1     Running   0             80m   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-proxy-hvkqd                           1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-scheduler-minikube                    1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
storage-provisioner                        1/1     Running   1 <span class="o">(</span>77m ago<span class="o">)</span>   82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Verify the <code class="language-plaintext highlighter-rouge">calicoctl</code></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nt">-n</span> kube-system calicoctl <span class="nt">--</span> /calicoctl get nodes <span class="nt">-o</span> wide
Failed to get resources: Version mismatch.
Client Version:   v3.22.1
Cluster Version:  v3.20.0
Use <span class="nt">--allow-version-mismatch</span> to override.

<span class="nb">command </span>terminated with <span class="nb">exit </span>code 1
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Repeat it with <code class="language-plaintext highlighter-rouge">--allow-version-mismatch</code>.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nt">-n</span> kube-system calicoctl <span class="nt">--</span> /calicoctl get nodes <span class="nt">-o</span> wide <span class="nt">--allow-version-mismatch</span>
NAME           ASN       IPV4             IPV6
minikube       <span class="o">(</span>64512<span class="o">)</span>   172.16.30.6/24
minikube-m02   <span class="o">(</span>64512<span class="o">)</span>   172.16.30.7/24
minikube-m03   <span class="o">(</span>64512<span class="o">)</span>   172.16.30.8/24

pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>We can confirm that the <code class="language-plaintext highlighter-rouge">calicoctl</code> is working and we can see some BGP Autonomous systems shown as well. Currently all three nodes are part of the same ASN (<code class="language-plaintext highlighter-rouge">64512</code>).</p>

<p>Create an alias for the <code class="language-plaintext highlighter-rouge">calicoctl</code></p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span><span class="nb">alias </span><span class="nv">calicoctl</span><span class="o">=</span><span class="s2">"kubectl exec -i -n kube-system calicoctl -- /calicoctl --allow-version-mismatch"</span>
</code></pre></div></div>

<p>By default, calicoctl will attempt to read from the Kubernetes API using the default kubeconfig located at $(HOME)/.kube/config.</p>

<p>If the default kubeconfig does not exist, or you would like to specify alternative API access information, you can do so using the following configuration options.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ DATASTORE_TYPE</span><span class="o">=</span>kubernetes <span class="nv">KUBECONFIG</span><span class="o">=</span>~/.kube/config calicoctl get nodes
NAME
minikube
minikube-m02
minikube-m03

pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Verify all API-Resources installed by <code class="language-plaintext highlighter-rouge">Calico</code> CNI plugin.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl api-resources | <span class="nb">grep </span>calico
bgpconfigurations                              crd.projectcalico.org/v1               <span class="nb">false        </span>BGPConfiguration
bgppeers                                       crd.projectcalico.org/v1               <span class="nb">false        </span>BGPPeer
blockaffinities                                crd.projectcalico.org/v1               <span class="nb">false        </span>BlockAffinity
clusterinformations                            crd.projectcalico.org/v1               <span class="nb">false        </span>ClusterInformation
felixconfigurations                            crd.projectcalico.org/v1               <span class="nb">false        </span>FelixConfiguration
globalnetworkpolicies                          crd.projectcalico.org/v1               <span class="nb">false        </span>GlobalNetworkPolicy
globalnetworksets                              crd.projectcalico.org/v1               <span class="nb">false        </span>GlobalNetworkSet
hostendpoints                                  crd.projectcalico.org/v1               <span class="nb">false        </span>HostEndpoint
ipamblocks                                     crd.projectcalico.org/v1               <span class="nb">false        </span>IPAMBlock
ipamconfigs                                    crd.projectcalico.org/v1               <span class="nb">false        </span>IPAMConfig
ipamhandles                                    crd.projectcalico.org/v1               <span class="nb">false        </span>IPAMHandle
ippools                                        crd.projectcalico.org/v1               <span class="nb">false        </span>IPPool
kubecontrollersconfigurations                  crd.projectcalico.org/v1               <span class="nb">false        </span>KubeControllersConfiguration
networkpolicies                                crd.projectcalico.org/v1               <span class="nb">true         </span>NetworkPolicy
networksets                                    crd.projectcalico.org/v1               <span class="nb">true         </span>NetworkSet
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Verify some of these resources with <code class="language-plaintext highlighter-rouge">kubectl get</code> and <code class="language-plaintext highlighter-rouge">kubectl describe</code> commands.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get bgppeers <span class="nt">-A</span>
No resources found
</code></pre></div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get ippools
NAME                  AGE
default-ipv4-ippool   97m
pradeep@learnk8s<span class="nv">$ </span>kubectl describe ippools
Name:         default-ipv4-ippool
Namespace:
Labels:       &lt;none&gt;
Annotations:  projectcalico.org/metadata: <span class="o">{</span><span class="s2">"uid"</span>:<span class="s2">"cf94cf43-8887-4528-a934-ca498f0422e1"</span>,<span class="s2">"creationTimestamp"</span>:<span class="s2">"2022-03-19T18:20:36Z"</span><span class="o">}</span>
API Version:  crd.projectcalico.org/v1
Kind:         IPPool
Metadata:
  Creation Timestamp:  2022-03-19T18:20:36Z
  Generation:          1
  Managed Fields:
    API Version:  crd.projectcalico.org/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:projectcalico.org/metadata:
      f:spec:
        .:
        f:blockSize:
        f:cidr:
        f:ipipMode:
        f:natOutgoing:
        f:nodeSelector:
        f:vxlanMode:
    Manager:         Go-http-client
    Operation:       Update
    Time:            2022-03-19T18:20:36Z
  Resource Version:  659
  UID:               8d9e076a-dae6-4bbd-8f44-920da83472ba
Spec:
  Block Size:     26
  Cidr:           10.244.0.0/16
  Ipip Mode:      Always
  Nat Outgoing:   <span class="nb">true
  </span>Node Selector:  all<span class="o">()</span>
  Vxlan Mode:     Never
Events:           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get ipamblocks
NAME                AGE
10-244-120-64-26    98m
10-244-151-0-26     93m
10-244-205-192-26   95m
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>We have seen these IPAM blocks already, when we verified the routing tables on each node.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get networkpolicies
No resources found <span class="k">in </span>default namespace.
</code></pre></div></div>

<p>Let us create a network policy now.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat network-policy.yaml</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkPolicy</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">default-deny-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">podSelector</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">policyTypes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Ingress</span>
<span class="s">pradeep@learnk8s$</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> network-policy.yaml
networkpolicy.networking.k8s.io/default-deny-ingress created
</code></pre></div></div>
<p>Verify that the network policy is created.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get netpol
NAME                   POD-SELECTOR   AGE
default-deny-ingress   &lt;none&gt;         2s
</code></pre></div></div>
<p>Describe it for more details.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe netpol
Name:         default-deny-ingress
Namespace:    default
Created on:   2022-03-20 01:45:29 +0530 IST
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; <span class="o">(</span>Allowing the specific traffic to all pods <span class="k">in </span>this namespace<span class="o">)</span>
  Allowing ingress traffic:
    &lt;none&gt; <span class="o">(</span>Selected pods are isolated <span class="k">for </span>ingress connectivity<span class="o">)</span>
  Not affecting egress traffic
  Policy Types: Ingress
pradeep@learnk8s<span class="nv">$ </span><span class="nb">cat </span>network-policy.yaml
<span class="nt">---</span>
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: <span class="o">{}</span>
  policyTypes:
  - Ingress
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>If you recall, we have two pods in our cluster at the moment.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME           READY   STATUS    RESTARTS   AGE    IP               NODE           NOMINATED NODE   READINESS GATES
nginx          1/1     Running   0          105m   10.244.205.193   minikube-m02   &lt;none&gt;           &lt;none&gt;
nginx-manual   1/1     Running   0          52m    10.244.151.1     minikube-m03   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>

<p>Now that the network policy is applied, let us check the connectivity between these two pods again.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m02
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ docker ps | grep nginx
0d1f5d390956   nginx                  "/docker-entrypoint.â€¦"   2 hours ago      Up 2 hours                k8s_nginx_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0
6b67d9586b86   k8s.gcr.io/pause:3.6   "/pause"                 2 hours ago      Up 2 hours                k8s_POD_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0
$ docker inspect 0d1f5d390956 | grep Pid
            "Pid": 11380,
            "PidMode": "",
            "PidsLimit": null,
$ sudo nsenter -t 11380 -n ping 10.244.151.1
PING 10.244.151.1 (10.244.151.1): 56 data bytes
^C
--- 10.244.151.1 ping statistics ---
3 packets transmitted, 0 packets received, 100% packet loss
$
</span></code></pre></div></div>
<p>From <code class="language-plaintext highlighter-rouge">nginx</code> pod, we are not able to ping to <code class="language-plaintext highlighter-rouge">nginx-manual</code> pod.</p>

<p>Similarly, verify the other way, ping from <code class="language-plaintext highlighter-rouge">nginx-manual</code> to <code class="language-plaintext highlighter-rouge">nginx</code> Pod (which was working earlier, that we tested already!).</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m03
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ docker ps | grep nginx
244b50fad9d4   nginx                  "/docker-entrypoint.â€¦"   55 minutes ago   Up 55 minutes             k8s_nginx_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0
e7117e519923   k8s.gcr.io/pause:3.6   "/pause"                 55 minutes ago   Up 55 minutes             k8s_POD_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0
$ docker inspect 244b50fad9d4 | grep Pid
            "Pid": 42125,
            "PidMode": "",
            "PidsLimit": null,
$ sudo nsenter -t 42125 -n ping 10.244.205.193
PING 10.244.205.193 (10.244.205.193): 56 data bytes
^C
--- 10.244.205.193 ping statistics ---
2 packets transmitted, 0 packets received, 100% packet loss
$ exit 0
logout
pradeep@learnk8s$
</span></code></pre></div></div>

<p>With network policy applied, the communication is blocked.</p>

<p>Let us create another network policy, this time to allow all ingress.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat allow-ingress.yaml</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkPolicy</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">allow-all-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">podSelector</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">ingress</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="pi">{}</span>
  <span class="na">policyTypes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Ingress</span>

</code></pre></div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> allow-ingress.yaml
networkpolicy.networking.k8s.io/allow-all-ingress created
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get netpol
NAME                   POD-SELECTOR   AGE
allow-all-ingress      &lt;none&gt;         5s
default-deny-ingress   &lt;none&gt;         11m
</code></pre></div></div>

<p>Describe the new policy.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe netpol allow-all-ingress
Name:         allow-all-ingress
Namespace:    default
Created on:   2022-03-20 01:56:32 +0530 IST
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; <span class="o">(</span>Allowing the specific traffic to all pods <span class="k">in </span>this namespace<span class="o">)</span>
  Allowing ingress traffic:
    To Port: &lt;any&gt; <span class="o">(</span>traffic allowed to all ports<span class="o">)</span>
    From: &lt;any&gt; <span class="o">(</span>traffic not restricted by <span class="nb">source</span><span class="o">)</span>
  Not affecting egress traffic
  Policy Types: Ingress
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>

<p>Now that we have allowed the ingress traffic to all pods/all ports.</p>

<p>Verify again. Now that we know the Pid of the containers, those steps are not needed.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m02
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ sudo nsenter -t 11380 -n ping 10.244.151.1
PING 10.244.151.1 (10.244.151.1): 56 data bytes
64 bytes from 10.244.151.1: seq=0 ttl=62 time=3.834 ms
64 bytes from 10.244.151.1: seq=1 ttl=62 time=1.409 ms
64 bytes from 10.244.151.1: seq=2 ttl=62 time=0.723 ms
^C
--- 10.244.151.1 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.723/1.988/3.834 ms
$ exit 0
logout
</span></code></pre></div></div>
<p>Yay! Ping is working again.</p>

<p>Similarly, verify in the other direction.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m03
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ sudo nsenter -t 42125 -n ping 10.244.205.193
PING 10.244.205.193 (10.244.205.193): 56 data bytes
64 bytes from 10.244.205.193: seq=0 ttl=62 time=1.132 ms
64 bytes from 10.244.205.193: seq=1 ttl=62 time=1.672 ms
64 bytes from 10.244.205.193: seq=2 ttl=62 time=1.695 ms
^C
--- 10.244.205.193 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 1.132/1.499/1.695 ms
$ exit 0
logout
</span></code></pre></div></div>

<p>This completes our initial discussion on the Calico CNI. We just scratched the surface of it, there are many more features, which will be discussed in other posts.</p>]]></content><author><name>Kubernetes</name></author><category term="Kubernetes" /><category term="minikube" /><summary type="html"><![CDATA[Kubernetes Networking with Calico CNI]]></summary></entry><entry><title type="html">Kubernetes Networking with Minikube (Kindnet CNI)</title><link href="https://www.pradeepgadde.com/blog/kubernetes/2022/03/19/kubernetes-networking.html" rel="alternate" type="text/html" title="Kubernetes Networking with Minikube (Kindnet CNI)" /><published>2022-03-19T10:55:04+05:30</published><updated>2022-03-19T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/kubernetes/2022/03/19/kubernetes-networking</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/kubernetes/2022/03/19/kubernetes-networking.html"><![CDATA[<p>Hello!</p>

<p>Welcome to Kubernetes Networking.</p>

<p>Let us setup a fresh minikube cluster with default settings and with 3 nodes in it.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube start <span class="nt">--nodes</span><span class="o">=</span>3
ðŸ˜„  minikube v1.25.2 on Darwin 12.2.1
âœ¨  Automatically selected the hyperkit driver
ðŸ’¾  Downloading driver docker-machine-driver-hyperkit:
    <span class="o">&gt;</span> docker-machine-driver-hyper...: 65 B / 65 B <span class="o">[</span><span class="nt">----------</span><span class="o">]</span> 100.00% ? p/s 0s
    <span class="o">&gt;</span> docker-machine-driver-hyper...: 8.35 MiB / 8.35 MiB  100.00% 9.87 MiB p/s
ðŸ”‘  The <span class="s1">'hyperkit'</span> driver requires elevated permissions. The following commands will be executed:

    <span class="nv">$ </span><span class="nb">sudo chown </span>root:wheel /Users/pradeep/.minikube/bin/docker-machine-driver-hyperkit
    <span class="nv">$ </span><span class="nb">sudo chmod </span>u+s /Users/pradeep/.minikube/bin/docker-machine-driver-hyperkit


Password:
ðŸ’¿  Downloading VM boot image ...
    <span class="o">&gt;</span> minikube-v1.25.2.iso.sha256: 65 B / 65 B <span class="o">[</span><span class="nt">-------------</span><span class="o">]</span> 100.00% ? p/s 0s
    <span class="o">&gt;</span> minikube-v1.25.2.iso: 237.06 MiB / 237.06 MiB <span class="o">[]</span> 100.00% 6.12 MiB p/s 39s
ðŸ‘  Starting control plane node minikube <span class="k">in </span>cluster minikube
ðŸ’¾  Downloading Kubernetes v1.23.3 preload ...
    <span class="o">&gt;</span> preloaded-images-k8s-v17-v1...: 505.68 MiB / 505.68 MiB  100.00% 9.37 MiB
ðŸ”¥  Creating hyperkit VM <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB, <span class="nv">Disk</span><span class="o">=</span>20000MB<span class="o">)</span> ...
ðŸ³  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    â–ª kubelet.housekeeping-interval<span class="o">=</span>5m
    â–ª kubelet.cni-conf-dir<span class="o">=</span>/etc/cni/net.mk
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ðŸ”—  Configuring CNI <span class="o">(</span>Container Networking Interface<span class="o">)</span> ...
ðŸ”Ž  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass

ðŸ‘  Starting worker node minikube-m02 <span class="k">in </span>cluster minikube
ðŸ”¥  Creating hyperkit VM <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB, <span class="nv">Disk</span><span class="o">=</span>20000MB<span class="o">)</span> ...
ðŸŒ  Found network options:
    â–ª <span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.3
ðŸ³  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    â–ª <span class="nb">env </span><span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.3
ðŸ”Ž  Verifying Kubernetes components...

ðŸ‘  Starting worker node minikube-m03 <span class="k">in </span>cluster minikube
ðŸ”¥  Creating hyperkit VM <span class="o">(</span><span class="nv">CPUs</span><span class="o">=</span>2, <span class="nv">Memory</span><span class="o">=</span>2200MB, <span class="nv">Disk</span><span class="o">=</span>20000MB<span class="o">)</span> ...
ðŸŒ  Found network options:
    â–ª <span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.3,172.16.30.4
ðŸ³  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    â–ª <span class="nb">env </span><span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.3
    â–ª <span class="nb">env </span><span class="nv">NO_PROXY</span><span class="o">=</span>172.16.30.3,172.16.30.4
ðŸ”Ž  Verifying Kubernetes components...
ðŸ„  Done! kubectl is now configured to use <span class="s2">"minikube"</span> cluster and <span class="s2">"default"</span> namespace by default
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Our cluster nodes are assinged the IPs: <code class="language-plaintext highlighter-rouge">172.16.30.3</code>, <code class="language-plaintext highlighter-rouge">172.16.30.4</code>, and <code class="language-plaintext highlighter-rouge">172.16.30.5</code> respectively.</p>

<p>Also, note that the CNI config directory <code class="language-plaintext highlighter-rouge">kubelet.cni-conf-dir=/etc/cni/net.mk</code> and minikube is configuring CNI (Container Networking Interface) during the start.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get nodes <span class="nt">-o</span> wide
NAME           STATUS   ROLES                  AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube       Ready    control-plane,master   6m54s   v1.23.3   172.16.30.3   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
minikube-m02   Ready    &lt;none&gt;                 4m43s   v1.23.3   172.16.30.4   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
minikube-m03   Ready    &lt;none&gt;                 74s     v1.23.3   172.16.30.5   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Let us take a look at all the pods in this newly deployed cluster. Currently, all the pods are in the <code class="language-plaintext highlighter-rouge">kube-system</code> namespace. The pods that are of interest to us in this post are the ones starting with the name <code class="language-plaintext highlighter-rouge">kindnet</code>.</p>

<p>So what is Kindnet? Kindnet is a simple CNI plugin for Kubernetes with IPv4 and IPv6 support that provides the Cluster Networking.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-A</span>
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-64897985d-llrjh            1/1     Running   0          7m25s
kube-system   etcd-minikube                      1/1     Running   0          7m40s
kube-system   kindnet-4mrvw                      1/1     Running   0          2m2s
kube-system   kindnet-cpv4s                      1/1     Running   0          5m31s
kube-system   kindnet-xqjlm                      1/1     Running   0          7m26s
kube-system   kube-apiserver-minikube            1/1     Running   0          7m37s
kube-system   kube-controller-manager-minikube   1/1     Running   0          7m37s
kube-system   kube-proxy-b97w8                   1/1     Running   0          7m26s
kube-system   kube-proxy-gtlw7                   1/1     Running   0          2m2s
kube-system   kube-proxy-rts4b                   1/1     Running   0          5m31s
kube-system   kube-scheduler-minikube            1/1     Running   0          7m37s
kube-system   storage-provisioner                1/1     Running   1          7m35s
</code></pre></div></div>

<p>There are three kindnet pods, and if we check the IP address of these pods, all of them are having the same IP address as that of their node.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-A</span> <span class="nt">-o</span> wide | <span class="nb">grep </span>kindnet
kube-system   kindnet-4mrvw                      1/1     Running   0          6m17s   172.16.30.5   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-cpv4s                      1/1     Running   0          9m46s   172.16.30.4   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-xqjlm                      1/1     Running   0          11m     172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Let us login to the first node, and check its routing table and all the interfaces currently configured on this node.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether d6:df:bb:d6:c7:bc brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.3/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 85395sec preferred_lft 85395sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:df:e0:80:59 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: veth2b322de6@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 8e:09:87:56:8c:50 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.0.1/32 brd 10.244.0.1 scope global veth2b322de6
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.3 metric 1024
10.244.0.2 dev veth2b322de6 scope host
10.244.1.0/24 via 172.16.30.4 dev eth0
10.244.2.0/24 via 172.16.30.5 dev eth0
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.3
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.3 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
</span></code></pre></div></div>

<p>This <code class="language-plaintext highlighter-rouge">minikube</code> node has the IP address of <code class="language-plaintext highlighter-rouge">172.16.30.3/24</code> on the <code class="language-plaintext highlighter-rouge">eth0</code> interface and  there is a static route to <code class="language-plaintext highlighter-rouge">10.244.1.0/24</code> with next-hop as the <code class="language-plaintext highlighter-rouge">minikube-m02</code> node with IP address of <code class="language-plaintext highlighter-rouge">172.16.30.4</code> and similarly another static route to <code class="language-plaintext highlighter-rouge">10.244.2.0/24</code> with next-hop as <code class="language-plaintext highlighter-rouge">172.16.30.5</code> which is the IP address of the the <code class="language-plaintext highlighter-rouge">minikube-m03</code> node.</p>

<p>There is another virtual interface (<code class="language-plaintext highlighter-rouge">veth</code>) called <code class="language-plaintext highlighter-rouge">veth2b322de6@if4</code> with an IP address of <code class="language-plaintext highlighter-rouge">10.244.0.1/32</code>. So what are these subnets <code class="language-plaintext highlighter-rouge">10.244.X.0/24</code> used for?</p>

<p>If we look at the <code class="language-plaintext highlighter-rouge">kindnet</code> CNI configuration file (located at the path given in the <code class="language-plaintext highlighter-rouge">minikube start</code> output shown above), on this <code class="language-plaintext highlighter-rouge">minikube</code> node,  we can see that there is a subnet <code class="language-plaintext highlighter-rouge">10.244.0.0/24</code> range defined.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">$</span><span class="w"> </span><span class="err">cat</span><span class="w"> </span><span class="err">/etc/cni/net.mk/</span><span class="mi">10</span><span class="err">-kindnet.conflist</span><span class="w">
</span><span class="p">{</span><span class="w">
	</span><span class="nl">"cniVersion"</span><span class="p">:</span><span class="w"> </span><span class="s2">"0.3.1"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"kindnet"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"plugins"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
	</span><span class="p">{</span><span class="w">
		</span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ptp"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"ipMasq"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
		</span><span class="nl">"ipam"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
			</span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"host-local"</span><span class="p">,</span><span class="w">
			</span><span class="nl">"dataDir"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/run/cni-ipam-state"</span><span class="p">,</span><span class="w">
			</span><span class="nl">"routes"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">


				</span><span class="p">{</span><span class="w"> </span><span class="nl">"dst"</span><span class="p">:</span><span class="w"> </span><span class="s2">"0.0.0.0/0"</span><span class="w"> </span><span class="p">}</span><span class="w">
			</span><span class="p">],</span><span class="w">
			</span><span class="nl">"ranges"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">


				</span><span class="p">[</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"subnet"</span><span class="p">:</span><span class="w"> </span><span class="s2">"10.244.0.0/24"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">]</span><span class="w">
			</span><span class="p">]</span><span class="w">
		</span><span class="p">}</span><span class="w">
		</span><span class="p">,</span><span class="w">
		</span><span class="nl">"mtu"</span><span class="p">:</span><span class="w"> </span><span class="mi">1500</span><span class="w">

	</span><span class="p">},</span><span class="w">
	</span><span class="p">{</span><span class="w">
		</span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"portmap"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"capabilities"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
			</span><span class="nl">"portMappings"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
		</span><span class="p">}</span><span class="w">
	</span><span class="p">}</span><span class="w">
	</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>
<p>Similarly, let us verify the same details in other two nodes of the cluster.</p>

<p>First, login to the <code class="language-plaintext highlighter-rouge">minikube-m02</code> node.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m02
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 2e:6c:52:09:b4:bb brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.4/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 84780sec preferred_lft 84780sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:b9:25:87:9f brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.4 metric 1024
10.244.0.0/24 via 172.16.30.3 dev eth0
10.244.2.0/24 via 172.16.30.5 dev eth0
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.4
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.4 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$ cat /etc/cni/net.mk/10-kindnet.conflist

{
	"cniVersion": "0.3.1",
	"name": "kindnet",
	"plugins": [
	{
		"type": "ptp",
		"ipMasq": false,
		"ipam": {
			"type": "host-local",
			"dataDir": "/run/cni-ipam-state",
			"routes": [


				{ "dst": "0.0.0.0/0" }
			],
			"ranges": [


				[ { "subnet": "10.244.1.0/24" } ]
			]
		}
		,
		"mtu": 1500

	},
	{
		"type": "portmap",
		"capabilities": {
			"portMappings": true
		}
	}
	]
}
$
</span></code></pre></div></div>
<p>We can see a similar setup. Local subnet range defined in the CNI (kindnet) configuration file is the <code class="language-plaintext highlighter-rouge">10.244.1.0/24</code> subnet and two static routes (<code class="language-plaintext highlighter-rouge">10.244.0.0/24</code> via <code class="language-plaintext highlighter-rouge">172.16.30.3</code> and another static route <code class="language-plaintext highlighter-rouge">10.244.2.0/24 via 172.16.30.5</code>).</p>

<p>Finally, let us check the same in the <code class="language-plaintext highlighter-rouge">minikube-m03</code> node.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m03
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 84738sec preferred_lft 84738sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.5 metric 1024
10.244.0.0/24 via 172.16.30.3 dev eth0
10.244.1.0/24 via 172.16.30.4 dev eth0
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.5
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.5 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$ cat /etc/cni/net.mk/10-kindnet.conflist

{
	"cniVersion": "0.3.1",
	"name": "kindnet",
	"plugins": [
	{
		"type": "ptp",
		"ipMasq": false,
		"ipam": {
			"type": "host-local",
			"dataDir": "/run/cni-ipam-state",
			"routes": [


				{ "dst": "0.0.0.0/0" }
			],
			"ranges": [


				[ { "subnet": "10.244.2.0/24" } ]
			]
		}
		,
		"mtu": 1500

	},
	{
		"type": "portmap",
		"capabilities": {
			"portMappings": true
		}
	}
	]
}
$ exit
logout
</span></code></pre></div></div>

<p>Here also, very similar setup. <code class="language-plaintext highlighter-rouge">10.244.2.0/24</code> is the local subnet and remote subnets are cofnigured via static routes (<code class="language-plaintext highlighter-rouge">10.244.0.0/24 via 172.16.30.3</code> and <code class="language-plaintext highlighter-rouge">10.244.1.0/24 via 172.16.30.4</code>).</p>

<p>One thing to note is that, in both of the worker nodes, there isnâ€™t any <code class="language-plaintext highlighter-rouge">veth</code> interface present yet, unlike the <code class="language-plaintext highlighter-rouge">controleplane</code> node.</p>

<p>To understand, why let us get the IP addresses of all the running pods.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-A</span> <span class="nt">-o</span> wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
kube-system   coredns-64897985d-llrjh            1/1     Running   0          32m   10.244.0.2    minikube       &lt;none&gt;           &lt;none&gt;
kube-system   etcd-minikube                      1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-4mrvw                      1/1     Running   0          27m   172.16.30.5   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-cpv4s                      1/1     Running   0          31m   172.16.30.4   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-xqjlm                      1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-minikube            1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-minikube   1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-b97w8                   1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-gtlw7                   1/1     Running   0          27m   172.16.30.5   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-rts4b                   1/1     Running   0          31m   172.16.30.4   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-minikube            1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   storage-provisioner                1/1     Running   1          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>There is one Pod named <code class="language-plaintext highlighter-rouge">coredns-64897985d-llrjh</code> in the <code class="language-plaintext highlighter-rouge">kube-system</code> namespace with an IP address of <code class="language-plaintext highlighter-rouge">10.244.0.2</code> which is in the same subnet range defined in the CNI config on the <code class="language-plaintext highlighter-rouge">minikube</code> node.</p>

<p>Let us create our first pod (we can use any image, it does not matter). For test purposes, let us create a new pod using the <code class="language-plaintext highlighter-rouge">busybox</code> image.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl run busybox <span class="nt">--image</span><span class="o">=</span>busybox
pod/busybox created
</code></pre></div></div>

<p>Verify the IP address assigned and the node on which it is running.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME      READY   STATUS      RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
busybox   0/1     Completed   0          12s   10.244.2.2   minikube-m03   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>We can see that the scheduler has assigned the <code class="language-plaintext highlighter-rouge">minikube-m03</code> node for this new pod and this <code class="language-plaintext highlighter-rouge">busybox</code> pod obtained its IP address <code class="language-plaintext highlighter-rouge">10.244.2.2</code> which is from the CNI assigned subnet (<code class="language-plaintext highlighter-rouge">10.244.2.0/24</code>) for this node.</p>

<p>Now, let us go back to the <code class="language-plaintext highlighter-rouge">minikube-m03</code> node and check the list of interfaces and see if there is anything new!</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m03
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 83749sec preferred_lft 83749sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: veth07b7de9e@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 6a:9a:f7:af:ac:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.2.1/32 brd 10.244.2.1 scope global veth07b7de9e
       valid_lft forever preferred_lft forever
$
</span></code></pre></div></div>
<p>Compared to the initial setup, there is one new interface (#5, with the name <code class="language-plaintext highlighter-rouge">veth07b7de9e@if4</code>). This is very similar to the <code class="language-plaintext highlighter-rouge">controlplane</code> node now.</p>

<p>This <code class="language-plaintext highlighter-rouge">veth</code> interface has the first IP address (<code class="language-plaintext highlighter-rouge">10.244.2.1/32</code>) from the CNI assigned subnet.</p>

<p>I tried to login to this container and check few things from inside the container, but before I do that the container crashed, so I have deleted it.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> busybox <span class="nt">--</span> /bin/bash
error: unable to upgrade connection: container not found <span class="o">(</span><span class="s2">"busybox"</span><span class="o">)</span>
pradeep@learnk8s<span class="nv">$ </span>kubectl get pods
NAME      READY   STATUS             RESTARTS      AGE
busybox   0/1     CrashLoopBackOff   6 <span class="o">(</span>63s ago<span class="o">)</span>   7m3s
pradeep@learnk8s<span class="nv">$ </span>kubectl delete pod busybox
pod <span class="s2">"busybox"</span> deleted
</code></pre></div></div>

<p>Create another containter, this time using another image, <code class="language-plaintext highlighter-rouge">nginx</code> for test purposes.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl run nginx <span class="nt">--image</span><span class="o">=</span>nginx
pod/nginx created
</code></pre></div></div>

<p>Verify the IP address of this new pod and the node on which it is running.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME    READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          25s   10.244.2.3   minikube-m03   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>Ah!, this pod also got assigned to the same node, <code class="language-plaintext highlighter-rouge">minikube-m03</code> and look at the IP address, the next IP address in the same range, <code class="language-plaintext highlighter-rouge">10.244.2.3</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m03
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 83069sec preferred_lft 83069sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
6: veth37e46d8f@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 7e:96:f9:98:5f:4a brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.2.1/32 brd 10.244.2.1 scope global veth37e46d8f
       valid_lft forever preferred_lft forever
$
</span></code></pre></div></div>
<p>From the routing table, we can see that the new Pod IP is routed via the <code class="language-plaintext highlighter-rouge">veth</code> interface <code class="language-plaintext highlighter-rouge">10.244.2.3 dev veth37e46d8f</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.5 metric 1024
10.244.0.0/24 via 172.16.30.3 dev eth0
10.244.1.0/24 via 172.16.30.4 dev eth0
10.244.2.3 dev veth37e46d8f scope host
172.16.30.0/24 dev eth0 proto kernel scope <span class="nb">link </span>src 172.16.30.5
172.16.30.1 dev eth0 proto dhcp scope <span class="nb">link </span>src 172.16.30.5 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope <span class="nb">link </span>src 172.17.0.1 linkdown
<span class="err">$</span>
</code></pre></div></div>

<p>Let us manually schedule a Pod on the <code class="language-plaintext highlighter-rouge">minikube-m02</code> node as well and observe.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat my-pod.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-manual</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">minikube-m02</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
</code></pre></div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> my-pod.yaml
pod/nginx-manual created
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME           READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
nginx-manual   1/1     Running   0          73s   10.244.1.2   minikube-m02   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s<span class="err">$</span>
</code></pre></div></div>
<p>We can see that, this pod has been given an IP (<code class="language-plaintext highlighter-rouge">10.244.1.2</code>) from the 10.244.1.0/24 subnet allocated to <code class="language-plaintext highlighter-rouge">minikube-m02</code> node.</p>

<p>Let us login to this node and verify the list of currently running pods with the <code class="language-plaintext highlighter-rouge">docker ps</code> command.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> minikube-m02
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS     NAMES
d6397d143118   nginx                  "/docker-entrypoint.â€¦"   3 minutes ago   Up 3 minutes             k8s_nginx_nginx-manual_default_950195d8-bc1d-420f-aede-fcd4a273f0e0_0
d2fb47206b71   k8s.gcr.io/pause:3.6   "/pause"                 4 minutes ago   Up 4 minutes             k8s_POD_nginx-manual_default_950195d8-bc1d-420f-aede-fcd4a273f0e0_0
ea4e0c180710   6de166512aa2           "/bin/kindnetd"          9 minutes ago   Up 9 minutes             k8s_kindnet-cni_kindnet-cpv4s_kube-system_5d3d3977-c5ff-4bea-917a-e0db52896da2_1
6ae7df286995   9b7cc9982109           "/usr/local/bin/kubeâ€¦"   9 minutes ago   Up 9 minutes             k8s_kube-proxy_kube-proxy-rts4b_kube-system_9773fb68-a469-4cb2-827b-546076677b3b_1
d77c3e7c06e4   k8s.gcr.io/pause:3.6   "/pause"                 9 minutes ago   Up 9 minutes             k8s_POD_kindnet-cpv4s_kube-system_5d3d3977-c5ff-4bea-917a-e0db52896da2_1
5e18805db658   k8s.gcr.io/pause:3.6   "/pause"                 9 minutes ago   Up 9 minutes             k8s_POD_kube-proxy-rts4b_kube-system_9773fb68-a469-4cb2-827b-546076677b3b_1
$
</span></code></pre></div></div>
<p>Obtain the container ID for the <code class="language-plaintext highlighter-rouge">nginx-manual</code> pod. In this case ,it is <code class="language-plaintext highlighter-rouge">d6397d143118</code>.</p>

<p>Use the <code class="language-plaintext highlighter-rouge">docker inspect</code> command to get the PID of this container.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker inspect d2fb47206b71 | <span class="nb">grep </span>Pid
            <span class="s2">"Pid"</span>: 4200,
            <span class="s2">"PidMode"</span>: <span class="s2">""</span>,
            <span class="s2">"PidsLimit"</span>: null,
<span class="nv">$ </span>
</code></pre></div></div>
<p>Now, take the <code class="language-plaintext highlighter-rouge">Pid</code> and use the <code class="language-plaintext highlighter-rouge">nsenter</code> command to issue the <code class="language-plaintext highlighter-rouge">ip a</code> command from inside this Pod namespace. This requires root privileges, so use <code class="language-plaintext highlighter-rouge">sudo</code> command.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 4200 <span class="nt">-n</span> ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    <span class="nb">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    <span class="nb">link</span>/sit 0.0.0.0 brd 0.0.0.0
4: eth0@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    <span class="nb">link</span>/ether 52:a1:40:3b:54:18 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.1.2/24 brd 10.244.1.255 scope global eth0
       valid_lft forever preferred_lft forever
<span class="err">$</span>
</code></pre></div></div>

<p>We can see that the Pod IP (<code class="language-plaintext highlighter-rouge">10.244.1.2</code>) is present on the interface ``#4<code class="language-plaintext highlighter-rouge"> named </code>eth0@if5`.</p>

<p>Look at all the interfaces from this <code class="language-plaintext highlighter-rouge">minikube-m02</code> node.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    <span class="nb">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    <span class="nb">link</span>/ether 2e:6c:52:09:b4:bb brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.4/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 85770sec preferred_lft 85770sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    <span class="nb">link</span>/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    <span class="nb">link</span>/ether 02:42:41:3c:16:fe brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: vethfa21a27c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    <span class="nb">link</span>/ether b2:26:8c:88:bf:d3 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.1.1/32 brd 10.244.1.1 scope global vethfa21a27c
       valid_lft forever preferred_lft forever
<span class="err">$</span>
</code></pre></div></div>
<p>From the interface naming, we can see the link between the Pod namespace and the host.  The <code class="language-plaintext highlighter-rouge">4: eth0@if5:</code> interface with the IP address <code class="language-plaintext highlighter-rouge">10.244.1.2/24</code> is linked to the <code class="language-plaintext highlighter-rouge">5: vethfa21a27c@if4:</code> interface with the IP address <code class="language-plaintext highlighter-rouge">10.244.1.1/32</code>.</p>

<p>The name followed by the <code class="language-plaintext highlighter-rouge">@</code> symbol tells us the interface number. For example, inside the Pod namespace, it is shown as <code class="language-plaintext highlighter-rouge">@if5</code> for the interface number <code class="language-plaintext highlighter-rouge">4:</code>, this corresponds to interface number <code class="language-plaintext highlighter-rouge">5:</code> on the host, which is nothing but the <code class="language-plaintext highlighter-rouge">5: vethfa21a27c@if4:</code>. From this <code class="language-plaintext highlighter-rouge">5: vethfa21a27c@if4:</code> on the host, <code class="language-plaintext highlighter-rouge">@if4</code> corresponds to the interface number <code class="language-plaintext highlighter-rouge">4</code> on the Pod, which is the <code class="language-plaintext highlighter-rouge">eth0</code> interface indicated by <code class="language-plaintext highlighter-rouge">4: eth0@if5:</code>.</p>

<p>Now you can see the full picture of the internal connectivity.</p>

<p>Just to confirm, this Pod hosted on <code class="language-plaintext highlighter-rouge">minikube-m02</code> can communicate with a Pod with IP address 10.244.0.2 hosted on the <code class="language-plaintext highlighter-rouge">minikube</code> node.
This can be confirmed by first looking at the Pod routing table, using the same <code class="language-plaintext highlighter-rouge">nsenter</code> command. There is a <code class="language-plaintext highlighter-rouge">default</code> route pointing to the gateway <code class="language-plaintext highlighter-rouge">10.244.1.1</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 4200 <span class="nt">-n</span> ip route
default via 10.244.1.1 dev eth0
10.244.1.0/24 via 10.244.1.1 dev eth0 src 10.244.1.2
10.244.1.1 dev eth0 scope <span class="nb">link </span>src 10.244.1.2
<span class="err">$</span>
</code></pre></div></div>

<p>Ping to another Pod in the same cluster, but on another node.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 4200 <span class="nt">-n</span> ping 10.244.0.2 <span class="nt">-c</span> 3
PING 10.244.0.2 <span class="o">(</span>10.244.0.2<span class="o">)</span>: 56 data bytes
64 bytes from 10.244.0.2: <span class="nb">seq</span><span class="o">=</span>0 <span class="nv">ttl</span><span class="o">=</span>62 <span class="nb">time</span><span class="o">=</span>1.313 ms
64 bytes from 10.244.0.2: <span class="nb">seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>62 <span class="nb">time</span><span class="o">=</span>1.980 ms
64 bytes from 10.244.0.2: <span class="nb">seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>62 <span class="nb">time</span><span class="o">=</span>1.139 ms

<span class="nt">---</span> 10.244.0.2 ping statistics <span class="nt">---</span>
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max <span class="o">=</span> 1.139/1.477/1.980 ms
<span class="err">$</span>
</code></pre></div></div>

<p>We can also see that using the same <code class="language-plaintext highlighter-rouge">default</code> route, this Pod can communicate with the outside cluster as well.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>nsenter <span class="nt">-t</span> 4200 <span class="nt">-n</span> ping 8.8.8.8
PING 8.8.8.8 <span class="o">(</span>8.8.8.8<span class="o">)</span>: 56 data bytes
64 bytes from 8.8.8.8: <span class="nb">seq</span><span class="o">=</span>0 <span class="nv">ttl</span><span class="o">=</span>115 <span class="nb">time</span><span class="o">=</span>12.515 ms
64 bytes from 8.8.8.8: <span class="nb">seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>115 <span class="nb">time</span><span class="o">=</span>12.238 ms
64 bytes from 8.8.8.8: <span class="nb">seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>115 <span class="nb">time</span><span class="o">=</span>11.216 ms
64 bytes from 8.8.8.8: <span class="nb">seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>115 <span class="nb">time</span><span class="o">=</span>11.303 ms
^C
<span class="nt">---</span> 8.8.8.8 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max <span class="o">=</span> 11.216/11.818/12.515 ms
<span class="err">$</span>
</code></pre></div></div>

<p>The following diagram summarizes our discussion so far.</p>

<p><img src="/assets/images/k8s-minikube-kindnet.png" alt="" /></p>

<p>This concludes our initial verification of the Kubernetes networking on the minikube with the default CNI (kindnet) which uses the simple <code class="language-plaintext highlighter-rouge">static</code> routes to facilitate communication across nodes. We will look at other CNIs later in other posts.</p>]]></content><author><name>Kubernetes</name></author><category term="Kubernetes" /><category term="minikube" /><summary type="html"><![CDATA[Hello!]]></summary></entry><entry><title type="html">Kubernetes Deployment Rollback</title><link href="https://www.pradeepgadde.com/blog/kubernetes/2022/03/01/kubernetes-rollback.html" rel="alternate" type="text/html" title="Kubernetes Deployment Rollback" /><published>2022-03-01T10:55:04+05:30</published><updated>2022-03-01T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/kubernetes/2022/03/01/kubernetes-rollback</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/kubernetes/2022/03/01/kubernetes-rollback.html"><![CDATA[<h1 id="kubernetes-rollback">Kubernetes Rollback</h1>

<h3 id="rollback">Rollback</h3>

<p>If youâ€™ve decided to undo the current rollout and rollback to the previous revision, you can do so.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout undo deployment/nginx-deployment
deployment.apps/nginx-deployment rolled back
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout status deployment nginx-deployment
deployment <span class="s2">"nginx-deployment"</span> successfully rolled out
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         image updated to 1.21
3         &lt;none&gt;
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 <span class="nv">app</span><span class="o">=</span>nginx
Annotations:            deployment.kubernetes.io/revision: 3
Selector:               <span class="nv">app</span><span class="o">=</span>nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  <span class="nv">app</span><span class="o">=</span>nginx
  Containers:
   nginx:
    Image:        nginx:1.20
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  <span class="nt">----</span>           <span class="nt">------</span>  <span class="nt">------</span>
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-7b96fbf5d8 <span class="o">(</span>3/3 replicas created<span class="o">)</span>
Events:
  Type     Reason                 Age                From                   Message
  <span class="nt">----</span>     <span class="nt">------</span>                 <span class="nt">----</span>               <span class="nt">----</span>                   <span class="nt">-------</span>
  Warning  ReplicaSetCreateError  20m                deployment-controller  Failed to create new replica <span class="nb">set</span> <span class="s2">"nginx-deployment-7b96fbf5d8"</span>: Unauthorized
  Normal   ScalingReplicaSet      13m                deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 0
  Normal   ScalingReplicaSet      50s                deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      48s                deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      48s                deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      46s                deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      45s <span class="o">(</span>x2 over 20m<span class="o">)</span>  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      43s                deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 0
</code></pre></div></div>
<p>Let us annotate revision 3.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl annotate deployments.apps nginx-deployment kubernetes.io/change-cause<span class="o">=</span><span class="s2">"image rolledback to 1.20"</span>
deployment.apps/nginx-deployment annotated
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         image updated to 1.21
3         image rolledback to 1.20
</code></pre></div></div>

<p>What happens if we rollback the deployment again?</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout undo deployment/nginx-deployment
deployment.apps/nginx-deployment rolled back
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
3         image rolledback to 1.20
4         image updated to 1.21
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment/nginx-deployment <span class="nt">--revision</span><span class="o">=</span>4
deployment.apps/nginx-deployment with revision <span class="c">#4</span>
Pod Template:
  Labels:	<span class="nv">app</span><span class="o">=</span>nginx
	pod-template-hash<span class="o">=</span>5778cd94ff
  Annotations:	kubernetes.io/change-cause: image updated to 1.21
  Containers:
   nginx:
    Image:	nginx:1.21
    Port:	80/TCP
    Host Port:	0/TCP
    Environment:	&lt;none&gt;
    Mounts:	&lt;none&gt;
  Volumes:	&lt;none&gt;
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 <span class="nv">app</span><span class="o">=</span>nginx
Annotations:            deployment.kubernetes.io/revision: 4
                        kubernetes.io/change-cause: image updated to 1.21
Selector:               <span class="nv">app</span><span class="o">=</span>nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  <span class="nv">app</span><span class="o">=</span>nginx
  Containers:
   nginx:
    Image:        nginx:1.21
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  <span class="nt">----</span>           <span class="nt">------</span>  <span class="nt">------</span>
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-5778cd94ff <span class="o">(</span>3/3 replicas created<span class="o">)</span>
Events:
  Type     Reason                 Age                    From                   Message
  <span class="nt">----</span>     <span class="nt">------</span>                 <span class="nt">----</span>                   <span class="nt">----</span>                   <span class="nt">-------</span>
  Warning  ReplicaSetCreateError  31m                    deployment-controller  Failed to create new replica <span class="nb">set</span> <span class="s2">"nginx-deployment-7b96fbf5d8"</span>: Unauthorized
  Normal   ScalingReplicaSet      23m                    deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      23m                    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      23m                    deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 0
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      11m <span class="o">(</span>x2 over 31m<span class="o">)</span>      deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 0
  Normal   ScalingReplicaSet      2m27s <span class="o">(</span>x2 over 24m<span class="o">)</span>    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      2m25s <span class="o">(</span>x2 over 23m<span class="o">)</span>    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      2m25s <span class="o">(</span>x2 over 23m<span class="o">)</span>    deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      2m20s <span class="o">(</span>x3 over 2m23s<span class="o">)</span>  deployment-controller  <span class="o">(</span>combined from similar events<span class="o">)</span>: Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 0
</code></pre></div></div>

<p>Let us update the deployment one more time, finally to the latest version.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">set </span>image deployment/nginx-deployment <span class="nv">nginx</span><span class="o">=</span>nginx:latest
deployment.apps/nginx-deployment image updated
</code></pre></div></div>
<p>Check the rollout history, and note that the existing annotation continued to the latest revision.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
3         image rolledback to 1.20
4         image updated to 1.21
5         image updated to 1.21
</code></pre></div></div>
<p>Change the annotation</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl annotate deployments.apps nginx-deployment kubernetes.io/change-cause<span class="o">=</span><span class="s2">"image updated to the latest"</span>
deployment.apps/nginx-deployment annotated
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
3         image rolledback to 1.20
4         image updated to 1.21
5         image updated to the latest
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 <span class="nv">app</span><span class="o">=</span>nginx
Annotations:            deployment.kubernetes.io/revision: 5
                        kubernetes.io/change-cause: image updated to the latest
Selector:               <span class="nv">app</span><span class="o">=</span>nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  <span class="nv">app</span><span class="o">=</span>nginx
  Containers:
   nginx:
    Image:        nginx:latest
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  <span class="nt">----</span>           <span class="nt">------</span>  <span class="nt">------</span>
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-67dffbbbb <span class="o">(</span>3/3 replicas created<span class="o">)</span>
Events:
  Type     Reason                 Age                  From                   Message
  <span class="nt">----</span>     <span class="nt">------</span>                 <span class="nt">----</span>                 <span class="nt">----</span>                   <span class="nt">-------</span>
  Warning  ReplicaSetCreateError  40m                  deployment-controller  Failed to create new replica <span class="nb">set</span> <span class="s2">"nginx-deployment-7b96fbf5d8"</span>: Unauthorized
  Normal   ScalingReplicaSet      32m                  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      32m                  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      32m                  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 0
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      20m <span class="o">(</span>x2 over 40m<span class="o">)</span>    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 0
  Normal   ScalingReplicaSet      11m <span class="o">(</span>x2 over 33m<span class="o">)</span>    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      11m <span class="o">(</span>x2 over 33m<span class="o">)</span>    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      11m <span class="o">(</span>x2 over 33m<span class="o">)</span>    deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      2m52s <span class="o">(</span>x9 over 11m<span class="o">)</span>  deployment-controller  <span class="o">(</span>combined from similar events<span class="o">)</span>: Scaled down replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 0
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods
NAME                               READY   STATUS                   RESTARTS         AGE
kodekloud-8477b7849-blzrs          1/1     Running                  1                13d
kodekloud-8477b7849-m65m8          1/1     Running                  1                13d
kodekloud-8477b7849-p7psw          1/1     Running                  0                32h
kodekloud-8477b7849-vf9zs          1/1     Running                  0                32h
kodekloud-cm                       0/1     ContainerStatusUnknown   1                13d
nginx-deployment-67dffbbbb-7ttxq   1/1     Running                  0                3m35s
nginx-deployment-67dffbbbb-8dtsf   1/1     Running                  0                3m38s
nginx-deployment-67dffbbbb-sz5bb   1/1     Running                  0                3m40s
pod-with-hostpath-volume           0/1     ContainerCreating        1                8d
pod-with-init-container            1/1     Running                  45 <span class="o">(</span>4m46s ago<span class="o">)</span>   12d
security-context-demo-cap          1/1     Running                  15 <span class="o">(</span>5m16s ago<span class="o">)</span>   9d
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
kodekloud-589c9f4b47          0         0         0       13d
kodekloud-676c6b9fcd          0         0         0       13d
kodekloud-8477b7849           4         4         4       13d
nginx-deployment-5778cd94ff   0         0         0       34m
nginx-deployment-67dffbbbb    3         3         3       3m52s
nginx-deployment-7b96fbf5d8   0         0         0       41m
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment nginx-deployment <span class="nt">--revision</span><span class="o">=</span>5
deployment.apps/nginx-deployment with revision <span class="c">#5</span>
Pod Template:
  Labels:	<span class="nv">app</span><span class="o">=</span>nginx
	pod-template-hash<span class="o">=</span>67dffbbbb
  Annotations:	kubernetes.io/change-cause: image updated to the latest
  Containers:
   nginx:
    Image:	nginx:latest
    Port:	80/TCP
    Host Port:	0/TCP
    Environment:	&lt;none&gt;
    Mounts:	&lt;none&gt;
  Volumes:	&lt;none&gt;
</code></pre></div></div>]]></content><author><name>Kubernetes</name></author><category term="Kubernetes" /><category term="minikube" /><summary type="html"><![CDATA[Kubernetes Rollback]]></summary></entry><entry><title type="html">Kubectl Annotate</title><link href="https://www.pradeepgadde.com/blog/kubernetes/2022/03/01/kubectl-annotate.html" rel="alternate" type="text/html" title="Kubectl Annotate" /><published>2022-03-01T10:55:04+05:30</published><updated>2022-03-01T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/kubernetes/2022/03/01/kubectl-annotate</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/kubernetes/2022/03/01/kubectl-annotate.html"><![CDATA[<h1 id="kubectl-annotate">Kubectl annotate</h1>

<h3 id="kubectl-annotate-1">Kubectl Annotate</h3>

<p>Annotations are like labels (key/value pairs), they store arbitrary string values.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl annotate <span class="nt">-h</span>
Update the annotations on one or more resources.

 All Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are
key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and
system extensions may use annotations to store their own data.

 Attempting to <span class="nb">set </span>an annotation that already exists will fail unless <span class="nt">--overwrite</span> is set. If <span class="nt">--resource-version</span> is
specified and does not match the current resource version on the server the <span class="nb">command </span>will fail.

Use <span class="s2">"kubectl api-resources"</span> <span class="k">for </span>a <span class="nb">complete </span>list of supported resources.

Examples:
  <span class="c"># Update pod 'foo' with the annotation 'description' and the value 'my frontend'</span>
  <span class="c"># If the same annotation is set multiple times, only the last value will be applied</span>
  kubectl annotate pods foo <span class="nv">description</span><span class="o">=</span><span class="s1">'my frontend'</span>

  <span class="c"># Update a pod identified by type and name in "pod.json"</span>
  kubectl annotate <span class="nt">-f</span> pod.json <span class="nv">description</span><span class="o">=</span><span class="s1">'my frontend'</span>

  <span class="c"># Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any</span>
existing value
  kubectl annotate <span class="nt">--overwrite</span> pods foo <span class="nv">description</span><span class="o">=</span><span class="s1">'my frontend running nginx'</span>

  <span class="c"># Update all pods in the namespace</span>
  kubectl annotate pods <span class="nt">--all</span> <span class="nv">description</span><span class="o">=</span><span class="s1">'my frontend running nginx'</span>

  <span class="c"># Update pod 'foo' only if the resource is unchanged from version 1</span>
  kubectl annotate pods foo <span class="nv">description</span><span class="o">=</span><span class="s1">'my frontend running nginx'</span> <span class="nt">--resource-version</span><span class="o">=</span>1

  <span class="c"># Update pod 'foo' by removing an annotation named 'description' if it exists</span>
  <span class="c"># Does not require the --overwrite flag</span>
  kubectl annotate pods foo description-

Options:
      <span class="nt">--all</span><span class="o">=</span><span class="nb">false</span>: Select all resources, <span class="k">in </span>the namespace of the specified resource types.
  <span class="nt">-A</span>, <span class="nt">--all-namespaces</span><span class="o">=</span><span class="nb">false</span>: If <span class="nb">true</span>, check the specified action <span class="k">in </span>all namespaces.
      <span class="nt">--allow-missing-template-keys</span><span class="o">=</span><span class="nb">true</span>: If <span class="nb">true</span>, ignore any errors <span class="k">in </span>templates when a field or map key is missing <span class="k">in
</span>the template. Only applies to golang and jsonpath output formats.
      <span class="nt">--dry-run</span><span class="o">=</span><span class="s1">'none'</span>: Must be <span class="s2">"none"</span>, <span class="s2">"server"</span>, or <span class="s2">"client"</span><span class="nb">.</span> If client strategy, only print the object that would be
sent, without sending it. If server strategy, submit server-side request without persisting the resource.
      <span class="nt">--field-manager</span><span class="o">=</span><span class="s1">'kubectl-annotate'</span>: Name of the manager used to track field ownership.
      <span class="nt">--field-selector</span><span class="o">=</span><span class="s1">''</span>: Selector <span class="o">(</span>field query<span class="o">)</span> to filter on, supports <span class="s1">'='</span>, <span class="s1">'=='</span>, and <span class="s1">'!='</span>.<span class="o">(</span>e.g. <span class="nt">--field-selector</span>
<span class="nv">key1</span><span class="o">=</span>value1,key2<span class="o">=</span>value2<span class="o">)</span><span class="nb">.</span> The server only supports a limited number of field queries per type.
  <span class="nt">-f</span>, <span class="nt">--filename</span><span class="o">=[]</span>: Filename, directory, or URL to files identifying the resource to update the annotation
  <span class="nt">-k</span>, <span class="nt">--kustomize</span><span class="o">=</span><span class="s1">''</span>: Process the kustomization directory. This flag can<span class="s1">'t be used together with -f or -R.
      --list=false: If true, display the annotations for a given resource.
      --local=false: If true, annotation will NOT contact api-server but run locally.
  -o, --output='': Output format. One of:
json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.
      --overwrite=false: If true, allow annotations to be overwritten, otherwise reject annotation updates that
overwrite existing annotations.
  -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage
related manifests organized within the same directory.
      --resource-version='': If non-empty, the annotation update will only succeed if this is the current
resource-version for the object. Only valid when specifying a single resource.
  -l, --selector='': Selector (label query) to filter on, supports '</span><span class="o">=</span><span class="s1">', '</span><span class="o">==</span><span class="s1">', and '</span><span class="o">!=</span><span class="s1">'.(e.g. -l
key1=value1,key2=value2).
      --show-managed-fields=false: If true, keep the managedFields when printing objects in JSON or YAML format.
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

Usage:
  kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
[options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
</span></code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause<span class="o">=</span><span class="s2">"image updated to 1.21"</span>
deployment.apps/nginx-deployment annotated
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         image updated to 1.21
</code></pre></div></div>
<p>Look at the Annotations section of the description.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 <span class="nv">app</span><span class="o">=</span>nginx
Annotations:            deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: image updated to 1.21
Selector:               <span class="nv">app</span><span class="o">=</span>nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  <span class="nv">app</span><span class="o">=</span>nginx
  Containers:
   nginx:
    Image:        nginx:1.21
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  <span class="nt">----</span>           <span class="nt">------</span>  <span class="nt">------</span>
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-5778cd94ff <span class="o">(</span>3/3 replicas created<span class="o">)</span>
Events:
  Type     Reason                 Age    From                   Message
  <span class="nt">----</span>     <span class="nt">------</span>                 <span class="nt">----</span>   <span class="nt">----</span>                   <span class="nt">-------</span>
  Warning  ReplicaSetCreateError  14m    deployment-controller  Failed to create new replica <span class="nb">set</span> <span class="s2">"nginx-deployment-7b96fbf5d8"</span>: Unauthorized
  Normal   ScalingReplicaSet      14m    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      6m57s  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      6m51s  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      6m51s  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      6m49s  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      6m49s  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      6m42s  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 0
</code></pre></div></div>

<p>We can see the details of a particular revision using the <code class="language-plaintext highlighter-rouge">--revision</code>  option.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment/nginx-deployment <span class="nt">--revision</span><span class="o">=</span>1
deployment.apps/nginx-deployment with revision <span class="c">#1</span>
Pod Template:
  Labels:	<span class="nv">app</span><span class="o">=</span>nginx
	pod-template-hash<span class="o">=</span>7b96fbf5d8
  Containers:
   nginx:
    Image:	nginx:1.20
    Port:	80/TCP
    Host Port:	0/TCP
    Environment:	&lt;none&gt;
    Mounts:	&lt;none&gt;
  Volumes:	&lt;none&gt;
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment/nginx-deployment <span class="nt">--revision</span><span class="o">=</span>2
deployment.apps/nginx-deployment with revision <span class="c">#2</span>
Pod Template:
  Labels:	<span class="nv">app</span><span class="o">=</span>nginx
	pod-template-hash<span class="o">=</span>5778cd94ff
  Annotations:	kubernetes.io/change-cause: image updated to 1.21
  Containers:
   nginx:
    Image:	nginx:1.21
    Port:	80/TCP
    Host Port:	0/TCP
    Environment:	&lt;none&gt;
    Mounts:	&lt;none&gt;
  Volumes:	&lt;none&gt;
</code></pre></div></div>]]></content><author><name>Kubernetes</name></author><category term="Kubernetes" /><category term="minikube" /><summary type="html"><![CDATA[Kubectl annotate]]></summary></entry><entry><title type="html">Kubectl Rollout</title><link href="https://www.pradeepgadde.com/blog/kubernetes/2022/03/01/kubernetes-rollout.html" rel="alternate" type="text/html" title="Kubectl Rollout" /><published>2022-03-01T10:55:04+05:30</published><updated>2022-03-01T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/kubernetes/2022/03/01/kubernetes-rollout</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/kubernetes/2022/03/01/kubernetes-rollout.html"><![CDATA[<h1 id="kubectl-rollout">Kubectl Rollout</h1>

<h3 id="rollout">Rollout</h3>

<p>Let us take a look at the current deployments.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get deployment
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kodekloud   4/4     4            4           13d
</code></pre></div></div>

<p>To manage the rollout of a resource like deployments, kubernetes provides the <code class="language-plaintext highlighter-rouge">rollout</code> option, with which we can view rollout history, see the status of the rollout and even undo a previous rollout.</p>

<p>Here is the detailed help information.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nt">-h</span>
Manage the rollout of a resource.

 Valid resource types include:

  <span class="k">*</span>  deployments
  <span class="k">*</span>  daemonsets
  <span class="k">*</span>  statefulsets

Examples:
  <span class="c"># Rollback to the previous deployment</span>
  kubectl rollout undo deployment/abc

  <span class="c"># Check the rollout status of a daemonset</span>
  kubectl rollout status daemonset/foo

Available Commands:
  <span class="nb">history     </span>View rollout <span class="nb">history
  </span>pause       Mark the provided resource as paused
  restart     Restart a resource
  resume      Resume a paused resource
  status      Show the status of the rollout
  undo        Undo a previous rollout

Usage:
  kubectl rollout SUBCOMMAND <span class="o">[</span>options]

Use <span class="s2">"kubectl &lt;command&gt; --help"</span> <span class="k">for </span>more information about a given command.
Use <span class="s2">"kubectl options"</span> <span class="k">for </span>a list of global command-line options <span class="o">(</span>applies to all commands<span class="o">)</span><span class="nb">.</span>
</code></pre></div></div>
<p>Let us use this option to view the history of our <code class="language-plaintext highlighter-rouge">kodekloud</code> deployment.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout <span class="nb">history </span>deployment kodekloud
deployment.apps/kodekloud
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
3         &lt;none&gt;
4         &lt;none&gt;
</code></pre></div></div>

<p>We do see that there are multiple revisions to this <code class="language-plaintext highlighter-rouge">kodekloud</code> deployment, but we do not see any <code class="language-plaintext highlighter-rouge">CHANGE-CAUSE</code>.</p>

<p>Even in the description, we do see only one line related to <code class="language-plaintext highlighter-rouge">deployment.kubernetes.io/revision</code>. Pay attention to the <code class="language-plaintext highlighter-rouge">Annotations</code> section.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe deployments.apps
Name:               kodekloud
Namespace:          default
CreationTimestamp:  Tue, 15 Feb 2022 12:43:38 +0530
Labels:             <span class="nv">app</span><span class="o">=</span>kodekloud
Annotations:        deployment.kubernetes.io/revision: 4
Selector:           <span class="nv">app</span><span class="o">=</span>kodekloud
Replicas:           4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:       Recreate
MinReadySeconds:    0
Pod Template:
  Labels:  <span class="nv">app</span><span class="o">=</span>kodekloud
  Containers:
   webapp-color:
    Image:        kodekloud/webapp-color:v2
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  <span class="nt">----</span>           <span class="nt">------</span>  <span class="nt">------</span>
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   kodekloud-8477b7849 <span class="o">(</span>4/4 replicas created<span class="o">)</span>
Events:          &lt;none&gt;
</code></pre></div></div>

<p>Create a new deployment</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat nginx-deployment.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nginx-deployment</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">nginx</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:1.20</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> nginx-deployment.yaml
deployment.apps/nginx-deployment created
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get deployments.apps
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
kodekloud          4/4     4            4           13d
nginx-deployment   3/3     3            3           31s
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 <span class="nv">app</span><span class="o">=</span>nginx
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               <span class="nv">app</span><span class="o">=</span>nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  <span class="nv">app</span><span class="o">=</span>nginx
  Containers:
   nginx:
    Image:        nginx:1.20
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  <span class="nt">----</span>           <span class="nt">------</span>  <span class="nt">------</span>
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-7b96fbf5d8 <span class="o">(</span>3/3 replicas created<span class="o">)</span>
Events:
  Type     Reason                 Age   From                   Message
  <span class="nt">----</span>     <span class="nt">------</span>                 <span class="nt">----</span>  <span class="nt">----</span>                   <span class="nt">-------</span>
  Warning  ReplicaSetCreateError  33s   deployment-controller  Failed to create new replica <span class="nb">set</span> <span class="s2">"nginx-deployment-7b96fbf5d8"</span>: Unauthorized
  Normal   ScalingReplicaSet      33s   deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 3
</code></pre></div></div>

<h3 id="update-deployment-using-kubectl-set">Update Deployment (using Kubectl Set)</h3>

<p>Earlier we have used <code class="language-plaintext highlighter-rouge">kubectl edit</code> to make changes to an existing deployment.</p>

<p>Here we will explore another option <code class="language-plaintext highlighter-rouge">kubectl set</code>.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">set</span> <span class="nt">-h</span>
Configure application resources.

 These commands <span class="nb">help </span>you make changes to existing application resources.

Available Commands:
  <span class="nb">env            </span>Update environment variables on a pod template
  image          Update the image of a pod template
  resources      Update resource requests/limits on objects with pod templates
  selector       Set the selector on a resource
  serviceaccount Update the service account of a resource
  subject        Update the user, group, or service account <span class="k">in </span>a role binding or cluster role binding

Usage:
  kubectl <span class="nb">set </span>SUBCOMMAND <span class="o">[</span>options]

Use <span class="s2">"kubectl &lt;command&gt; --help"</span> <span class="k">for </span>more information about a given command.
Use <span class="s2">"kubectl options"</span> <span class="k">for </span>a list of global command-line options <span class="o">(</span>applies to all commands<span class="o">)</span><span class="nb">.</span>
</code></pre></div></div>
<p>Let us set the image for our <code class="language-plaintext highlighter-rouge">nginx-deployment</code> to a new version <code class="language-plaintext highlighter-rouge">1.21</code>.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">set </span>image deployment/nginx-deployment <span class="nv">nginx</span><span class="o">=</span>nginx:1.21
deployment.apps/nginx-deployment image updated
</code></pre></div></div>
<p>We can see the rollout status like this, using the <code class="language-plaintext highlighter-rouge">kubectl rollout status</code> command.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl rollout status deployment/nginx-deployment

deployment <span class="s2">"nginx-deployment"</span> successfully rolled out
</code></pre></div></div>
<p>Describe the deployment</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 <span class="nv">app</span><span class="o">=</span>nginx
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               <span class="nv">app</span><span class="o">=</span>nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  <span class="nv">app</span><span class="o">=</span>nginx
  Containers:
   nginx:
    Image:        nginx:1.21
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  <span class="nt">----</span>           <span class="nt">------</span>  <span class="nt">------</span>
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-5778cd94ff <span class="o">(</span>3/3 replicas created<span class="o">)</span>
Events:
  Type     Reason                 Age    From                   Message
  <span class="nt">----</span>     <span class="nt">------</span>                 <span class="nt">----</span>   <span class="nt">----</span>                   <span class="nt">-------</span>
  Warning  ReplicaSetCreateError  10m    deployment-controller  Failed to create new replica <span class="nb">set</span> <span class="s2">"nginx-deployment-7b96fbf5d8"</span>: Unauthorized
  Normal   ScalingReplicaSet      10m    deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      3m18s  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      3m12s  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      3m12s  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      3m10s  deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      3m10s  deployment-controller  Scaled up replica <span class="nb">set </span>nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      3m3s   deployment-controller  Scaled down replica <span class="nb">set </span>nginx-deployment-7b96fbf5d8 to 0
</code></pre></div></div>]]></content><author><name>Kubernetes</name></author><category term="Kubernetes" /><category term="minikube" /><summary type="html"><![CDATA[Kubectl Rollout]]></summary></entry><entry><title type="html">Kubernetes Storage Class</title><link href="https://www.pradeepgadde.com/blog/kubernetes/2022/02/28/kubernetes-storageclass.html" rel="alternate" type="text/html" title="Kubernetes Storage Class" /><published>2022-02-28T10:55:04+05:30</published><updated>2022-02-28T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/kubernetes/2022/02/28/kubernetes-storageclass</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/kubernetes/2022/02/28/kubernetes-storageclass.html"><![CDATA[<h1 id="kubernetes-storageclass">Kubernetes StorageClass</h1>

<h3 id="storageclass">StorageClass</h3>

<p>In the previous example, we used a storageClass named <code class="language-plaintext highlighter-rouge">manual</code>, but that does not seem to be present.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get sc
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
standard <span class="o">(</span>default<span class="o">)</span>   k8s.io/minikube-hostpath   Delete          Immediate           <span class="nb">false                  </span>12d
</code></pre></div></div>

<p>There is a storage class by name standard, and is the default storage class.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe sc
Name:            standard
IsDefaultClass:  Yes
Annotations:     kubectl.kubernetes.io/last-applied-configuration<span class="o">={</span><span class="s2">"apiVersion"</span>:<span class="s2">"storage.k8s.io/v1"</span>,<span class="s2">"kind"</span>:<span class="s2">"StorageClass"</span>,<span class="s2">"metadata"</span>:<span class="o">{</span><span class="s2">"annotations"</span>:<span class="o">{</span><span class="s2">"storageclass.kubernetes.io/is-default-class"</span>:<span class="s2">"true"</span><span class="o">}</span>,<span class="s2">"labels"</span>:<span class="o">{</span><span class="s2">"addonmanager.kubernetes.io/mode"</span>:<span class="s2">"EnsureExists"</span><span class="o">}</span>,<span class="s2">"name"</span>:<span class="s2">"standard"</span><span class="o">}</span>,<span class="s2">"provisioner"</span>:<span class="s2">"k8s.io/minikube-hostpath"</span><span class="o">}</span>
,storageclass.kubernetes.io/is-default-class<span class="o">=</span><span class="nb">true
</span>Provisioner:           k8s.io/minikube-hostpath
Parameters:            &lt;none&gt;
AllowVolumeExpansion:  &lt;<span class="nb">unset</span><span class="o">&gt;</span>
MountOptions:          &lt;none&gt;
ReclaimPolicy:         Delete
VolumeBindingMode:     Immediate
Events:                &lt;none&gt;
</code></pre></div></div>
<p>:memo: Changing the storage class name to <code class="language-plaintext highlighter-rouge">standard</code> also did not help solve the Forbidden issue with the <code class="language-plaintext highlighter-rouge">nginx</code>.</p>

<p>Strangely, inside the Pod, we can access the <code class="language-plaintext highlighter-rouge">index.html</code> file with <code class="language-plaintext highlighter-rouge">cat</code> command, but <code class="language-plaintext highlighter-rouge">curl</code> is not working.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> my-pv-pod <span class="nt">--</span> /bin/bash
root@my-pv-pod:/# <span class="nb">cat</span> /usr/share/nginx/html/indext.html
Hello from Kubernetes storage
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl explain sc
KIND:     StorageClass
VERSION:  storage.k8s.io/v1

DESCRIPTION:
     StorageClass describes the parameters <span class="k">for </span>a class of storage <span class="k">for </span>which
     PersistentVolumes can be dynamically provisioned.

     StorageClasses are non-namespaced<span class="p">;</span> the name of the storage class according
     to etcd is <span class="k">in </span>ObjectMeta.Name.

FIELDS:
   allowVolumeExpansion	&lt;boolean&gt;
     AllowVolumeExpansion shows whether the storage class allow volume <span class="nb">expand

   </span>allowedTopologies	&lt;<span class="o">[]</span>Object&gt;
     Restrict the node topologies where volumes can be dynamically provisioned.
     Each volume plugin defines its own supported topology specifications. An
     empty TopologySelectorTerm list means there is no topology restriction.
     This field is only honored by servers that <span class="nb">enable </span>the VolumeScheduling
     feature.

   apiVersion	&lt;string&gt;
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind	&lt;string&gt;
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata	&lt;Object&gt;
     Standard object<span class="s1">'s metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   mountOptions	&lt;[]string&gt;
     Dynamically provisioned PersistentVolumes of this storage class are created
     with these mountOptions, e.g. ["ro", "soft"]. Not validated - mount of the
     PVs will simply fail if one is invalid.

   parameters	&lt;map[string]string&gt;
     Parameters holds the parameters for the provisioner that should create
     volumes of this storage class.

   provisioner	&lt;string&gt; -required-
     Provisioner indicates the type of the provisioner.

   reclaimPolicy	&lt;string&gt;
     Dynamically provisioned PersistentVolumes of this storage class are created
     with this reclaimPolicy. Defaults to Delete.

   volumeBindingMode	&lt;string&gt;
     VolumeBindingMode indicates how PersistentVolumeClaims should be
     provisioned and bound. When unset, VolumeBindingImmediate is used. This
     field is only honored by servers that enable the VolumeScheduling feature.
</span></code></pre></div></div>

<h3 id="static-binding">Static Binding</h3>

<p>Here is the PersistentVolume definition. If we omit the storageClassName, PVC is net getting bound. As you see, there is a default storageClass named <code class="language-plaintext highlighter-rouge">standard</code> and PVC is using it by default.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span><span class="nb">cat </span>pv.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv
spec:
  storageClassName: standard
  capacity:
    storage: 512m
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /data/config
</code></pre></div></div>

<p>Create the PV from the YAML file.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> pv.yaml
persistentvolume/pv created
</code></pre></div></div>

<p>Verify the available PVs. The newly created PV should be in <code class="language-plaintext highlighter-rouge">Available</code> state. Currently, there are no Claims for this volume.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv     512m       RWX            Retain           Available           standard                8s
</code></pre></div></div>
<p>Describe the PV for addtional details.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pv pv
Name:            pv
Labels:          &lt;none&gt;
Annotations:     &lt;none&gt;
Finalizers:      <span class="o">[</span>kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Available
Claim:
Reclaim Policy:  Retain
Access Modes:    RWX
VolumeMode:      Filesystem
Capacity:        512m
Node Affinity:   &lt;none&gt;
Message:
Source:
    Type:          HostPath <span class="o">(</span>bare host directory volume<span class="o">)</span>
    Path:          /data/config
    HostPathType:
Events:            &lt;none&gt;
</code></pre></div></div>
<p>Now, create a PVC with same <code class="language-plaintext highlighter-rouge">accessModes: ReadWriteMany</code>.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat pvc.yaml</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pvc</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">256m</span>
</code></pre></div></div>
<p>Create PVC from the YAML file.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> pvc.yaml
persistentvolumeclaim/pvc created
</code></pre></div></div>
<p>Verify the newly created PVC. It should be in <code class="language-plaintext highlighter-rouge">Bound</code> state. Note the <code class="language-plaintext highlighter-rouge">STORAGECLASS</code> column. It is using the default one named <code class="language-plaintext highlighter-rouge">standard</code>.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pvc
NAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc    Bound    pv       512m       RWX            standard       4s
</code></pre></div></div>
<p>Describe the PVC for addtional details.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pvc
Name:          pvc
Namespace:     default
StorageClass:  standard
Status:        Bound
Volume:        pv
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: <span class="nb">yes
               </span>pv.kubernetes.io/bound-by-controller: <span class="nb">yes
</span>Finalizers:    <span class="o">[</span>kubernetes.io/pvc-protection]
Capacity:      512m
Access Modes:  RWX
VolumeMode:    Filesystem
Used By:       &lt;none&gt;
Events:        &lt;none&gt;
</code></pre></div></div>
<p>Now, create a Pod that uses the newly defined PVC.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat pod.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/data/app/config"</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">configpvc</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">configpvc</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">pvc</span>
  <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">Never</span>
</code></pre></div></div>
<p>Create the Pod from the YAML file.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> pod.yaml
pod/app created
</code></pre></div></div>
<p>Use the <code class="language-plaintext highlighter-rouge">-o wide</code> option to check the IP and NODE details.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods app <span class="nt">-o</span> wide
NAME   READY   STATUS    RESTARTS   AGE   IP           NODE      NOMINATED NODE   READINESS GATES
app    1/1     Running   0          18s   10.244.1.5   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>
<p>Describe the Pod and look for Volumes section.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pods app
Name:         app
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Mon, 28 Feb 2022 09:25:46 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.5
IPs:
  IP:  10.244.1.5
Containers:
  app:
    Container ID:   docker://09871149e09a1da02a86a9e38ace82082def59aa38dffbf739e14f7694f0f79e
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Mon, 28 Feb 2022 09:25:50 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /data/app/config from configpvc <span class="o">(</span>rw<span class="o">)</span>
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vfx8b <span class="o">(</span>ro<span class="o">)</span>
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  configpvc:
    Type:       PersistentVolumeClaim <span class="o">(</span>a reference to a PersistentVolumeClaim <span class="k">in </span>the same namespace<span class="o">)</span>
    ClaimName:  pvc
    ReadOnly:   <span class="nb">false
  </span>kube-api-access-vfx8b:
    Type:                    Projected <span class="o">(</span>a volume that contains injected data from multiple sources<span class="o">)</span>
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             <span class="nb">true
</span>QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute <span class="nv">op</span><span class="o">=</span>Exists <span class="k">for </span>300s
                             node.kubernetes.io/unreachable:NoExecute <span class="nv">op</span><span class="o">=</span>Exists <span class="k">for </span>300s
Events:
  Type    Reason     Age   From               Message
  <span class="nt">----</span>    <span class="nt">------</span>     <span class="nt">----</span>  <span class="nt">----</span>               <span class="nt">-------</span>
  Normal  Scheduled  63s   default-scheduler  Successfully assigned default/app to k8s-m02
  Normal  Pulling    62s   kubelet            Pulling image <span class="s2">"nginx"</span>
  Normal  Pulled     59s   kubelet            Successfully pulled image <span class="s2">"nginx"</span> <span class="k">in </span>3.044922832s
  Normal  Created    59s   kubelet            Created container app
  Normal  Started    59s   kubelet            Started container app
</code></pre></div></div>
<p>Login to the Container and create a file in the <code class="language-plaintext highlighter-rouge">/data/app/config</code> folder, which is the container MountPath.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> app <span class="nt">--</span> /bin/sh
<span class="c"># cd /data/app/config</span>
<span class="c"># ls -l</span>
total 0
<span class="c"># touch hello.txt</span>
<span class="c"># echo "Testing PV, PVC, and SC in K8S!" &gt; hello.txt</span>
<span class="c"># exit</span>
</code></pre></div></div>
<p>Login to the minikube node and check if you are able to view the <code class="language-plaintext highlighter-rouge">hello.txt</code> file in the host.
On the control plane, there is no such file in the <code class="language-plaintext highlighter-rouge">/data/config</code> folder.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-p</span> k8s
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ls /data/config/
$ ls -la /data/config/
total 8
drwxr-xr-x 2 root root 4096 Feb 28 03:33 .
drwxr-xr-x 3 root root 4096 Feb 28 03:33 ..
$ exit
logout
</span></code></pre></div></div>
<p>On the other node, <code class="language-plaintext highlighter-rouge">k8s-m02</code>,  there is the <code class="language-plaintext highlighter-rouge">hello.txt</code> file.  This is because, the <code class="language-plaintext highlighter-rouge">app</code> pod got created in this node.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> k8s-m02 <span class="nt">-p</span> k8s
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ ls /data/config
hello.txt
$ ls -la /data/config/
total 12
drwxr-xr-x 2 root root 4096 Feb 28 03:58 .
drwxr-xr-x 3 root root 4096 Feb 28 03:55 ..
-rw-r--r-- 1 root root   32 Feb 28 03:58 hello.txt
$ cat /data/config/hello.txt
Testing PV, PVC, and SC in K8S!
$ exit
logout
</span></code></pre></div></div>
<p>We can also view the contents of the <code class="language-plaintext highlighter-rouge">hello.txt</code> file.</p>

<p>Verify the PV, PVC, and SC details.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pv,pvc,sc
NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE
persistentvolume/pv   512m       RWX            Retain           Bound    default/pvc   standard                9m35s

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/pvc   Bound    pv       512m       RWX            standard       8m17s

NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
storageclass.storage.k8s.io/standard <span class="o">(</span>default<span class="o">)</span>   k8s.io/minikube-hostpath   Delete          Immediate           <span class="nb">false                  </span>12d
</code></pre></div></div>
<p>Describe the PV one more time, after the Pod creation, to see if there is any change.
One change that we notice is that the Claim is showing <code class="language-plaintext highlighter-rouge">default/pvc</code> which was empty earlier. Also, Staus changed to <code class="language-plaintext highlighter-rouge">Bound</code> from <code class="language-plaintext highlighter-rouge">Available</code>.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pv
Name:            pv
Labels:          &lt;none&gt;
Annotations:     pv.kubernetes.io/bound-by-controller: <span class="nb">yes
</span>Finalizers:      <span class="o">[</span>kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Bound
Claim:           default/pvc
Reclaim Policy:  Retain
Access Modes:    RWX
VolumeMode:      Filesystem
Capacity:        512m
Node Affinity:   &lt;none&gt;
Message:
Source:
    Type:          HostPath <span class="o">(</span>bare host directory volume<span class="o">)</span>
    Path:          /data/config
    HostPathType:
Events:            &lt;none&gt;
</code></pre></div></div>
<p>Similarly describe the PVC and look for the changes. The  <code class="language-plaintext highlighter-rouge">Used By</code> value has changed to <code class="language-plaintext highlighter-rouge">app</code> now, showing that it is used by this Pod.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pvc
Name:          pvc
Namespace:     default
StorageClass:  standard
Status:        Bound
Volume:        pv
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: <span class="nb">yes
               </span>pv.kubernetes.io/bound-by-controller: <span class="nb">yes
</span>Finalizers:    <span class="o">[</span>kubernetes.io/pvc-protection]
Capacity:      512m
Access Modes:  RWX
VolumeMode:    Filesystem
Used By:       app
Events:        &lt;none&gt;
</code></pre></div></div>
<p>Also, look at the StorageClass description. Verify that, <code class="language-plaintext highlighter-rouge">IsDefaultClass:  Yes</code> .</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe sc
Name:            standard
IsDefaultClass:  Yes
Annotations:     kubectl.kubernetes.io/last-applied-configuration<span class="o">={</span><span class="s2">"apiVersion"</span>:<span class="s2">"storage.k8s.io/v1"</span>,<span class="s2">"kind"</span>:<span class="s2">"StorageClass"</span>,<span class="s2">"metadata"</span>:<span class="o">{</span><span class="s2">"annotations"</span>:<span class="o">{</span><span class="s2">"storageclass.kubernetes.io/is-default-class"</span>:<span class="s2">"true"</span><span class="o">}</span>,<span class="s2">"labels"</span>:<span class="o">{</span><span class="s2">"addonmanager.kubernetes.io/mode"</span>:<span class="s2">"EnsureExists"</span><span class="o">}</span>,<span class="s2">"name"</span>:<span class="s2">"standard"</span><span class="o">}</span>,<span class="s2">"provisioner"</span>:<span class="s2">"k8s.io/minikube-hostpath"</span><span class="o">}</span>
,storageclass.kubernetes.io/is-default-class<span class="o">=</span><span class="nb">true
</span>Provisioner:           k8s.io/minikube-hostpath
Parameters:            &lt;none&gt;
AllowVolumeExpansion:  &lt;<span class="nb">unset</span><span class="o">&gt;</span>
MountOptions:          &lt;none&gt;
ReclaimPolicy:         Delete
VolumeBindingMode:     Immediate
Events:                &lt;none&gt;
</code></pre></div></div>
<p>Now delete the pod.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl delete pod app
pod <span class="s2">"app"</span> deleted
</code></pre></div></div>
<p>After deleting the pod, check if the data is persistent or not. You should still be able to view the contents of the <code class="language-plaintext highlighter-rouge">hello.txt</code> file on the <code class="language-plaintext highlighter-rouge">k8s-m02</code> node, even after deleting the Pod, confirming that the storage is persistent now.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> k8s-m02 <span class="nt">-p</span> k8s
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ cd /data/config/
$ cat hello.txt
Testing PV, PVC, and SC in K8S!
$ exit
logout
</span></code></pre></div></div>

<h3 id="dynamic-binding">Dynamic Binding</h3>

<p>Define a new storage class named <code class="language-plaintext highlighter-rouge">pradeep-sc-demo</code> in the <code class="language-plaintext highlighter-rouge">kube-system</code> namespace  using a sample YAML file.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat sc.yaml</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pradeep-sc-demo</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">storageclass.beta.kubernetes.io/is-default-class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">false"</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">addonmanager.kubernetes.io/mode</span><span class="pi">:</span> <span class="s">Reconcile</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">k8s.io/minikube-hostpath</span>
</code></pre></div></div>
<p>Create the storage class from this YAML file  and verify that it was created.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> sc.yaml
storageclass.storage.k8s.io/pradeep-sc-demo created
</code></pre></div></div>
<p>Now you should see two storageclasses, both with same PROVISIONER details. Also, not the <code class="language-plaintext highlighter-rouge">default</code> value next to the <code class="language-plaintext highlighter-rouge">standard</code> class, which is not present for the newly defined storage class.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get storageclass
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
pradeep-sc-demo      k8s.io/minikube-hostpath   Delete          Immediate           <span class="nb">false                  </span>47s
standard <span class="o">(</span>default<span class="o">)</span>   k8s.io/minikube-hostpath   Delete          Immediate           <span class="nb">false                  </span>12d
</code></pre></div></div>

<p>Let us modify the PVC definition file to specify this <code class="language-plaintext highlighter-rouge">pradeep-sc-demo</code> StorageClass, instead of the <code class="language-plaintext highlighter-rouge">standard</code>.
First, let us delete the existing PVC.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl delete pvc pvc
persistentvolumeclaim <span class="s2">"pvc"</span> deleted
</code></pre></div></div>
<p>Modify the PVC definition, to include the new storage class.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat pvc.yaml</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pvc</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">pradeep-sc-demo</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">256m</span>
</code></pre></div></div>
<p>Create the PVC from the modified definition file.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> pvc.yaml
persistentvolumeclaim/pvc created
</code></pre></div></div>
<p>Verify the Status of the new PVC.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pvc
NAME   STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
pvc    Pending                                      pradeep-sc-demo   64s
</code></pre></div></div>
<p>It is still showing <code class="language-plaintext highlighter-rouge">Pending</code>, to find out why let us describe it.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pvc
Name:          pvc
Namespace:     default
StorageClass:  pradeep-sc-demo
Status:        Pending
Volume:
Labels:        &lt;none&gt;
Annotations:   volume.beta.kubernetes.io/storage-provisioner: k8s.io/minikube-hostpath
               volume.kubernetes.io/storage-provisioner: k8s.io/minikube-hostpath
Finalizers:    <span class="o">[</span>kubernetes.io/pvc-protection]
Capacity:
Access Modes:
VolumeMode:    Filesystem
Used By:       &lt;none&gt;
Events:
  Type    Reason                Age               From                         Message
  <span class="nt">----</span>    <span class="nt">------</span>                <span class="nt">----</span>              <span class="nt">----</span>                         <span class="nt">-------</span>
  Normal  ExternalProvisioning  2s <span class="o">(</span>x8 over 76s<span class="o">)</span>  persistentvolume-controller  waiting <span class="k">for </span>a volume to be created, either by external provisioner <span class="s2">"k8s.io/minikube-hostpath"</span> or manually created by system administrator
</code></pre></div></div>

<p>We can see that the PVC is waiting for a matching volume to be created.</p>

<p>Our existing PV is using the <code class="language-plaintext highlighter-rouge">standard</code> storageclass which is not matching with this PVC definition. So let us delete the existing PV and modify it to specify the new SC.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl delete pv pv
persistentvolume <span class="s2">"pv"</span> deleted
</code></pre></div></div>
<p>Here is the modified PV definition.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat pv.yaml</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pv</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">pradeep-sc-demo</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">512m</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/data/config</span>
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pv,pvc
NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS      REASON   AGE
persistentvolume/pv   512m       RWX            Retain           Bound    default/pvc   pradeep-sc-demo            15s

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
persistentvolumeclaim/pvc   Bound    pv       512m       RWX            pradeep-sc-demo   6m29s
</code></pre></div></div>
<p>Now that the storageclass is matching, both the PV and PVC are in <code class="language-plaintext highlighter-rouge">Bound</code> State.</p>

<p>Create the <code class="language-plaintext highlighter-rouge">app</code> pod again using the same manifest file that we used earlier.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat pod.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/data/app/config"</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">configpvc</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">configpvc</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">pvc</span>
  <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">Never</span>
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> pod.yaml
pod/app created
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods app <span class="nt">-o</span> wide
NAME   READY   STATUS    RESTARTS   AGE     IP           NODE      NOMINATED NODE   READINESS GATES
app    1/1     Running   0          2m19s   10.244.1.6   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>
<p>Shell into the Pod and create a file in the mounted directory.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec </span>app <span class="nt">-it</span> <span class="nt">--</span> /bin/sh
<span class="c"># cd /data/app/config</span>
<span class="c"># ls -l</span>
total 4
<span class="nt">-rw-r--r--</span> 1 root root 32 Feb 28 03:58 hello.txt
<span class="c"># touch HiAgain.txt</span>
<span class="c"># echo "Hello again from K8s PV,PVC, and SC!" &gt; HiAgain.txt</span>
<span class="c"># cat HiAgain.txt</span>
Hello again from K8s PV,PVC, and SC!
<span class="c"># exit</span>
</code></pre></div></div>
<p>Becuase of persistent nature, we still see the old <code class="language-plaintext highlighter-rouge">hello.txt</code> file in this new container. 
We have created another file called <code class="language-plaintext highlighter-rouge">HiAgain.txt</code> with some sample text.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pv,pvc,sc
NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS      REASON   AGE
persistentvolume/pv   512m       RWX            Retain           Bound    default/pvc   pradeep-sc-demo            6m43s

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
persistentvolumeclaim/pvc   Bound    pv       512m       RWX            pradeep-sc-demo   12m

NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
storageclass.storage.k8s.io/pradeep-sc-demo      k8s.io/minikube-hostpath   Delete          Immediate           <span class="nb">false                  </span>18m
storageclass.storage.k8s.io/standard <span class="o">(</span>default<span class="o">)</span>   k8s.io/minikube-hostpath   Delete          Immediate           <span class="nb">false                  </span>12d
</code></pre></div></div>

<p>Let us delete the Pod again and verify the files.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl delete pod app
pod <span class="s2">"app"</span> deleted
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-n</span> k8s-m02 <span class="nt">-p</span> k8s
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ cd /data/config/
$ ls
HiAgain.txt  hello.txt
$ ls -la
total 16
drwxr-xr-x 2 root root 4096 Feb 28 04:50 .
drwxr-xr-x 3 root root 4096 Feb 28 03:55 ..
-rw-r--r-- 1 root root   37 Feb 28 04:50 HiAgain.txt
-rw-r--r-- 1 root root   32 Feb 28 03:58 hello.txt
$ cat HiAgain.txt
Hello again from K8s PV,PVC, and SC!
$ cat hello.txt
Testing PV, PVC, and SC in K8S!
$ exit
logout
</span></code></pre></div></div>
<p>This confirms that, the storage is persistent again (even after Pod deletion).</p>]]></content><author><name>Kubernetes</name></author><category term="Kubernetes" /><category term="minikube" /><summary type="html"><![CDATA[Kubernetes StorageClass]]></summary></entry><entry><title type="html">Kubernetes Persistent Volumes and Claims</title><link href="https://www.pradeepgadde.com/blog/kubernetes/2022/02/27/kubernetes-persistentvolume-claims.html" rel="alternate" type="text/html" title="Kubernetes Persistent Volumes and Claims" /><published>2022-02-27T10:55:04+05:30</published><updated>2022-02-27T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/kubernetes/2022/02/27/kubernetes-persistentvolume-claims</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/kubernetes/2022/02/27/kubernetes-persistentvolume-claims.html"><![CDATA[<h1 id="kubernetes-persistent-volumes-and-claimes">Kubernetes Persistent Volumes and Claimes</h1>

<h3 id="persistentvolume">PersistentVolume</h3>

<p>Create a directory called <code class="language-plaintext highlighter-rouge">/mnt/data</code> on the <code class="language-plaintext highlighter-rouge">k8s</code> node. Within that directory, create <code class="language-plaintext highlighter-rouge">index.html</code> file with some text.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>minikube ssh <span class="nt">-p</span> k8s
                         _             _
            _         _ <span class="o">(</span> <span class="o">)</span>           <span class="o">(</span> <span class="o">)</span>
  ___ ___  <span class="o">(</span>_<span class="o">)</span>  ___  <span class="o">(</span>_<span class="o">)</span>| |/<span class="s1">')  _   _ | |_      __
/'</span> _ <span class="sb">`</span> _ <span class="sb">`</span><span class="se">\|</span> |/<span class="s1">' _ `\| || , &lt;  ( ) ( )| '</span>_<span class="sb">`</span><span class="se">\ </span> /<span class="s1">'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'</span><span class="o">(</span>_,__/<span class="s1">'`\____)

$ sudo mkdir /mnt/data
$ sudo sh -c "echo '</span>Hello from Kubernetes storage<span class="s1">' &gt; /mnt/data/indext.html"
$ cat /mnt/data/indext.html
Hello from Kubernetes storage
$ exit
logout
</span></code></pre></div></div>

<p>Now create a <code class="language-plaintext highlighter-rouge">PersistentVolume</code> named <code class="language-plaintext highlighter-rouge">my-pv-volume</code> of 1Gi using the <code class="language-plaintext highlighter-rouge">hostPath</code> pointing to the newly created directory. There are three types of <code class="language-plaintext highlighter-rouge">accessModes</code>, for this example use, <code class="language-plaintext highlighter-rouge">RWO</code>.</p>

<table>
  <thead>
    <tr>
      <th>RWO</th>
      <th>ReadWriteOnce</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RWX</td>
      <td>ReadWriteMany</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>ROX</td>
      <td>ReadyOnlyMany</td>
    </tr>
  </tbody>
</table>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat persistent-volume-demo.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-volume</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">local</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">manual</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/mnt/data"</span>
</code></pre></div></div>

<p>Create the PV using this YAML file.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> persistent-volume-demo.yaml
persistentvolume/my-pv-volume created
</code></pre></div></div>

<p>Verify the available PVs.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pv
NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
my-pv-volume   1Gi        RWO            Retain           Available           manual                  4s
</code></pre></div></div>

<p>Describe the PersistentVolume for addtional details.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pv
Name:            my-pv-volume
Labels:          <span class="nb">type</span><span class="o">=</span><span class="nb">local
</span>Annotations:     &lt;none&gt;
Finalizers:      <span class="o">[</span>kubernetes.io/pv-protection]
StorageClass:    manual
Status:          Available
Claim:
Reclaim Policy:  Retain
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        1Gi
Node Affinity:   &lt;none&gt;
Message:
Source:
    Type:          HostPath <span class="o">(</span>bare host directory volume<span class="o">)</span>
    Path:          /mnt/data
    HostPathType:
Events:            &lt;none&gt;
</code></pre></div></div>

<p><a href="#toc"><strong>:top:</strong></a></p>

<h3 id="persistentvolumeclaim">PersistentVolumeClaim</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat persistent-volume-claim-demo.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-claim</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">manual</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">300Mi</span>
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> persistent-volume-claim-demo.yaml
persistentvolumeclaim/my-pv-claim created
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pvc
NAME          STATUS   VOLUME         CAPACITY   ACCESS MODES   STORAGECLASS   AGE
my-pv-claim   Bound    my-pv-volume   1Gi        RWO            manual         40s
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pvc
Name:          my-pv-claim
Namespace:     default
StorageClass:  manual
Status:        Bound
Volume:        my-pv-volume
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: <span class="nb">yes
               </span>pv.kubernetes.io/bound-by-controller: <span class="nb">yes
</span>Finalizers:    <span class="o">[</span>kubernetes.io/pvc-protection]
Capacity:      1Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       &lt;none&gt;
Events:        &lt;none&gt;
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pv
NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
my-pv-volume   1Gi        RWO            Retain           Bound    default/my-pv-claim   manual                  12m
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pv
Name:            my-pv-volume
Labels:          <span class="nb">type</span><span class="o">=</span><span class="nb">local
</span>Annotations:     pv.kubernetes.io/bound-by-controller: <span class="nb">yes
</span>Finalizers:      <span class="o">[</span>kubernetes.io/pv-protection]
StorageClass:    manual
Status:          Bound
Claim:           default/my-pv-claim
Reclaim Policy:  Retain
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        1Gi
Node Affinity:   &lt;none&gt;
Message:
Source:
    Type:          HostPath <span class="o">(</span>bare host directory volume<span class="o">)</span>
    Path:          /mnt/data
    HostPathType:
Events:            &lt;none&gt;
</code></pre></div></div>

<p>The next step is to create a Pod that uses this PersistentVolumeClaim as a volume.</p>

<p>Here is the configuration file for the Pod:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat my-pv-pod.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-storage</span>
      <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
        <span class="na">claimName</span><span class="pi">:</span> <span class="s">my-pv-claim</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-container</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
      <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">http-server"</span>
      <span class="na">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/usr/share/nginx/html"</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-storage</span>
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> my-pv-pod.yaml
pod/my-pv-pod created
</code></pre></div></div>
<p>The scheduler has placed this Pod on <code class="language-plaintext highlighter-rouge">k8s-m02</code> node.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl get pods my-pv-pod <span class="nt">-o</span> wide
NAME        READY   STATUS    RESTARTS   AGE   IP           NODE      NOMINATED NODE   READINESS GATES
my-pv-pod   1/1     Running   0          10s   10.244.1.2   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>
<p>Let us describe the pod and look at the <code class="language-plaintext highlighter-rouge">Volumes</code> section.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl describe pods my-pv-pod
Name:         my-pv-pod
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Sun, 27 Feb 2022 22:35:45 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.2
IPs:
  IP:  10.244.1.2
Containers:
  my-pv-container:
    Container ID:   docker://9dc92c5c1037ad4d534fe6440275f62a68f521e3bd22fe47b57c06a8b3414e24
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sun, 27 Feb 2022 22:35:49 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /usr/share/nginx/html from my-pv-storage <span class="o">(</span>rw<span class="o">)</span>
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xk6kh <span class="o">(</span>ro<span class="o">)</span>
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  my-pv-storage:
    Type:       PersistentVolumeClaim <span class="o">(</span>a reference to a PersistentVolumeClaim <span class="k">in </span>the same namespace<span class="o">)</span>
    ClaimName:  my-pv-claim
    ReadOnly:   <span class="nb">false
  </span>kube-api-access-xk6kh:
    Type:                    Projected <span class="o">(</span>a volume that contains injected data from multiple sources<span class="o">)</span>
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             <span class="nb">true
</span>QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute <span class="nv">op</span><span class="o">=</span>Exists <span class="k">for </span>300s
                             node.kubernetes.io/unreachable:NoExecute <span class="nv">op</span><span class="o">=</span>Exists <span class="k">for </span>300s
Events:
  Type    Reason     Age   From               Message
  <span class="nt">----</span>    <span class="nt">------</span>     <span class="nt">----</span>  <span class="nt">----</span>               <span class="nt">-------</span>
  Normal  Scheduled  19s   default-scheduler  Successfully assigned default/my-pv-pod to k8s-m02
  Normal  Pulling    18s   kubelet            Pulling image <span class="s2">"nginx"</span>
  Normal  Pulled     15s   kubelet            Successfully pulled image <span class="s2">"nginx"</span> <span class="k">in </span>2.858388422s
  Normal  Created    15s   kubelet            Created container my-pv-container
  Normal  Started    15s   kubelet            Started container my-pv-container
</code></pre></div></div>
<p>It looks like no issues, but let us verify the <code class="language-plaintext highlighter-rouge">nginx</code> application by logging into the pod and issuing the <code class="language-plaintext highlighter-rouge">curl localhost</code> command.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl <span class="nb">exec</span> <span class="nt">-it</span> my-pv-pod <span class="nt">--</span> /bin/sh
<span class="c"># curl localhost</span>
&lt;html&gt;
&lt;<span class="nb">head</span><span class="o">&gt;</span>&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.21.6&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
<span class="c"># exit</span>
</code></pre></div></div>

<p>Hmm! This seems to be not working. It is forbidden. But this is expected right, we did not have the volume (hostPath) on this <code class="language-plaintext highlighter-rouge">k8s-m02</code> node. We created it on <code class="language-plaintext highlighter-rouge">k8s</code> node only.</p>

<p>To solve this, we can add a <code class="language-plaintext highlighter-rouge">nodeName</code> spec to the Pod definition.
Delete this pod and after adding the <code class="language-plaintext highlighter-rouge">nodeName</code> recreate the Pod.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl delete pod my-pv-pod
pod <span class="s2">"my-pv-pod"</span> deleted
</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>vi my-pv-pod.yaml
</code></pre></div></div>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnk8s$ cat my-pv-pod.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">nodeName</span><span class="pi">:</span> <span class="s">k8s</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-storage</span>
      <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
        <span class="na">claimName</span><span class="pi">:</span> <span class="s">my-pv-claim</span>
  <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-container</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
      <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">http-server"</span>
      <span class="na">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/usr/share/nginx/html"</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">my-pv-storage</span>

</code></pre></div></div>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnk8s<span class="nv">$ </span>kubectl create <span class="nt">-f</span> my-pv-pod.yaml
pod/my-pv-pod created
</code></pre></div></div>
<p>:warning: There seems to be some issue with this. Even after scheduling this pod on the node <code class="language-plaintext highlighter-rouge">k8s</code>, nginx container is reporting Forbidden. There are couple of issues already reported, but could not find any resolution to this problem yet.</p>

<p>https://github.com/kubernetes/website/issues/9523</p>]]></content><author><name>Kubernetes</name></author><category term="Kubernetes" /><category term="minikube" /><summary type="html"><![CDATA[Kubernetes Persistent Volumes and Claimes]]></summary></entry><entry><title type="html">OpenShift</title><link href="https://www.pradeepgadde.com/blog/openshift/automation/2022/02/23/openshift.html" rel="alternate" type="text/html" title="OpenShift" /><published>2022-02-23T10:55:04+05:30</published><updated>2022-02-23T10:55:04+05:30</updated><id>https://www.pradeepgadde.com/blog/openshift/automation/2022/02/23/openshift</id><content type="html" xml:base="https://www.pradeepgadde.com/blog/openshift/automation/2022/02/23/openshift.html"><![CDATA[<p><img src="/assets/images/RedHatOpenShift.png" alt="" /></p>
<h2 id="redhat-openshift">RedHat OpenShift</h2>
<p>Getting Started with OpenShift.</p>

<h3 id="learning-resources">Learning Resources</h3>

<p>https://github.com/sandervanvugt/openshift</p>

<p>OCP</p>

<p>Kubernetes Distribution with additional features: Source Code Integration, CI/CD</p>

<p>Service Mesh like Istio Integration</p>

<p>CodeReady Containers https://developers.redhat.com/products/codeready-containers/overview</p>

<p>Offerings</p>

<p>RHOCP : RedHat OpenShift Container Platform: managed by customers (on-prem)</p>

<p>RedHat OpenShift Dedicated: RedHat managed cluster in AWS/GCP/Azure/IBM Clouds</p>

<p>RedHat OpenShift Online: shared across multiple customers, RedHat managed</p>

<p>RedHat OpenShift Kubernetes Engine : Just kubernetes</p>

<p>RedHat CodeReady Containers : a minimal installation</p>

<p>OKD: Open source upstream (OpenShift Kubernetes Distribution)</p>

<p>OpenShift is more expensive than Rancher, which is another Kubernetes distribution.</p>

<h2 id="understanding-containers">Understanding Containers</h2>

<p>Docker Container engine:  common</p>

<p>CRI-o , native in RHEL8</p>

<p>In OpenShift, containers are managed in Pods. A pod consists of one or more containers</p>

<p>OpenShift adds features on top of Kubernetes, but uses core k8s infrastructure</p>

<p>OpenShift adds resource types to K8s environment</p>

<p>Most OpenShift services are implemented in containers</p>

<h2 id="setting-up-a-lab">Setting up a Lab</h2>

<p>RedHat Certified Specialist in Containers and Kubernetes EX180</p>

<p>Chapter#6</p>

<p>OpenShift 3.x MiniShift (not Recommended)</p>

<p>OpenShift 4.x CRC  (Recommended), CRC is awesome!</p>

<p>Alternative option: 30-day trial version of Developer Sandbox</p>

<p>CodeReady Containers (CRC) : all-in-one RedHat licensed OpenShift 4.x installation</p>

<p>Min 4vCPUs, 16GB RAM minimum</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>crc
No <span class="nb">command </span>given
CodeReady Containers is a tool that manages a <span class="nb">local </span>OpenShift 4.x cluster optimized <span class="k">for </span>testing and development purposes

Usage:
  crc <span class="o">[</span>flags]
  crc <span class="o">[</span><span class="nb">command</span><span class="o">]</span>

Available Commands:
  bundle      Manage CRC bundles
  cleanup     Undo config changes
  completion  generate the autocompletion script <span class="k">for </span>the specified shell
  config      Modify crc configuration
  console     Open the OpenShift Web Console <span class="k">in </span>the default browser
  delete      Delete the OpenShift cluster
  <span class="nb">help        </span>Help about any <span class="nb">command
  </span>ip          Get IP address of the running OpenShift cluster
  oc-env      Add the <span class="s1">'oc'</span> executable to PATH
  podman-env  Setup podman environment
  setup       Set up prerequisites <span class="k">for </span>the OpenShift cluster
  start       Start the OpenShift cluster
  status      Display status of the OpenShift cluster
  stop        Stop the OpenShift cluster
  version     Print version information

Flags:
  <span class="nt">-h</span>, <span class="nt">--help</span>               <span class="nb">help </span><span class="k">for </span>crc
      <span class="nt">--log-level</span> string   log level <span class="o">(</span>e.g. <span class="s2">"debug | info | warn | error"</span><span class="o">)</span> <span class="o">(</span>default <span class="s2">"info"</span><span class="o">)</span>

Use <span class="s2">"crc [command] --help"</span> <span class="k">for </span>more information about a command.
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>crc status
CRC VM:          Stopped
OpenShift:       Stopped <span class="o">(</span>v4.9.12<span class="o">)</span>
Disk Usage:      0B of 0B <span class="o">(</span>Inside the CRC VM<span class="o">)</span>
Cache Usage:     12.78GB
Cache Directory: /Users/pradeep/.crc/cache
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>crc start
WARN A new version <span class="o">(</span>1.39.0<span class="o">)</span> has been published on https://developers.redhat.com/content-gateway/file/pub/openshift-v4/clients/crc/1.39.0/crc-macos-amd64.pkg
INFO Checking <span class="k">if </span>running as non-root
INFO Checking <span class="k">if </span>crc-admin-helper executable is cached
INFO Checking <span class="k">for </span>obsolete admin-helper executable
INFO Checking <span class="k">if </span>running on a supported CPU architecture
INFO Checking minimum RAM requirements
INFO Checking <span class="k">if </span>running emulated on a M1 CPU
INFO Checking <span class="k">if </span>HyperKit is installed
INFO Checking <span class="k">if </span>qcow-tool is installed
INFO Checking <span class="k">if </span>crc-driver-hyperkit is installed
INFO Starting CodeReady Containers VM <span class="k">for </span>OpenShift 4.9.12...
INFO CodeReady Containers instance is running with IP 127.0.0.1
INFO CodeReady Containers VM is running
INFO Check internal and public DNS query...
INFO Check DNS query from host...
INFO Verifying validity of the kubelet certificates...
INFO Starting OpenShift kubelet service
INFO Kubelet client certificate has expired, renewing it... <span class="o">[</span>will take up to 8 minutes]


INFO Kubelet serving certificate has expired, waiting <span class="k">for </span>automatic renewal... <span class="o">[</span>will take up to 8 minutes]
INFO Waiting <span class="k">for </span>kube-apiserver availability... <span class="o">[</span>takes around 2min]
INFO Waiting <span class="k">for </span>user<span class="s1">'s pull secret part of instance disk...

INFO Starting OpenShift cluster... [waiting for the cluster to stabilize]
INFO Operator operator-lifecycle-manager-packageserver is progressing
INFO 2 operators are progressing: kube-apiserver, openshift-controller-manager
INFO 2 operators are progressing: kube-apiserver, openshift-controller-manager
INFO 2 operators are progressing: kube-apiserver, openshift-controller-manager
INFO 2 operators are progressing: kube-apiserver, openshift-controller-manager
INFO 2 operators are progressing: kube-apiserver, openshift-controller-manager

INFO 2 operators are progressing: kube-apiserver, openshift-controller-manager
INFO Operator openshift-controller-manager is progressing
INFO Operator openshift-controller-manager is progressing
INFO Operator openshift-controller-manager is progressing
INFO Operator openshift-controller-manager is progressing
INFO Operator openshift-controller-manager is progressing
INFO Operator openshift-controller-manager is progressing
INFO All operators are available. Ensuring stability...
INFO Operator authentication is not yet available
ERRO Cluster is not ready: cluster operators are still not stable after 10m44.701469756s
INFO Adding crc-admin and crc-developer contexts to kubeconfig...
Started the OpenShift cluster.

The server is accessible via web console at:
  https://console-openshift-console.apps-crc.testing

Log in as administrator:
  Username: kubeadmin
  Password: ZyxGy-rKUxa-kE4Pp-EJLFc

Log in as user:
  Username: developer
  Password: developer

Use the '</span>oc<span class="s1">' command line interface:
  $ eval $(crc oc-env)
  $ oc login -u developer https://api.crc.testing:6443
</span></code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>crc status
CRC VM:          Running
OpenShift:       Starting <span class="o">(</span>v4.9.12<span class="o">)</span>
Disk Usage:      13.14GB of 32.74GB <span class="o">(</span>Inside the CRC VM<span class="o">)</span>
Cache Usage:     12.78GB
Cache Directory: /Users/pradeep/.crc/cache
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span><span class="nb">eval</span> <span class="si">$(</span>crc oc-env<span class="si">)</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>crc console <span class="nt">--credentials</span>
To login as a regular user, run <span class="s1">'oc login -u developer -p developer https://api.crc.testing:6443'</span><span class="nb">.</span>
To login as an admin, run <span class="s1">'oc login -u kubeadmin -p ZyxGy-rKUxa-kE4Pp-EJLFc https://api.crc.testing:6443'</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc login <span class="nt">-u</span> developer https://api.crc.testing:6443
Logged into <span class="s2">"https://api.crc.testing:6443"</span> as <span class="s2">"developer"</span> using existing credentials.

You don<span class="s1">'t have any projects. You can try to create a new project, by running

    oc new-project &lt;projectname&gt;
</span></code></pre></div></div>

<h2 id="setting-up-an-application-in-openshift">Setting up an Application in OpenShift</h2>

<h3 id="using-console">Using Console</h3>

<p><img src="/assets/images/OS-1.png" alt="/assets/images/OS-1" /></p>

<p><img src="/assets/images/OS-2.png" alt="/assets/images/OS-2" /></p>

<p><img src="/assets/images/OS-3.png" alt="/assets/images/OS-3" /></p>

<p><img src="/assets/images/OS-4.png" alt="/assets/images/OS-4" /></p>

<p><img src="/assets/images/OS-5.png" alt="/assets/images/OS-1" /></p>

<p><img src="/assets/images/OS-6.png" alt="/assets/images/OS-2" /></p>

<p><img src="/assets/images/OS-7.png" alt="/assets/images/OS-3" /></p>

<p><img src="/assets/images/OS-8.png" alt="/assets/images/OS-4" /></p>

<p><img src="/assets/images/OS-9.png" alt="/assets/images/OS-1" /></p>

<p><img src="/assets/images/OS-10.png" alt="/assets/images/OS-2" /></p>

<p><img src="/assets/images/OS-11.png" alt="/assets/images/OS-3" /></p>

<p><img src="/assets/images/OS-12.png" alt="/assets/images/OS-4" /></p>

<p><img src="/assets/images/OS-13.png" alt="/assets/images/OS-1" /></p>

<p><img src="/assets/images/OS-14.png" alt="/assets/images/OS-2" /></p>

<p><img src="/assets/images/OS-15.png" alt="/assets/images/OS-3" /></p>

<p><img src="/assets/images/OS-16.png" alt="/assets/images/OS-4" /></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc login <span class="nt">--token</span><span class="o">=</span>sha256~pWLPko6xkJ71k00adi-k3pyXCVr4TJrIs5l8muLOUnk <span class="nt">--server</span><span class="o">=</span>https://api.crc.testing:6443
Logged into <span class="s2">"https://api.crc.testing:6443"</span> as <span class="s2">"developer"</span> using the token provided.

You have one project on this server: <span class="s2">"pradeep-os-demo"</span>

Using project <span class="s2">"pradeep-os-demo"</span><span class="nb">.</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get all
NAME                                READY   STATUS      RESTARTS   AGE
pod/pradeep-cake-1-build            0/1     Completed   0          40m
pod/pradeep-cake-64796f74fd-mmh6p   1/1     Running     0          9m3s

NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>             AGE
service/pradeep-cake   ClusterIP   10.217.5.152   &lt;none&gt;        8080/TCP,8443/TCP   40m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pradeep-cake   1/1     1            1           40m

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/pradeep-cake-64796f74fd   1         1         1       9m3s
replicaset.apps/pradeep-cake-95f4d7758    0         0         0       40m

NAME                                          TYPE     FROM   LATEST
buildconfig.build.openshift.io/pradeep-cake   Source   Git    2

NAME                                      TYPE     FROM          STATUS     STARTED          DURATION
build.build.openshift.io/pradeep-cake-1   Source   Git@ba4b1cd   Complete   40 minutes ago   31m38s

NAME                                          IMAGE REPOSITORY                                                                       TAGS     UPDATED
imagestream.image.openshift.io/pradeep-cake   default-route-openshift-image-registry.apps-crc.testing/pradeep-os-demo/pradeep-cake   latest   9 minutes ago

NAME                                    HOST/PORT                                       PATH   SERVICES       PORT       TERMINATION   WILDCARD
route.route.openshift.io/pradeep-cake   pradeep-cake-pradeep-os-demo.apps-crc.testing          pradeep-cake   8080-tcp                 None
</code></pre></div></div>

<h2 id="switching-to-administrator-persona">Switching to Administrator Persona</h2>

<p><img src="/assets/images/OS-17.png" alt="" /></p>

<p><img src="/assets/images/OS-18.png" alt="/assets/images/OS-2" /></p>

<p><img src="/assets/images/OS-19.png" alt="/assets/images/OS-3" /></p>

<p><img src="/assets/images/OS-20.png" alt="/assets/images/OS-4" /></p>

<p><img src="/assets/images/OS-21.png" alt="/assets/images/OS-4" /></p>

<p>After deleting the pod from failed build</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get all
NAME                                READY   STATUS    RESTARTS   AGE
pod/pradeep-cake-64796f74fd-mmh6p   1/1     Running   0          13m

NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>             AGE
service/pradeep-cake   ClusterIP   10.217.5.152   &lt;none&gt;        8080/TCP,8443/TCP   45m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pradeep-cake   1/1     1            1           45m

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/pradeep-cake-64796f74fd   1         1         1       13m
replicaset.apps/pradeep-cake-95f4d7758    0         0         0       45m

NAME                                          TYPE     FROM   LATEST
buildconfig.build.openshift.io/pradeep-cake   Source   Git    2

NAME                                      TYPE     FROM          STATUS     STARTED          DURATION
build.build.openshift.io/pradeep-cake-1   Source   Git@ba4b1cd   Complete   45 minutes ago   31m38s

NAME                                          IMAGE REPOSITORY                                                                       TAGS     UPDATED
imagestream.image.openshift.io/pradeep-cake   default-route-openshift-image-registry.apps-crc.testing/pradeep-os-demo/pradeep-cake   latest   13 minutes ago

NAME                                    HOST/PORT                                       PATH   SERVICES       PORT       TERMINATION   WILDCARD
route.route.openshift.io/pradeep-cake   pradeep-cake-pradeep-os-demo.apps-crc.testing          pradeep-cake   8080-tcp                 None
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc projects
You have one project on this server: <span class="s2">"pradeep-os-demo"</span><span class="nb">.</span>

Using project <span class="s2">"pradeep-os-demo"</span> on server <span class="s2">"https://api.crc.testing:6443"</span><span class="nb">.</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get pods
NAME                            READY   STATUS    RESTARTS   AGE
pradeep-cake-64796f74fd-mmh6p   1/1     Running   0          18m
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get nodes
Error from server <span class="o">(</span>Forbidden<span class="o">)</span>: nodes is forbidden: User <span class="s2">"developer"</span> cannot list resource <span class="s2">"nodes"</span> <span class="k">in </span>API group <span class="s2">""</span> at the cluster scope
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get deploy
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
pradeep-cake   1/1     1            1           50m
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get svc
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>             AGE
pradeep-cake   ClusterIP   10.217.5.152   &lt;none&gt;        8080/TCP,8443/TCP   50m
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc describe pod pradeep-cake-64796f74fd-mmh6p
Name:         pradeep-cake-64796f74fd-mmh6p
Namespace:    pradeep-os-demo
Priority:     0
Node:         crc-xxcfw-master-0/192.168.126.11
Start Time:   Tue, 22 Feb 2022 20:54:31 +0530
Labels:       <span class="nv">app</span><span class="o">=</span>pradeep-cake
              <span class="nv">deploymentconfig</span><span class="o">=</span>pradeep-cake
              pod-template-hash<span class="o">=</span>64796f74fd
Annotations:  k8s.v1.cni.cncf.io/network-status:
                <span class="o">[{</span>
                    <span class="s2">"name"</span>: <span class="s2">"openshift-sdn"</span>,
                    <span class="s2">"interface"</span>: <span class="s2">"eth0"</span>,
                    <span class="s2">"ips"</span>: <span class="o">[</span>
                        <span class="s2">"10.217.0.68"</span>
                    <span class="o">]</span>,
                    <span class="s2">"default"</span>: <span class="nb">true</span>,
                    <span class="s2">"dns"</span>: <span class="o">{}</span>
                <span class="o">}]</span>
              k8s.v1.cni.cncf.io/networks-status:
                <span class="o">[{</span>
                    <span class="s2">"name"</span>: <span class="s2">"openshift-sdn"</span>,
                    <span class="s2">"interface"</span>: <span class="s2">"eth0"</span>,
                    <span class="s2">"ips"</span>: <span class="o">[</span>
                        <span class="s2">"10.217.0.68"</span>
                    <span class="o">]</span>,
                    <span class="s2">"default"</span>: <span class="nb">true</span>,
                    <span class="s2">"dns"</span>: <span class="o">{}</span>
                <span class="o">}]</span>
              openshift.io/scc: restricted
Status:       Running
IP:           10.217.0.68
IPs:
  IP:           10.217.0.68
Controlled By:  ReplicaSet/pradeep-cake-64796f74fd
Containers:
  pradeep-cake:
    Container ID:   cri-o://1801d269a5ba5c12a1228678fe09452e670f94944dc91003d3d0fe38f3d9f4d8
    Image:          image-registry.openshift-image-registry.svc:5000/pradeep-os-demo/pradeep-cake@sha256:66900b89f25f97553f3fe81f91354d4fd1746d0265ad135b34acffbf211b62ef
    Image ID:       image-registry.openshift-image-registry.svc:5000/pradeep-os-demo/pradeep-cake@sha256:66900b89f25f97553f3fe81f91354d4fd1746d0265ad135b34acffbf211b62ef
    Ports:          8080/TCP, 8443/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Tue, 22 Feb 2022 20:56:37 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-789n7 <span class="o">(</span>ro<span class="o">)</span>
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-789n7:
    Type:                    Projected <span class="o">(</span>a volume that contains injected data from multiple sources<span class="o">)</span>
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             <span class="nb">true
    </span>ConfigMapName:           openshift-service-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute <span class="nv">op</span><span class="o">=</span>Exists <span class="k">for </span>300s
                             node.kubernetes.io/unreachable:NoExecute <span class="nv">op</span><span class="o">=</span>Exists <span class="k">for </span>300s
Events:
  Type    Reason          Age   From               Message
  <span class="nt">----</span>    <span class="nt">------</span>          <span class="nt">----</span>  <span class="nt">----</span>               <span class="nt">-------</span>
  Normal  Scheduled       19m   default-scheduler  Successfully assigned pradeep-os-demo/pradeep-cake-64796f74fd-mmh6p to crc-xxcfw-master-0
  Normal  AddedInterface  19m   multus             Add eth0 <span class="o">[</span>10.217.0.68/23] from openshift-sdn
  Normal  Pulling         19m   kubelet            Pulling image <span class="s2">"image-registry.openshift-image-registry.svc:5000/pradeep-os-demo/pradeep-cake@sha256:66900b89f25f97553f3fe81f91354d4fd1746d0265ad135b34acffbf211b62ef"</span>
  Normal  Pulled          17m   kubelet            Successfully pulled image <span class="s2">"image-registry.openshift-image-registry.svc:5000/pradeep-os-demo/pradeep-cake@sha256:66900b89f25f97553f3fe81f91354d4fd1746d0265ad135b34acffbf211b62ef"</span> <span class="k">in </span>2m1.585839677s
  Normal  Created         17m   kubelet            Created container pradeep-cake
  Normal  Started         17m   kubelet            Started container pradeep-cake
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc api-resources
NAME                                  SHORTNAMES       APIVERSION                                    NAMESPACED   KIND
bindings                                               v1                                            <span class="nb">true         </span>Binding
componentstatuses                     cs               v1                                            <span class="nb">false        </span>ComponentStatus
configmaps                            cm               v1                                            <span class="nb">true         </span>ConfigMap
endpoints                             ep               v1                                            <span class="nb">true         </span>Endpoints
events                                ev               v1                                            <span class="nb">true         </span>Event
limitranges                           limits           v1                                            <span class="nb">true         </span>LimitRange
namespaces                            ns               v1                                            <span class="nb">false        </span>Namespace
nodes                                 no               v1                                            <span class="nb">false        </span>Node
persistentvolumeclaims                pvc              v1                                            <span class="nb">true         </span>PersistentVolumeClaim
persistentvolumes                     pv               v1                                            <span class="nb">false        </span>PersistentVolume
pods                                  po               v1                                            <span class="nb">true         </span>Pod
podtemplates                                           v1                                            <span class="nb">true         </span>PodTemplate
replicationcontrollers                rc               v1                                            <span class="nb">true         </span>ReplicationController
resourcequotas                        quota            v1                                            <span class="nb">true         </span>ResourceQuota
secrets                                                v1                                            <span class="nb">true         </span>Secret
serviceaccounts                       sa               v1                                            <span class="nb">true         </span>ServiceAccount
services                              svc              v1                                            <span class="nb">true         </span>Service
mutatingwebhookconfigurations                          admissionregistration.k8s.io/v1               <span class="nb">false        </span>MutatingWebhookConfiguration
validatingwebhookconfigurations                        admissionregistration.k8s.io/v1               <span class="nb">false        </span>ValidatingWebhookConfiguration
customresourcedefinitions             crd,crds         apiextensions.k8s.io/v1                       <span class="nb">false        </span>CustomResourceDefinition
apiservices                                            apiregistration.k8s.io/v1                     <span class="nb">false        </span>APIService
apirequestcounts                                       apiserver.openshift.io/v1                     <span class="nb">false        </span>APIRequestCount
controllerrevisions                                    apps/v1                                       <span class="nb">true         </span>ControllerRevision
daemonsets                            ds               apps/v1                                       <span class="nb">true         </span>DaemonSet
deployments                           deploy           apps/v1                                       <span class="nb">true         </span>Deployment
replicasets                           rs               apps/v1                                       <span class="nb">true         </span>ReplicaSet
statefulsets                          sts              apps/v1                                       <span class="nb">true         </span>StatefulSet
deploymentconfigs                     dc               apps.openshift.io/v1                          <span class="nb">true         </span>DeploymentConfig
tokenreviews                                           authentication.k8s.io/v1                      <span class="nb">false        </span>TokenReview
localsubjectaccessreviews                              authorization.k8s.io/v1                       <span class="nb">true         </span>LocalSubjectAccessReview
selfsubjectaccessreviews                               authorization.k8s.io/v1                       <span class="nb">false        </span>SelfSubjectAccessReview
selfsubjectrulesreviews                                authorization.k8s.io/v1                       <span class="nb">false        </span>SelfSubjectRulesReview
subjectaccessreviews                                   authorization.k8s.io/v1                       <span class="nb">false        </span>SubjectAccessReview
clusterrolebindings                                    authorization.openshift.io/v1                 <span class="nb">false        </span>ClusterRoleBinding
clusterroles                                           authorization.openshift.io/v1                 <span class="nb">false        </span>ClusterRole
localresourceaccessreviews                             authorization.openshift.io/v1                 <span class="nb">true         </span>LocalResourceAccessReview
localsubjectaccessreviews                              authorization.openshift.io/v1                 <span class="nb">true         </span>LocalSubjectAccessReview
resourceaccessreviews                                  authorization.openshift.io/v1                 <span class="nb">false        </span>ResourceAccessReview
rolebindingrestrictions                                authorization.openshift.io/v1                 <span class="nb">true         </span>RoleBindingRestriction
rolebindings                                           authorization.openshift.io/v1                 <span class="nb">true         </span>RoleBinding
roles                                                  authorization.openshift.io/v1                 <span class="nb">true         </span>Role
selfsubjectrulesreviews                                authorization.openshift.io/v1                 <span class="nb">true         </span>SelfSubjectRulesReview
subjectaccessreviews                                   authorization.openshift.io/v1                 <span class="nb">false        </span>SubjectAccessReview
subjectrulesreviews                                    authorization.openshift.io/v1                 <span class="nb">true         </span>SubjectRulesReview
horizontalpodautoscalers              hpa              autoscaling/v1                                <span class="nb">true         </span>HorizontalPodAutoscaler
clusterautoscalers                    ca               autoscaling.openshift.io/v1                   <span class="nb">false        </span>ClusterAutoscaler
machineautoscalers                    ma               autoscaling.openshift.io/v1beta1              <span class="nb">true         </span>MachineAutoscaler
cronjobs                              cj               batch/v1                                      <span class="nb">true         </span>CronJob
<span class="nb">jobs                                                   </span>batch/v1                                      <span class="nb">true         </span>Job
buildconfigs                          bc               build.openshift.io/v1                         <span class="nb">true         </span>BuildConfig
builds                                                 build.openshift.io/v1                         <span class="nb">true         </span>Build
certificatesigningrequests            csr              certificates.k8s.io/v1                        <span class="nb">false        </span>CertificateSigningRequest
credentialsrequests                                    cloudcredential.openshift.io/v1               <span class="nb">true         </span>CredentialsRequest
apiservers                                             config.openshift.io/v1                        <span class="nb">false        </span>APIServer
authentications                                        config.openshift.io/v1                        <span class="nb">false        </span>Authentication
builds                                                 config.openshift.io/v1                        <span class="nb">false        </span>Build
clusteroperators                      co               config.openshift.io/v1                        <span class="nb">false        </span>ClusterOperator
clusterversions                                        config.openshift.io/v1                        <span class="nb">false        </span>ClusterVersion
consoles                                               config.openshift.io/v1                        <span class="nb">false        </span>Console
dnses                                                  config.openshift.io/v1                        <span class="nb">false        </span>DNS
featuregates                                           config.openshift.io/v1                        <span class="nb">false        </span>FeatureGate
images                                                 config.openshift.io/v1                        <span class="nb">false        </span>Image
infrastructures                                        config.openshift.io/v1                        <span class="nb">false        </span>Infrastructure
ingresses                                              config.openshift.io/v1                        <span class="nb">false        </span>Ingress
networks                                               config.openshift.io/v1                        <span class="nb">false        </span>Network
oauths                                                 config.openshift.io/v1                        <span class="nb">false        </span>OAuth
operatorhubs                                           config.openshift.io/v1                        <span class="nb">false        </span>OperatorHub
projects                                               config.openshift.io/v1                        <span class="nb">false        </span>Project
proxies                                                config.openshift.io/v1                        <span class="nb">false        </span>Proxy
schedulers                                             config.openshift.io/v1                        <span class="nb">false        </span>Scheduler
consoleclidownloads                                    console.openshift.io/v1                       <span class="nb">false        </span>ConsoleCLIDownload
consoleexternalloglinks                                console.openshift.io/v1                       <span class="nb">false        </span>ConsoleExternalLogLink
consolelinks                                           console.openshift.io/v1                       <span class="nb">false        </span>ConsoleLink
consolenotifications                                   console.openshift.io/v1                       <span class="nb">false        </span>ConsoleNotification
consoleplugins                                         console.openshift.io/v1alpha1                 <span class="nb">false        </span>ConsolePlugin
consolequickstarts                                     console.openshift.io/v1                       <span class="nb">false        </span>ConsoleQuickStart
consoleyamlsamples                                     console.openshift.io/v1                       <span class="nb">false        </span>ConsoleYAMLSample
podnetworkconnectivitychecks                           controlplane.operator.openshift.io/v1alpha1   <span class="nb">true         </span>PodNetworkConnectivityCheck
leases                                                 coordination.k8s.io/v1                        <span class="nb">true         </span>Lease
endpointslices                                         discovery.k8s.io/v1                           <span class="nb">true         </span>EndpointSlice
events                                ev               events.k8s.io/v1                              <span class="nb">true         </span>Event
flowschemas                                            flowcontrol.apiserver.k8s.io/v1beta1          <span class="nb">false        </span>FlowSchema
prioritylevelconfigurations                            flowcontrol.apiserver.k8s.io/v1beta1          <span class="nb">false        </span>PriorityLevelConfiguration
helmchartrepositories                                  helm.openshift.io/v1beta1                     <span class="nb">false        </span>HelmChartRepository
images                                                 image.openshift.io/v1                         <span class="nb">false        </span>Image
imagesignatures                                        image.openshift.io/v1                         <span class="nb">false        </span>ImageSignature
imagestreamimages                     isimage          image.openshift.io/v1                         <span class="nb">true         </span>ImageStreamImage
imagestreamimports                                     image.openshift.io/v1                         <span class="nb">true         </span>ImageStreamImport
imagestreammappings                                    image.openshift.io/v1                         <span class="nb">true         </span>ImageStreamMapping
imagestreams                          is               image.openshift.io/v1                         <span class="nb">true         </span>ImageStream
imagestreamtags                       istag            image.openshift.io/v1                         <span class="nb">true         </span>ImageStreamTag
imagetags                             itag             image.openshift.io/v1                         <span class="nb">true         </span>ImageTag
configs                                                imageregistry.operator.openshift.io/v1        <span class="nb">false        </span>Config
imagepruners                                           imageregistry.operator.openshift.io/v1        <span class="nb">false        </span>ImagePruner
dnsrecords                                             ingress.operator.openshift.io/v1              <span class="nb">true         </span>DNSRecord
network-attachment-definitions        net-attach-def   k8s.cni.cncf.io/v1                            <span class="nb">true         </span>NetworkAttachmentDefinition
machinehealthchecks                   mhc,mhcs         machine.openshift.io/v1beta1                  <span class="nb">true         </span>MachineHealthCheck
machines                                               machine.openshift.io/v1beta1                  <span class="nb">true         </span>Machine
machinesets                                            machine.openshift.io/v1beta1                  <span class="nb">true         </span>MachineSet
containerruntimeconfigs               ctrcfg           machineconfiguration.openshift.io/v1          <span class="nb">false        </span>ContainerRuntimeConfig
controllerconfigs                                      machineconfiguration.openshift.io/v1          <span class="nb">false        </span>ControllerConfig
kubeletconfigs                                         machineconfiguration.openshift.io/v1          <span class="nb">false        </span>KubeletConfig
machineconfigpools                    mcp              machineconfiguration.openshift.io/v1          <span class="nb">false        </span>MachineConfigPool
machineconfigs                        mc               machineconfiguration.openshift.io/v1          <span class="nb">false        </span>MachineConfig
baremetalhosts                        bmh,bmhost       metal3.io/v1alpha1                            <span class="nb">true         </span>BareMetalHost
provisionings                                          metal3.io/v1alpha1                            <span class="nb">false        </span>Provisioning
storagestates                                          migration.k8s.io/v1alpha1                     <span class="nb">false        </span>StorageState
storageversionmigrations                               migration.k8s.io/v1alpha1                     <span class="nb">false        </span>StorageVersionMigration
alertmanagerconfigs                                    monitoring.coreos.com/v1alpha1                <span class="nb">true         </span>AlertmanagerConfig
alertmanagers                                          monitoring.coreos.com/v1                      <span class="nb">true         </span>Alertmanager
podmonitors                                            monitoring.coreos.com/v1                      <span class="nb">true         </span>PodMonitor
probes                                                 monitoring.coreos.com/v1                      <span class="nb">true         </span>Probe
prometheuses                                           monitoring.coreos.com/v1                      <span class="nb">true         </span>Prometheus
prometheusrules                                        monitoring.coreos.com/v1                      <span class="nb">true         </span>PrometheusRule
servicemonitors                                        monitoring.coreos.com/v1                      <span class="nb">true         </span>ServiceMonitor
thanosrulers                                           monitoring.coreos.com/v1                      <span class="nb">true         </span>ThanosRuler
clusternetworks                                        network.openshift.io/v1                       <span class="nb">false        </span>ClusterNetwork
egressnetworkpolicies                                  network.openshift.io/v1                       <span class="nb">true         </span>EgressNetworkPolicy
hostsubnets                                            network.openshift.io/v1                       <span class="nb">false        </span>HostSubnet
netnamespaces                                          network.openshift.io/v1                       <span class="nb">false        </span>NetNamespace
egressrouters                                          network.operator.openshift.io/v1              <span class="nb">true         </span>EgressRouter
operatorpkis                                           network.operator.openshift.io/v1              <span class="nb">true         </span>OperatorPKI
ingressclasses                                         networking.k8s.io/v1                          <span class="nb">false        </span>IngressClass
ingresses                             ing              networking.k8s.io/v1                          <span class="nb">true         </span>Ingress
networkpolicies                       netpol           networking.k8s.io/v1                          <span class="nb">true         </span>NetworkPolicy
runtimeclasses                                         node.k8s.io/v1                                <span class="nb">false        </span>RuntimeClass
oauthaccesstokens                                      oauth.openshift.io/v1                         <span class="nb">false        </span>OAuthAccessToken
oauthauthorizetokens                                   oauth.openshift.io/v1                         <span class="nb">false        </span>OAuthAuthorizeToken
oauthclientauthorizations                              oauth.openshift.io/v1                         <span class="nb">false        </span>OAuthClientAuthorization
oauthclients                                           oauth.openshift.io/v1                         <span class="nb">false        </span>OAuthClient
tokenreviews                                           oauth.openshift.io/v1                         <span class="nb">false        </span>TokenReview
useroauthaccesstokens                                  oauth.openshift.io/v1                         <span class="nb">false        </span>UserOAuthAccessToken
authentications                                        operator.openshift.io/v1                      <span class="nb">false        </span>Authentication
cloudcredentials                                       operator.openshift.io/v1                      <span class="nb">false        </span>CloudCredential
clustercsidrivers                                      operator.openshift.io/v1                      <span class="nb">false        </span>ClusterCSIDriver
configs                                                operator.openshift.io/v1                      <span class="nb">false        </span>Config
consoles                                               operator.openshift.io/v1                      <span class="nb">false        </span>Console
csisnapshotcontrollers                                 operator.openshift.io/v1                      <span class="nb">false        </span>CSISnapshotController
dnses                                                  operator.openshift.io/v1                      <span class="nb">false        </span>DNS
etcds                                                  operator.openshift.io/v1                      <span class="nb">false        </span>Etcd
imagecontentsourcepolicies                             operator.openshift.io/v1alpha1                <span class="nb">false        </span>ImageContentSourcePolicy
ingresscontrollers                                     operator.openshift.io/v1                      <span class="nb">true         </span>IngressController
kubeapiservers                                         operator.openshift.io/v1                      <span class="nb">false        </span>KubeAPIServer
kubecontrollermanagers                                 operator.openshift.io/v1                      <span class="nb">false        </span>KubeControllerManager
kubeschedulers                                         operator.openshift.io/v1                      <span class="nb">false        </span>KubeScheduler
kubestorageversionmigrators                            operator.openshift.io/v1                      <span class="nb">false        </span>KubeStorageVersionMigrator
networks                                               operator.openshift.io/v1                      <span class="nb">false        </span>Network
openshiftapiservers                                    operator.openshift.io/v1                      <span class="nb">false        </span>OpenShiftAPIServer
openshiftcontrollermanagers                            operator.openshift.io/v1                      <span class="nb">false        </span>OpenShiftControllerManager
servicecas                                             operator.openshift.io/v1                      <span class="nb">false        </span>ServiceCA
storages                                               operator.openshift.io/v1                      <span class="nb">false        </span>Storage
catalogsources                        catsrc           operators.coreos.com/v1alpha1                 <span class="nb">true         </span>CatalogSource
clusterserviceversions                csv,csvs         operators.coreos.com/v1alpha1                 <span class="nb">true         </span>ClusterServiceVersion
installplans                          ip               operators.coreos.com/v1alpha1                 <span class="nb">true         </span>InstallPlan
operatorconditions                    condition        operators.coreos.com/v2                       <span class="nb">true         </span>OperatorCondition
operatorgroups                        og               operators.coreos.com/v1                       <span class="nb">true         </span>OperatorGroup
operators                                              operators.coreos.com/v1                       <span class="nb">false        </span>Operator
subscriptions                         sub,subs         operators.coreos.com/v1alpha1                 <span class="nb">true         </span>Subscription
packagemanifests                                       packages.operators.coreos.com/v1              <span class="nb">true         </span>PackageManifest
poddisruptionbudgets                  pdb              policy/v1                                     <span class="nb">true         </span>PodDisruptionBudget
podsecuritypolicies                   psp              policy/v1beta1                                <span class="nb">false        </span>PodSecurityPolicy
projectrequests                                        project.openshift.io/v1                       <span class="nb">false        </span>ProjectRequest
projects                                               project.openshift.io/v1                       <span class="nb">false        </span>Project
appliedclusterresourcequotas                           quota.openshift.io/v1                         <span class="nb">true         </span>AppliedClusterResourceQuota
clusterresourcequotas                 clusterquota     quota.openshift.io/v1                         <span class="nb">false        </span>ClusterResourceQuota
clusterrolebindings                                    rbac.authorization.k8s.io/v1                  <span class="nb">false        </span>ClusterRoleBinding
clusterroles                                           rbac.authorization.k8s.io/v1                  <span class="nb">false        </span>ClusterRole
rolebindings                                           rbac.authorization.k8s.io/v1                  <span class="nb">true         </span>RoleBinding
roles                                                  rbac.authorization.k8s.io/v1                  <span class="nb">true         </span>Role
routes                                                 route.openshift.io/v1                         <span class="nb">true         </span>Route
configs                                                samples.operator.openshift.io/v1              <span class="nb">false        </span>Config
priorityclasses                       pc               scheduling.k8s.io/v1                          <span class="nb">false        </span>PriorityClass
rangeallocations                                       security.internal.openshift.io/v1             <span class="nb">false        </span>RangeAllocation
podsecuritypolicyreviews                               security.openshift.io/v1                      <span class="nb">true         </span>PodSecurityPolicyReview
podsecuritypolicyselfsubjectreviews                    security.openshift.io/v1                      <span class="nb">true         </span>PodSecurityPolicySelfSubjectReview
podsecuritypolicysubjectreviews                        security.openshift.io/v1                      <span class="nb">true         </span>PodSecurityPolicySubjectReview
rangeallocations                                       security.openshift.io/v1                      <span class="nb">false        </span>RangeAllocation
securitycontextconstraints            scc              security.openshift.io/v1                      <span class="nb">false        </span>SecurityContextConstraints
csidrivers                                             storage.k8s.io/v1                             <span class="nb">false        </span>CSIDriver
csinodes                                               storage.k8s.io/v1                             <span class="nb">false        </span>CSINode
csistoragecapacities                                   storage.k8s.io/v1beta1                        <span class="nb">true         </span>CSIStorageCapacity
storageclasses                        sc               storage.k8s.io/v1                             <span class="nb">false        </span>StorageClass
volumeattachments                                      storage.k8s.io/v1                             <span class="nb">false        </span>VolumeAttachment
brokertemplateinstances                                template.openshift.io/v1                      <span class="nb">false        </span>BrokerTemplateInstance
processedtemplates                                     template.openshift.io/v1                      <span class="nb">true         </span>Template
templateinstances                                      template.openshift.io/v1                      <span class="nb">true         </span>TemplateInstance
templates                                              template.openshift.io/v1                      <span class="nb">true         </span>Template
profiles                                               tuned.openshift.io/v1                         <span class="nb">true         </span>Profile
tuneds                                                 tuned.openshift.io/v1                         <span class="nb">true         </span>Tuned
<span class="nb">groups                                                 </span>user.openshift.io/v1                          <span class="nb">false        </span>Group
identities                                             user.openshift.io/v1                          <span class="nb">false        </span>Identity
useridentitymappings                                   user.openshift.io/v1                          <span class="nb">false        </span>UserIdentityMapping
<span class="nb">users                                                  </span>user.openshift.io/v1                          <span class="nb">false        </span>User
ippools                                                whereabouts.cni.cncf.io/v1alpha1              <span class="nb">true         </span>IPPool
overlappingrangeipreservations                         whereabouts.cni.cncf.io/v1alpha1              <span class="nb">true         </span>OverlappingRangeIPReservation
</code></pre></div></div>

<h2 id="setting-up-an-application-using-oc-new-app">Setting up an Application using oc new-app</h2>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc <span class="nb">whoami
</span>developer
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc new-project mysql
Now using project <span class="s2">"mysql"</span> on server <span class="s2">"https://api.crc.testing:6443"</span><span class="nb">.</span>

You can add applications to this project with the <span class="s1">'new-app'</span> command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application <span class="k">in </span>Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node <span class="nt">--image</span><span class="o">=</span>k8s.gcr.io/serve_hostname

</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc new-app <span class="nt">--docker-image</span><span class="o">=</span>mysql:latest <span class="nt">--name</span><span class="o">=</span>mysql-openshift <span class="nt">-e</span> <span class="nv">MYSQL_USER</span><span class="o">=</span>myuser <span class="nt">-e</span> <span class="nv">MYSQL_PASSWORD</span><span class="o">=</span>password <span class="nt">-e</span>
<span class="nv">MYSQL_DATABASE</span><span class="o">=</span>mydb <span class="nt">-e</span> <span class="nv">MYSQL_ROOT_PASSWORD</span><span class="o">=</span>password
Flag <span class="nt">--docker-image</span> has been deprecated, Deprecated flag use <span class="nt">--image</span>
<span class="nt">--</span><span class="o">&gt;</span> Found container image 17b062d <span class="o">(</span>5 days old<span class="o">)</span> from Docker Hub <span class="k">for</span> <span class="s2">"mysql:latest"</span>

    <span class="k">*</span> An image stream tag will be created as <span class="s2">"mysql-openshift:latest"</span> that will track this image

<span class="nt">--</span><span class="o">&gt;</span> Creating resources ...
    imagestream.image.openshift.io <span class="s2">"mysql-openshift"</span> created
    deployment.apps <span class="s2">"mysql-openshift"</span> created
    service <span class="s2">"mysql-openshift"</span> created
<span class="nt">--</span><span class="o">&gt;</span> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     <span class="s1">'oc expose service/mysql-openshift'</span>
    Run <span class="s1">'oc status'</span> to view your app.
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc status
In project mysql on server https://api.crc.testing:6443

svc/mysql-openshift - 10.217.4.70 ports 3306, 33060
  deployment/mysql-openshift deploys istag/mysql-openshift:latest
    deployment <span class="c">#2 running for about a minute - 0/1 pods</span>
    deployment <span class="c">#1 deployed 2 minutes ago - 0/1 pods growing to 1</span>


1 info identified, use <span class="s1">'oc status --suggest'</span> to see details.
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/mysql-openshift-f5b68568f-shwz7   1/1     Running   0          8m43s

NAME                      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>              AGE
service/mysql-openshift   ClusterIP   10.217.4.70   &lt;none&gt;        3306/TCP,33060/TCP   8m46s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mysql-openshift   1/1     1            1           8m46s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-openshift-68996c4c8c   0         0         0       8m46s
replicaset.apps/mysql-openshift-f5b68568f    1         1         1       8m43s

NAME                                             IMAGE REPOSITORY                                                                TAGS     UPDATED
imagestream.image.openshift.io/mysql-openshift   default-route-openshift-image-registry.apps-crc.testing/mysql/mysql-openshift   latest   8 minutes ago
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get pods <span class="nt">-o</span> wide
NAME                              READY   STATUS    RESTARTS   AGE     IP            NODE                 NOMINATED NODE   READINESS GATES
mysql-openshift-f5b68568f-shwz7   1/1     Running   0          9m19s   10.217.0.64   crc-xxcfw-master-0   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>

<p>Log in to the web console and see the new application in Project: mysql &gt; Overview</p>

<p><img src="/assets/images/OS-22.png" alt="/assets/images/OS-22" /></p>

<p><img src="/assets/images/OS-23.png" alt="" /></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get events
LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE
12m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-n8wtz"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
12m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-8kv89"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
12m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-fl2b8"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
12m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-wskrp"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
12m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-kgncj"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
12m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-4c8dz"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
12m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-r5ksg"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
12m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-pc94d"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
12m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-n2gkp"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
11m         Warning   FailedCreate        replicaset/mysql-openshift-68996c4c8c   <span class="o">(</span>combined from similar events<span class="o">)</span>: Error creating: Pod <span class="s2">"mysql-openshift-68996c4c8c-5pkcx"</span> is invalid: spec.containers[0].image: Invalid value: <span class="s2">" "</span>: must not have leading or trailing whitespace
12m         Normal    Scheduled           pod/mysql-openshift-f5b68568f-shwz7     Successfully assigned mysql/mysql-openshift-f5b68568f-shwz7 to crc-xxcfw-master-0
12m         Normal    AddedInterface      pod/mysql-openshift-f5b68568f-shwz7     Add eth0 <span class="o">[</span>10.217.0.64/23] from openshift-sdn
10m         Normal    Pulling             pod/mysql-openshift-f5b68568f-shwz7     Pulling image <span class="s2">"mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f"</span>
11m         Warning   Failed              pod/mysql-openshift-f5b68568f-shwz7     Failed to pull image <span class="s2">"mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f"</span>: rpc error: code <span class="o">=</span> Unknown desc <span class="o">=</span> reading blob sha256:5d726bac08eac44e8c39b1afbe20d14e2288e6846dd17299ae5725de86124bc7: Get <span class="s2">"https://registry-1.docker.io/v2/library/mysql/blobs/sha256:5d726bac08eac44e8c39b1afbe20d14e2288e6846dd17299ae5725de86124bc7"</span>: net/http: TLS handshake <span class="nb">timeout
</span>11m         Warning   Failed              pod/mysql-openshift-f5b68568f-shwz7     Error: ErrImagePull
11m         Normal    BackOff             pod/mysql-openshift-f5b68568f-shwz7     Back-off pulling image <span class="s2">"mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f"</span>
11m         Warning   Failed              pod/mysql-openshift-f5b68568f-shwz7     Error: ImagePullBackOff
9m31s       Normal    Pulled              pod/mysql-openshift-f5b68568f-shwz7     Successfully pulled image <span class="s2">"mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f"</span> <span class="k">in </span>1m17.121697748s
9m31s       Normal    Created             pod/mysql-openshift-f5b68568f-shwz7     Created container mysql-openshift
9m31s       Normal    Started             pod/mysql-openshift-f5b68568f-shwz7     Started container mysql-openshift
12m         Normal    SuccessfulCreate    replicaset/mysql-openshift-f5b68568f    Created pod: mysql-openshift-f5b68568f-shwz7
12m         Normal    ScalingReplicaSet   deployment/mysql-openshift              Scaled up replica <span class="nb">set </span>mysql-openshift-68996c4c8c to 1
12m         Normal    ScalingReplicaSet   deployment/mysql-openshift              Scaled up replica <span class="nb">set </span>mysql-openshift-f5b68568f to 1
9m30s       Normal    ScalingReplicaSet   deployment/mysql-openshift              Scaled down replica <span class="nb">set </span>mysql-openshift-68996c4c8c to 0
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc logs pod/mysql-openshift-f5b68568f-shwz7
2022-02-23 04:10:46+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Entrypoint script <span class="k">for </span>MySQL Server 8.0.28-1debian10 started.
2022-02-23 04:10:47+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Initializing database files
2022-02-23T04:10:47.109360Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-013169] <span class="o">[</span>Server] /usr/sbin/mysqld <span class="o">(</span>mysqld 8.0.28<span class="o">)</span> initializing of server <span class="k">in </span>progress as process 23
2022-02-23T04:10:47.119625Z 1 <span class="o">[</span>System] <span class="o">[</span>MY-013576] <span class="o">[</span>InnoDB] InnoDB initialization has started.
2022-02-23T04:10:49.742825Z 1 <span class="o">[</span>System] <span class="o">[</span>MY-013577] <span class="o">[</span>InnoDB] InnoDB initialization has ended.
2022-02-23T04:10:51.821227Z 6 <span class="o">[</span>Warning] <span class="o">[</span>MY-010453] <span class="o">[</span>Server] root@localhost is created with an empty password <span class="o">!</span> Please consider switching off the <span class="nt">--initialize-insecure</span> option.
2022-02-23 04:10:56+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Database files initialized
2022-02-23 04:10:56+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Starting temporary server
2022-02-23T04:10:58.380929Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-010116] <span class="o">[</span>Server] /usr/sbin/mysqld <span class="o">(</span>mysqld 8.0.28<span class="o">)</span> starting as process 72
2022-02-23T04:10:58.577032Z 1 <span class="o">[</span>System] <span class="o">[</span>MY-013576] <span class="o">[</span>InnoDB] InnoDB initialization has started.
2022-02-23T04:10:59.882536Z 1 <span class="o">[</span>System] <span class="o">[</span>MY-013577] <span class="o">[</span>InnoDB] InnoDB initialization has ended.
2022-02-23T04:11:00.781991Z 0 <span class="o">[</span>Warning] <span class="o">[</span>MY-010068] <span class="o">[</span>Server] CA certificate ca.pem is self signed.
2022-02-23T04:11:00.782167Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-013602] <span class="o">[</span>Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported <span class="k">for </span>this channel.
2022-02-23T04:11:00.784296Z 0 <span class="o">[</span>Warning] <span class="o">[</span>MY-011810] <span class="o">[</span>Server] Insecure configuration <span class="k">for</span> <span class="nt">--pid-file</span>: Location <span class="s1">'/var/run/mysqld'</span> <span class="k">in </span>the path is accessible to all OS users. Consider choosing a different directory.
2022-02-23T04:11:00.851759Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-010931] <span class="o">[</span>Server] /usr/sbin/mysqld: ready <span class="k">for </span>connections. Version: <span class="s1">'8.0.28'</span>  socket: <span class="s1">'/var/run/mysqld/mysqld.sock'</span>  port: 0  MySQL Community Server - GPL.
2022-02-23T04:11:00.855887Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-011323] <span class="o">[</span>Server] X Plugin ready <span class="k">for </span>connections. Socket: /var/run/mysqld/mysqlx.sock
2022-02-23 04:11:00+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Temporary server started.
Warning: Unable to load <span class="s1">'/usr/share/zoneinfo/iso3166.tab'</span> as <span class="nb">time </span>zone. Skipping it.
Warning: Unable to load <span class="s1">'/usr/share/zoneinfo/leap-seconds.list'</span> as <span class="nb">time </span>zone. Skipping it.
Warning: Unable to load <span class="s1">'/usr/share/zoneinfo/zone.tab'</span> as <span class="nb">time </span>zone. Skipping it.
Warning: Unable to load <span class="s1">'/usr/share/zoneinfo/zone1970.tab'</span> as <span class="nb">time </span>zone. Skipping it.
2022-02-23 04:11:07+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Creating database mydb
2022-02-23 04:11:07+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Creating user myuser
2022-02-23 04:11:07+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Giving user myuser access to schema mydb

2022-02-23 04:11:07+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Stopping temporary server
2022-02-23T04:11:07.421551Z 13 <span class="o">[</span>System] <span class="o">[</span>MY-013172] <span class="o">[</span>Server] Received SHUTDOWN from user root. Shutting down mysqld <span class="o">(</span>Version: 8.0.28<span class="o">)</span><span class="nb">.</span>
2022-02-23T04:11:09.892006Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-010910] <span class="o">[</span>Server] /usr/sbin/mysqld: Shutdown <span class="nb">complete</span> <span class="o">(</span>mysqld 8.0.28<span class="o">)</span>  MySQL Community Server - GPL.
2022-02-23 04:11:10+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: Temporary server stopped

2022-02-23 04:11:10+00:00 <span class="o">[</span>Note] <span class="o">[</span>Entrypoint]: MySQL init process <span class="k">done</span><span class="nb">.</span> Ready <span class="k">for </span>start up.

2022-02-23T04:11:10.813524Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-010116] <span class="o">[</span>Server] /usr/sbin/mysqld <span class="o">(</span>mysqld 8.0.28<span class="o">)</span> starting as process 1
2022-02-23T04:11:10.841138Z 1 <span class="o">[</span>System] <span class="o">[</span>MY-013576] <span class="o">[</span>InnoDB] InnoDB initialization has started.
2022-02-23T04:11:11.181491Z 1 <span class="o">[</span>System] <span class="o">[</span>MY-013577] <span class="o">[</span>InnoDB] InnoDB initialization has ended.
2022-02-23T04:11:11.468439Z 0 <span class="o">[</span>Warning] <span class="o">[</span>MY-010068] <span class="o">[</span>Server] CA certificate ca.pem is self signed.
2022-02-23T04:11:11.468601Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-013602] <span class="o">[</span>Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported <span class="k">for </span>this channel.
2022-02-23T04:11:11.478843Z 0 <span class="o">[</span>Warning] <span class="o">[</span>MY-011810] <span class="o">[</span>Server] Insecure configuration <span class="k">for</span> <span class="nt">--pid-file</span>: Location <span class="s1">'/var/run/mysqld'</span> <span class="k">in </span>the path is accessible to all OS users. Consider choosing a different directory.
2022-02-23T04:11:11.506637Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-011323] <span class="o">[</span>Server] X Plugin ready <span class="k">for </span>connections. Bind-address: <span class="s1">'::'</span> port: 33060, socket: /var/run/mysqld/mysqlx.sock
2022-02-23T04:11:11.506839Z 0 <span class="o">[</span>System] <span class="o">[</span>MY-010931] <span class="o">[</span>Server] /usr/sbin/mysqld: ready <span class="k">for </span>connections. Version: <span class="s1">'8.0.28'</span>  socket: <span class="s1">'/var/run/mysqld/mysqld.sock'</span>  port: 3306  MySQL Community Server - GPL.
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc describe pod mysql-openshift-f5b68568f-shwz
Name:         mysql-openshift-f5b68568f-shwz7
Namespace:    mysql
Priority:     0
Node:         crc-xxcfw-master-0/192.168.126.11
Start Time:   Wed, 23 Feb 2022 09:37:25 +0530
Labels:       <span class="nv">deployment</span><span class="o">=</span>mysql-openshift
              pod-template-hash<span class="o">=</span>f5b68568f
Annotations:  k8s.v1.cni.cncf.io/network-status:
                <span class="o">[{</span>
                    <span class="s2">"name"</span>: <span class="s2">"openshift-sdn"</span>,
                    <span class="s2">"interface"</span>: <span class="s2">"eth0"</span>,
                    <span class="s2">"ips"</span>: <span class="o">[</span>
                        <span class="s2">"10.217.0.64"</span>
                    <span class="o">]</span>,
                    <span class="s2">"default"</span>: <span class="nb">true</span>,
                    <span class="s2">"dns"</span>: <span class="o">{}</span>
                <span class="o">}]</span>
              k8s.v1.cni.cncf.io/networks-status:
                <span class="o">[{</span>
                    <span class="s2">"name"</span>: <span class="s2">"openshift-sdn"</span>,
                    <span class="s2">"interface"</span>: <span class="s2">"eth0"</span>,
                    <span class="s2">"ips"</span>: <span class="o">[</span>
                        <span class="s2">"10.217.0.64"</span>
                    <span class="o">]</span>,
                    <span class="s2">"default"</span>: <span class="nb">true</span>,
                    <span class="s2">"dns"</span>: <span class="o">{}</span>
                <span class="o">}]</span>
              openshift.io/generated-by: OpenShiftNewApp
              openshift.io/scc: restricted
Status:       Running
IP:           10.217.0.64
IPs:
  IP:           10.217.0.64
Controlled By:  ReplicaSet/mysql-openshift-f5b68568f
Containers:
  mysql-openshift:
    Container ID:   cri-o://bd324439c67da1c3efd52709d4718b54f6e0619c0540c7ca1f8d59a61984dceb
    Image:          mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f
    Image ID:       docker.io/library/mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f
    Ports:          3306/TCP, 33060/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Wed, 23 Feb 2022 09:40:46 +0530
    Ready:          True
    Restart Count:  0
    Environment:
      MYSQL_DATABASE:       mydb
      MYSQL_PASSWORD:       password
      MYSQL_ROOT_PASSWORD:  password
      MYSQL_USER:           myuser
    Mounts:
      /var/lib/mysql from mysql-openshift-volume-1 <span class="o">(</span>rw<span class="o">)</span>
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mqqkz <span class="o">(</span>ro<span class="o">)</span>
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  mysql-openshift-volume-1:
    Type:       EmptyDir <span class="o">(</span>a temporary directory that shares a pod<span class="s1">'s lifetime)
    Medium:
    SizeLimit:  &lt;unset&gt;
  kube-api-access-mqqkz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
    ConfigMapName:           openshift-service-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason          Age                From               Message
  ----     ------          ----               ----               -------
  Normal   Scheduled       17m                default-scheduler  Successfully assigned mysql/mysql-openshift-f5b68568f-shwz7 to crc-xxcfw-master-0
  Normal   AddedInterface  17m                multus             Add eth0 [10.217.0.64/23] from openshift-sdn
  Warning  Failed          16m                kubelet            Failed to pull image "mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f": rpc error: code = Unknown desc = reading blob sha256:5d726bac08eac44e8c39b1afbe20d14e2288e6846dd17299ae5725de86124bc7: Get "https://registry-1.docker.io/v2/library/mysql/blobs/sha256:5d726bac08eac44e8c39b1afbe20d14e2288e6846dd17299ae5725de86124bc7": net/http: TLS handshake timeout
  Warning  Failed          16m                kubelet            Error: ErrImagePull
  Normal   BackOff         16m                kubelet            Back-off pulling image "mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f"
  Warning  Failed          16m                kubelet            Error: ImagePullBackOff
  Normal   Pulling         15m (x2 over 17m)  kubelet            Pulling image "mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f"
  Normal   Pulled          14m                kubelet            Successfully pulled image "mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f" in 1m17.121697748s
  Normal   Created         14m                kubelet            Created container mysql-openshift
  Normal   Started         14m                kubelet            Started container mysql-openshift
</span></code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc projects
You have access to the following projects and can switch between them with <span class="s1">' project &lt;projectname&gt;'</span>:

  <span class="k">*</span> mysql
    pradeep-os-demo

Using project <span class="s2">"mysql"</span> on server <span class="s2">"https://api.crc.testing:6443"</span><span class="nb">.</span>
</code></pre></div></div>

<p>Let us go back to our mysql application and expose it, to create a route.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc expose service/mysql-openshift
route.route.openshift.io/mysql-openshift exposed
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/mysql-openshift-f5b68568f-shwz7   1/1     Running   0          20m

NAME                      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>              AGE
service/mysql-openshift   ClusterIP   10.217.4.70   &lt;none&gt;        3306/TCP,33060/TCP   20m

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mysql-openshift   1/1     1            1           20m

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-openshift-68996c4c8c   0         0         0       20m
replicaset.apps/mysql-openshift-f5b68568f    1         1         1       20m

NAME                                             IMAGE REPOSITORY                                                                TAGS     UPDATED
imagestream.image.openshift.io/mysql-openshift   default-route-openshift-image-registry.apps-crc.testing/mysql/mysql-openshift   latest   20 minutes ago

NAME                                       HOST/PORT                                PATH   SERVICES          PORT       TERMINATION   WILDCARD
route.route.openshift.io/mysql-openshift   mysql-openshift-mysql.apps-crc.testing          mysql-openshift   3306-tcp                 None
</code></pre></div></div>

<p><img src="/assets/images/OS-24.png" alt="/assets/images/OS-24" /></p>

<p>Click on Open URL</p>

<p><img src="/assets/images/OS-25.png" alt="/assets/images/OS-25" /></p>

<h2 id="openshift-building-blocks">OpenShift Building Blocks</h2>

<p>A project is a Kubernetes  namespace that contains all resources running  in 
the OpenShift  application.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc config get-contexts
CURRENT   NAME                                             CLUSTER                AUTHINFO                         NAMESPACE
          /api-crc-testing:6443/developer                  api-crc-testing:6443   developer/api-crc-testing:6443
          crc-admin                                        api-crc-testing:6443   kubeadmin                        default
          crc-developer                                    api-crc-testing:6443   developer                        default
          k8s                                              k8s                    k8s                              default
<span class="k">*</span>         mysql/api-crc-testing:6443/developer             api-crc-testing:6443   developer/api-crc-testing:6443   mysql
          pradeep                                          k8s                    pradeep
          pradeep-os-demo/api-crc-testing:6443/developer   api-crc-testing:6443   developer/api-crc-testing:6443   pradeep-os-demo
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get pods <span class="nt">--show-labels</span>
NAME                              READY   STATUS    RESTARTS   AGE   LABELS
mysql-openshift-f5b68568f-shwz7   1/1     Running   0          32m   <span class="nv">deployment</span><span class="o">=</span>mysql-openshift,pod-template-hash<span class="o">=</span>f5b68568f
</code></pre></div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc describe rs mysql-openshift-f5b68568f
Name:           mysql-openshift-f5b68568f
Namespace:      mysql
Selector:       <span class="nv">deployment</span><span class="o">=</span>mysql-openshift,pod-template-hash<span class="o">=</span>f5b68568f
Labels:         <span class="nv">deployment</span><span class="o">=</span>mysql-openshift
                pod-template-hash<span class="o">=</span>f5b68568f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
                image.openshift.io/triggers:
                  <span class="o">[{</span><span class="s2">"from"</span>:<span class="o">{</span><span class="s2">"kind"</span>:<span class="s2">"ImageStreamTag"</span>,<span class="s2">"name"</span>:<span class="s2">"mysql-openshift:latest"</span><span class="o">}</span>,<span class="s2">"fieldPath"</span>:<span class="s2">"spec.template.spec.containers[?(@.name==</span><span class="se">\"</span><span class="s2">mysql-openshift</span><span class="se">\.</span><span class="s2">..
                openshift.io/generated-by: OpenShiftNewApp
Controlled By:  Deployment/mysql-openshift
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       deployment=mysql-openshift
                pod-template-hash=f5b68568f
  Annotations:  openshift.io/generated-by: OpenShiftNewApp
  Containers:
   mysql-openshift:
    Image:       mysql@sha256:e9c9e3680bbadd5230a62c5548793bd8e59cbcc868032781e48bd53e888bd82f
    Ports:       3306/TCP, 33060/TCP
    Host Ports:  0/TCP, 0/TCP
    Environment:
      MYSQL_DATABASE:       mydb
      MYSQL_PASSWORD:       password
      MYSQL_ROOT_PASSWORD:  password
      MYSQL_USER:           myuser
    Mounts:
      /var/lib/mysql from mysql-openshift-volume-1 (rw)
  Volumes:
   mysql-openshift-volume-1:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  &lt;unset&gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  33m   replicaset-controller  Created pod: mysql-openshift-f5b68568f-shwz7
</span></code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get all <span class="nt">--selector</span> <span class="nv">deployment</span><span class="o">=</span>mysql-openshift
NAME                                  READY   STATUS    RESTARTS   AGE
pod/mysql-openshift-f5b68568f-shwz7   1/1     Running   0          35m

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-openshift-68996c4c8c   0         0         0       35m
replicaset.apps/mysql-openshift-f5b68568f    1         1         1       35m
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc explain pods
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion	&lt;string&gt;
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind	&lt;string&gt;
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata	&lt;Object&gt;
     Standard object<span class="s1">'s metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec	&lt;Object&gt;
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status	&lt;Object&gt;
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status
</span></code></pre></div></div>

<h2 id="creating-a-pod-from-yaml-files">Creating a Pod from YAML files</h2>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc run bnginx <span class="nt">--image</span><span class="o">=</span>bitnami/nginx <span class="nt">--dry-run</span><span class="o">=</span>client <span class="nt">-o</span> yaml <span class="o">&gt;</span> bnginx.yaml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnOpenShift$ cat bnginx.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">run</span><span class="pi">:</span> <span class="s">bnginx</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">bnginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">bitnami/nginx</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">bnginx</span>
    <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">dnsPolicy</span><span class="pi">:</span> <span class="s">ClusterFirst</span>
  <span class="na">restartPolicy</span><span class="pi">:</span> <span class="s">Always</span>
<span class="na">status</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc create <span class="nt">-f</span> bnginx.yaml
pod/bnginx created
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/bnginx                            1/1     Running   0          79s
pod/mysql-openshift-f5b68568f-shwz7   1/1     Running   0          40m

NAME                      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>              AGE
service/mysql-openshift   ClusterIP   10.217.4.70   &lt;none&gt;        3306/TCP,33060/TCP   40m

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mysql-openshift   1/1     1            1           40m

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-openshift-68996c4c8c   0         0         0       40m
replicaset.apps/mysql-openshift-f5b68568f    1         1         1       40m

NAME                                             IMAGE REPOSITORY                                                                TAGS     UPDATED
imagestream.image.openshift.io/mysql-openshift   default-route-openshift-image-registry.apps-crc.testing/mysql/mysql-openshift   latest   40 minutes ago

NAME                                       HOST/PORT                                PATH   SERVICES          PORT       TERMINATION   WILDCARD
route.route.openshift.io/mysql-openshift   mysql-openshift-mysql.apps-crc.testing          mysql-openshift   3306-tcp                 None
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc <span class="nt">-it</span> <span class="nb">exec </span>bnginx <span class="nt">--</span> sh
<span class="nv">$ </span><span class="nb">uname</span> <span class="nt">-a</span>
Linux bnginx 4.18.0-305.30.1.el8_4.x86_64 <span class="c">#1 SMP Tue Nov 30 13:13:11 EST 2021 x86_64 GNU/Linux</span>
<span class="nv">$ </span><span class="nb">exit </span>0
</code></pre></div></div>

<h2 id="using-source-to-image-to-create-applications">Using Source-to-Image to Create Applications</h2>

<p>To create an Image, a Dockerfile could be used.
 Source 2 Image (S2I) takes application source code from a source control 
repository  (such as Git) and builds  a base container based on that to run 
the application. Also while doing so, the image is pushed to the OpenShift registry.</p>

<h3 id="images-and-image-streams">Images and Image Streams</h3>

<p>An Image is a runtime  template that contains all data that is needed  to run  a container</p>

<p>An Image Stream is a collection of  different versions that exist of an image</p>

<p>Builder Image provide  specific programming  language information  that is 
required  to create an application</p>

<p>Please note <code class="language-plaintext highlighter-rouge">is</code> is a short name for <code class="language-plaintext highlighter-rouge">image stream</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get is <span class="nt">-n</span> openshift
NAME                                                  IMAGE REPOSITORY                                                                                                        TAGS                                                     UPDATED
apicast-gateway                                       default-route-openshift-image-registry.apps-crc.testing/openshift/apicast-gateway                                       2.1.0.GA,2.10.0.GA,2.2.0.GA,2.3.0.GA + 7 more...         6 weeks ago
apicurito-ui                                          default-route-openshift-image-registry.apps-crc.testing/openshift/apicurito-ui                                          1.2,1.3,1.4,1.5,1.6,1.7,1.8                              6 weeks ago
cli                                                   default-route-openshift-image-registry.apps-crc.testing/openshift/cli                                                   latest                                                   6 weeks ago
cli-artifacts                                         default-route-openshift-image-registry.apps-crc.testing/openshift/cli-artifacts                                         latest                                                   6 weeks ago
dotnet                                                default-route-openshift-image-registry.apps-crc.testing/openshift/dotnet                                                2.1,2.1-el7,2.1-ubi8,3.1,3.1-el7 + 4 more...             6 weeks ago
dotnet-runtime                                        default-route-openshift-image-registry.apps-crc.testing/openshift/dotnet-runtime                                        2.1,2.1-el7,2.1-ubi8,3.1,3.1-el7 + 4 more...             6 weeks ago
driver-toolkit                                        default-route-openshift-image-registry.apps-crc.testing/openshift/driver-toolkit                                        49.84.202112162103-0,latest                              6 weeks ago
eap-cd-openshift                                      default-route-openshift-image-registry.apps-crc.testing/openshift/eap-cd-openshift                                      12,12.0,13,13.0,14,14.0,15,15.0,16,16.0 + 9 more...      6 weeks ago
eap-cd-runtime-openshift                              default-route-openshift-image-registry.apps-crc.testing/openshift/eap-cd-runtime-openshift                              18,18.0,19,19.0,20,20.0,latest                           6 weeks ago
fis-java-openshift                                    default-route-openshift-image-registry.apps-crc.testing/openshift/fis-java-openshift                                    1.0,2.0                                                  6 weeks ago
fis-karaf-openshift                                   default-route-openshift-image-registry.apps-crc.testing/openshift/fis-karaf-openshift                                   1.0,2.0                                                  6 weeks ago
fuse-apicurito-generator                              default-route-openshift-image-registry.apps-crc.testing/openshift/fuse-apicurito-generator                              1.2,1.3,1.4,1.5,1.6,1.7,1.8                              6 weeks ago
fuse7-console                                         default-route-openshift-image-registry.apps-crc.testing/openshift/fuse7-console                                         1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8                      6 weeks ago
fuse7-eap-openshift                                   default-route-openshift-image-registry.apps-crc.testing/openshift/fuse7-eap-openshift                                   1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8                      6 weeks ago
fuse7-java-openshift                                  default-route-openshift-image-registry.apps-crc.testing/openshift/fuse7-java-openshift                                  1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8                      6 weeks ago
fuse7-karaf-openshift                                 default-route-openshift-image-registry.apps-crc.testing/openshift/fuse7-karaf-openshift                                 1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8                      6 weeks ago
golang                                                default-route-openshift-image-registry.apps-crc.testing/openshift/golang                                                1.13.4-ubi7,1.14.7-ubi8,latest                           6 weeks ago
httpd                                                 default-route-openshift-image-registry.apps-crc.testing/openshift/httpd                                                 2.4,2.4-el7,2.4-el8,latest                               6 weeks ago
installer                                             default-route-openshift-image-registry.apps-crc.testing/openshift/installer                                             latest                                                   6 weeks ago
installer-artifacts                                   default-route-openshift-image-registry.apps-crc.testing/openshift/installer-artifacts                                   latest                                                   6 weeks ago
java                                                  default-route-openshift-image-registry.apps-crc.testing/openshift/java                                                  11,8,latest,openjdk-11-el7,openjdk-11-ubi8 + 2 more...   6 weeks ago
jboss-amq-62                                          default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-amq-62                                          1.1,1.2,1.3,1.4,1.5,1.6,1.7                              6 weeks ago
jboss-amq-63                                          default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-amq-63                                          1.0,1.1,1.2,1.3,1.4                                      6 weeks ago
jboss-datagrid65-client-openshift                     default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-datagrid65-client-openshift                     1.0,1.1                                                  6 weeks ago
jboss-datagrid65-openshift                            default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-datagrid65-openshift                            1.2,1.3,1.4,1.5,1.6                                      6 weeks ago
jboss-datagrid71-client-openshift                     default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-datagrid71-client-openshift                     1.0                                                      6 weeks ago
jboss-datagrid71-openshift                            default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-datagrid71-openshift                            1.0,1.1,1.2,1.3                                          6 weeks ago
jboss-datagrid72-openshift                            default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-datagrid72-openshift                            1.0,1.1,1.2                                              6 weeks ago
jboss-datagrid73-openshift                            default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-datagrid73-openshift                            1.0,1.1,1.2,1.3,1.4                                      6 weeks ago
jboss-datavirt64-driver-openshift                     default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-datavirt64-driver-openshift                     1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7                          6 weeks ago
jboss-datavirt64-openshift                            default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-datavirt64-openshift                            1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7                          6 weeks ago
jboss-decisionserver64-openshift                      default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-decisionserver64-openshift                      1.0,1.1,1.2,1.3,1.4,1.5,1.6                              6 weeks ago
jboss-eap64-openshift                                 default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap64-openshift                                 1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9 + 1 more...          6 weeks ago
jboss-eap70-openshift                                 default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap70-openshift                                 1.3,1.4,1.5,1.6,1.7                                      6 weeks ago
jboss-eap71-openshift                                 default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap71-openshift                                 1.1,1.2,1.3,1.4,latest                                   6 weeks ago
jboss-eap72-openjdk11-openshift-rhel8                 default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap72-openjdk11-openshift-rhel8                 1.0,1.1,latest                                           6 weeks ago
jboss-eap72-openshift                                 default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap72-openshift                                 1.0,1.1,1.2,latest                                       6 weeks ago
jboss-eap73-openjdk11-openshift                       default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap73-openjdk11-openshift                       7.3,7.3.0,latest                                         6 weeks ago
jboss-eap73-openjdk11-runtime-openshift               default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap73-openjdk11-runtime-openshift               7.3,7.3.0,latest                                         6 weeks ago
jboss-eap73-openshift                                 default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap73-openshift                                 7.3,7.3.0,latest                                         6 weeks ago
jboss-eap73-runtime-openshift                         default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap73-runtime-openshift                         7.3,7.3.0,latest                                         6 weeks ago
jboss-eap74-openjdk11-openshift                       default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap74-openjdk11-openshift                       7.4.0,latest                                             6 weeks ago
jboss-eap74-openjdk11-runtime-openshift               default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap74-openjdk11-runtime-openshift               7.4.0,latest                                             6 weeks ago
jboss-eap74-openjdk8-openshift                        default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap74-openjdk8-openshift                        7.4.0,latest                                             6 weeks ago
jboss-eap74-openjdk8-runtime-openshift                default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-eap74-openjdk8-runtime-openshift                7.4.0,latest                                             6 weeks ago
jboss-fuse70-console                                  default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-fuse70-console                                  1.0                                                      6 weeks ago
jboss-fuse70-eap-openshift                            default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-fuse70-eap-openshift                            1.0                                                      6 weeks ago
jboss-fuse70-java-openshift                           default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-fuse70-java-openshift                           1.0                                                      6 weeks ago
jboss-fuse70-karaf-openshift                          default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-fuse70-karaf-openshift                          1.0                                                      6 weeks ago
jboss-processserver64-openshift                       default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-processserver64-openshift                       1.0,1.1,1.2,1.3,1.4,1.5,1.6                              6 weeks ago
jboss-webserver31-tomcat7-openshift                   default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-webserver31-tomcat7-openshift                   1.0,1.1,1.2,1.3,1.4                                      6 weeks ago
jboss-webserver31-tomcat8-openshift                   default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-webserver31-tomcat8-openshift                   1.0,1.1,1.2,1.3,1.4                                      6 weeks ago
jboss-webserver54-openjdk11-tomcat9-openshift-rhel7   default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-webserver54-openjdk11-tomcat9-openshift-rhel7   1.0,latest                                               6 weeks ago
jboss-webserver54-openjdk11-tomcat9-openshift-ubi8    default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-webserver54-openjdk11-tomcat9-openshift-ubi8    1.0,latest                                               6 weeks ago
jboss-webserver54-openjdk8-tomcat9-openshift-rhel7    default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-webserver54-openjdk8-tomcat9-openshift-rhel7    1.0,latest                                               6 weeks ago
jboss-webserver54-openjdk8-tomcat9-openshift-ubi8     default-route-openshift-image-registry.apps-crc.testing/openshift/jboss-webserver54-openjdk8-tomcat9-openshift-ubi8     1.0,latest                                               6 weeks ago
jenkins                                               default-route-openshift-image-registry.apps-crc.testing/openshift/jenkins                                               2,latest                                                 6 weeks ago
jenkins-agent-base                                    default-route-openshift-image-registry.apps-crc.testing/openshift/jenkins-agent-base                                    latest                                                   6 weeks ago
jenkins-agent-maven                                   default-route-openshift-image-registry.apps-crc.testing/openshift/jenkins-agent-maven                                   latest,v4.0                                              6 weeks ago
jenkins-agent-nodejs                                  default-route-openshift-image-registry.apps-crc.testing/openshift/jenkins-agent-nodejs                                  latest,v4.0                                              6 weeks ago
mariadb                                               default-route-openshift-image-registry.apps-crc.testing/openshift/mariadb                                               10.3,10.3-el7,10.3-el8,10.5-el7 + 2 more...              6 weeks ago
must-gather                                           default-route-openshift-image-registry.apps-crc.testing/openshift/must-gather                                           latest                                                   6 weeks ago
mysql                                                 default-route-openshift-image-registry.apps-crc.testing/openshift/mysql                                                 8.0,8.0-el7,8.0-el8,latest                               6 weeks ago
nginx                                                 default-route-openshift-image-registry.apps-crc.testing/openshift/nginx                                                 1.16,1.16-el7,1.16-el8,1.18-ubi7 + 2 more...             6 weeks ago
nodejs                                                default-route-openshift-image-registry.apps-crc.testing/openshift/nodejs                                                12,12-ubi7,12-ubi8,14-ubi7,14-ubi8 + 2 more...           6 weeks ago
oauth-proxy                                           default-route-openshift-image-registry.apps-crc.testing/openshift/oauth-proxy                                           v4.4                                                     6 weeks ago
openjdk-11-rhel7                                      default-route-openshift-image-registry.apps-crc.testing/openshift/openjdk-11-rhel7                                      1.0,1.1                                                  6 weeks ago
openjdk-11-rhel8                                      default-route-openshift-image-registry.apps-crc.testing/openshift/openjdk-11-rhel8                                      1.0                                                      6 weeks ago
perl                                                  default-route-openshift-image-registry.apps-crc.testing/openshift/perl                                                  5.26-ubi8,5.30,5.30-el7,5.30-ubi8 + 1 more...            6 weeks ago
php                                                   default-route-openshift-image-registry.apps-crc.testing/openshift/php                                                   7.3,7.3-ubi7,7.3-ubi8,7.4-ubi8,latest                    6 weeks ago
postgresql                                            default-route-openshift-image-registry.apps-crc.testing/openshift/postgresql                                            10,10-el7,10-el8,12,12-el7,12-el8 + 4 more...            6 weeks ago
python                                                default-route-openshift-image-registry.apps-crc.testing/openshift/python                                                2.7,2.7-ubi7,2.7-ubi8,3.6-ubi8,3.8 + 4 more...           6 weeks ago
redhat-openjdk18-openshift                            default-route-openshift-image-registry.apps-crc.testing/openshift/redhat-openjdk18-openshift                            1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8                      6 weeks ago
redhat-sso70-openshift                                default-route-openshift-image-registry.apps-crc.testing/openshift/redhat-sso70-openshift                                1.3,1.4                                                  6 weeks ago
redhat-sso71-openshift                                default-route-openshift-image-registry.apps-crc.testing/openshift/redhat-sso71-openshift                                1.0,1.1,1.2,1.3                                          6 weeks ago
redhat-sso72-openshift                                default-route-openshift-image-registry.apps-crc.testing/openshift/redhat-sso72-openshift                                1.0,1.1,1.2,1.3,1.4                                      6 weeks ago
redhat-sso73-openshift                                default-route-openshift-image-registry.apps-crc.testing/openshift/redhat-sso73-openshift                                1.0,latest                                               6 weeks ago
redis                                                 default-route-openshift-image-registry.apps-crc.testing/openshift/redis                                                 5,5-el7,5-el8,latest                                     6 weeks ago
rhdm-decisioncentral-rhel8                            default-route-openshift-image-registry.apps-crc.testing/openshift/rhdm-decisioncentral-rhel8                            7.11.0                                                   6 weeks ago
rhdm-kieserver-rhel8                                  default-route-openshift-image-registry.apps-crc.testing/openshift/rhdm-kieserver-rhel8                                  7.11.0                                                   6 weeks ago
rhpam-businesscentral-monitoring-rhel8                default-route-openshift-image-registry.apps-crc.testing/openshift/rhpam-businesscentral-monitoring-rhel8                7.11.0                                                   6 weeks ago
rhpam-businesscentral-rhel8                           default-route-openshift-image-registry.apps-crc.testing/openshift/rhpam-businesscentral-rhel8                           7.11.0                                                   6 weeks ago
rhpam-kieserver-rhel8                                 default-route-openshift-image-registry.apps-crc.testing/openshift/rhpam-kieserver-rhel8                                 7.11.0                                                   6 weeks ago
rhpam-smartrouter-rhel8                               default-route-openshift-image-registry.apps-crc.testing/openshift/rhpam-smartrouter-rhel8                               7.11.0                                                   6 weeks ago
ruby                                                  default-route-openshift-image-registry.apps-crc.testing/openshift/ruby                                                  2.5-ubi8,2.6,2.6-ubi7,2.6-ubi8,2.7 + 4 more...           6 weeks ago
sso74-openshift-rhel8                                 default-route-openshift-image-registry.apps-crc.testing/openshift/sso74-openshift-rhel8                                 7.4,latest                                               6 weeks ago
tests                                                 default-route-openshift-image-registry.apps-crc.testing/openshift/tests                                                 latest                                                   6 weeks ago
tools                                                 default-route-openshift-image-registry.apps-crc.testing/openshift/tools                                                 latest                                                   6 weeks ago
ubi8-openjdk-11                                       default-route-openshift-image-registry.apps-crc.testing/openshift/ubi8-openjdk-11                                       1.3                                                      6 weeks ago
ubi8-openjdk-8                                        default-route-openshift-image-registry.apps-crc.testing/openshift/ubi8-openjdk-8                                        1.3                                                      6 weeks ag
</code></pre></div></div>

<p>The resulting  custom runtime image is used to run  your own application
Custom images are stored in the OpenShift internal registry, and are accessible  from the current project</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc <span class="nt">-o</span> yaml new-app php~https://github.com/pradeepgadde/simple-openshift-app <span class="nt">--name</span><span class="o">=</span>simple1 <span class="o">&gt;</span>
 s2i.yaml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">pradeep@learnOpenShift$ cat s2i.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">items</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">image.openshift.io/v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">ImageStream</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">openshift.io/generated-by</span><span class="pi">:</span> <span class="s">OpenShiftNewApp</span>
    <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">simple1</span>
      <span class="na">app.kubernetes.io/component</span><span class="pi">:</span> <span class="s">simple1</span>
      <span class="na">app.kubernetes.io/instance</span><span class="pi">:</span> <span class="s">simple1</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">simple1</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">lookupPolicy</span><span class="pi">:</span>
      <span class="na">local</span><span class="pi">:</span> <span class="no">false</span>
  <span class="na">status</span><span class="pi">:</span>
    <span class="na">dockerImageRepository</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
<span class="pi">-</span> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">build.openshift.io/v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">BuildConfig</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">openshift.io/generated-by</span><span class="pi">:</span> <span class="s">OpenShiftNewApp</span>
    <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">simple1</span>
      <span class="na">app.kubernetes.io/component</span><span class="pi">:</span> <span class="s">simple1</span>
      <span class="na">app.kubernetes.io/instance</span><span class="pi">:</span> <span class="s">simple1</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">simple1</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">nodeSelector</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">output</span><span class="pi">:</span>
      <span class="na">to</span><span class="pi">:</span>
        <span class="na">kind</span><span class="pi">:</span> <span class="s">ImageStreamTag</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">simple1:latest</span>
    <span class="na">postCommit</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">source</span><span class="pi">:</span>
      <span class="na">git</span><span class="pi">:</span>
        <span class="na">uri</span><span class="pi">:</span> <span class="s">https://github.com/pradeepgadde/simple-openshift-app</span>
      <span class="na">type</span><span class="pi">:</span> <span class="s">Git</span>
    <span class="na">strategy</span><span class="pi">:</span>
      <span class="na">sourceStrategy</span><span class="pi">:</span>
        <span class="na">from</span><span class="pi">:</span>
          <span class="na">kind</span><span class="pi">:</span> <span class="s">ImageStreamTag</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">php:7.4-ubi8</span>
          <span class="na">namespace</span><span class="pi">:</span> <span class="s">openshift</span>
      <span class="na">type</span><span class="pi">:</span> <span class="s">Source</span>
    <span class="na">triggers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">github</span><span class="pi">:</span>
        <span class="na">secret</span><span class="pi">:</span> <span class="s">2eqGHBsvG0TW29dPD4xB</span>
      <span class="na">type</span><span class="pi">:</span> <span class="s">GitHub</span>
    <span class="pi">-</span> <span class="na">generic</span><span class="pi">:</span>
        <span class="na">secret</span><span class="pi">:</span> <span class="s">GIZUWKlkGO4IDlvLLQP_</span>
      <span class="na">type</span><span class="pi">:</span> <span class="s">Generic</span>
    <span class="pi">-</span> <span class="na">type</span><span class="pi">:</span> <span class="s">ConfigChange</span>
    <span class="pi">-</span> <span class="na">imageChange</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">type</span><span class="pi">:</span> <span class="s">ImageChange</span>
  <span class="na">status</span><span class="pi">:</span>
    <span class="na">lastVersion</span><span class="pi">:</span> <span class="m">0</span>
<span class="pi">-</span> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">image.openshift.io/triggers</span><span class="pi">:</span> <span class="s1">'</span><span class="s">[{"from":{"kind":"ImageStreamTag","name":"simple1:latest"},"fieldPath":"spec.template.spec.containers[?(@.name==\"simple1\")].image"}]'</span>
      <span class="na">openshift.io/generated-by</span><span class="pi">:</span> <span class="s">OpenShiftNewApp</span>
    <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">simple1</span>
      <span class="na">app.kubernetes.io/component</span><span class="pi">:</span> <span class="s">simple1</span>
      <span class="na">app.kubernetes.io/instance</span><span class="pi">:</span> <span class="s">simple1</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">simple1</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">matchLabels</span><span class="pi">:</span>
        <span class="na">deployment</span><span class="pi">:</span> <span class="s">simple1</span>
    <span class="na">strategy</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">template</span><span class="pi">:</span>
      <span class="na">metadata</span><span class="pi">:</span>
        <span class="na">annotations</span><span class="pi">:</span>
          <span class="na">openshift.io/generated-by</span><span class="pi">:</span> <span class="s">OpenShiftNewApp</span>
        <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
        <span class="na">labels</span><span class="pi">:</span>
          <span class="na">deployment</span><span class="pi">:</span> <span class="s">simple1</span>
      <span class="na">spec</span><span class="pi">:</span>
        <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s1">'</span><span class="nv"> </span><span class="s">'</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">simple1</span>
          <span class="na">ports</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
            <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
          <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8443</span>
            <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
          <span class="na">resources</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">status</span><span class="pi">:</span> <span class="pi">{}</span>
<span class="pi">-</span> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">openshift.io/generated-by</span><span class="pi">:</span> <span class="s">OpenShiftNewApp</span>
    <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">simple1</span>
      <span class="na">app.kubernetes.io/component</span><span class="pi">:</span> <span class="s">simple1</span>
      <span class="na">app.kubernetes.io/instance</span><span class="pi">:</span> <span class="s">simple1</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">simple1</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">8080-tcp</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">8443-tcp</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">8443</span>
      <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8443</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">deployment</span><span class="pi">:</span> <span class="s">simple1</span>
  <span class="na">status</span><span class="pi">:</span>
    <span class="na">loadBalancer</span><span class="pi">:</span> <span class="pi">{}</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">List</span>
<span class="na">metadata</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc apply <span class="nt">-f</span> s2i.yaml
imagestream.image.openshift.io/simple1 created
buildconfig.build.openshift.io/simple1 created
deployment.apps/simple1 created
service/simple1 created
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc status
In project mysql on server https://api.crc.testing:6443

http://mysql-openshift-mysql.apps-crc.testing to pod port 3306-tcp <span class="o">(</span>svc/mysql-openshift<span class="o">)</span>
  deployment/mysql-openshift deploys istag/mysql-openshift:latest
    deployment <span class="c">#2 running for about an hour - 1 pod</span>
    deployment <span class="c">#1 deployed about an hour ago</span>

svc/simple1 - 10.217.5.217 ports 8080, 8443
  deployment/simple1 deploys istag/simple1:latest &lt;-
    bc/simple1 <span class="nb">source </span>builds https://github.com/pradeepgadde/simple-openshift-app on openshift/php:7.4-ubi8
      build <span class="c">#1 running for 51 seconds - 7695b44: Getting Started with OpenShift by sandervanvugt (Pradeep Gadde &lt;63095349+pradeepgadde@users.noreply.github.com&gt;)</span>
    deployment <span class="c">#1 running for 53 seconds - 0/1 pods growing to 1</span>

pod/bnginx runs bitnami/nginx


2 infos identified, use <span class="s1">'oc status --suggest'</span> to see details.
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get builds
NAME        TYPE     FROM          STATUS    STARTED         DURATION
simple1-1   Source   Git@7695b44   Running   2 minutes ago
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get pods
NAME                              READY   STATUS    RESTARTS   AGE
bnginx                            1/1     Running   0          33m
mysql-openshift-f5b68568f-shwz7   1/1     Running   0          73m
simple1-1-build                   1/1     Running   0          2m41s
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get pods <span class="nt">-o</span> wide
NAME                              READY   STATUS    RESTARTS   AGE     IP            NODE                 NOMINATED NODE   READINESS GATES
bnginx                            1/1     Running   0          33m     10.217.0.83   crc-xxcfw-master-0   &lt;none&gt;           &lt;none&gt;
mysql-openshift-f5b68568f-shwz7   1/1     Running   0          73m     10.217.0.64   crc-xxcfw-master-0   &lt;none&gt;           &lt;none&gt;
simple1-1-build                   1/1     Running   0          2m48s   10.217.0.98   crc-xxcfw-master-0   &lt;none&gt;           &lt;none&gt;
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get deploy
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
mysql-openshift   1/1     1            1           74m
simple1           0/1     0            0           3m52s
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get all
NAME                                  READY   STATUS    RESTARTS   AGE
pod/bnginx                            1/1     Running   0          36m
pod/mysql-openshift-f5b68568f-shwz7   1/1     Running   0          75m
pod/simple1-1-build                   1/1     Running   0          5m

NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>              AGE
service/mysql-openshift   ClusterIP   10.217.4.70    &lt;none&gt;        3306/TCP,33060/TCP   75m
service/simple1           ClusterIP   10.217.5.217   &lt;none&gt;        8080/TCP,8443/TCP    5m2s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mysql-openshift   1/1     1            1           75m
deployment.apps/simple1           0/1     0            0           5m3s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-openshift-68996c4c8c   0         0         0       75m
replicaset.apps/mysql-openshift-f5b68568f    1         1         1       75m
replicaset.apps/simple1-59c9b6d884           1         0         0       5m3s

NAME                                     TYPE     FROM   LATEST
buildconfig.build.openshift.io/simple1   Source   Git    1

NAME                                 TYPE     FROM          STATUS    STARTED         DURATION
build.build.openshift.io/simple1-1   Source   Git@7695b44   Running   5 minutes ago

NAME                                             IMAGE REPOSITORY                                                                TAGS     UPDATED
imagestream.image.openshift.io/mysql-openshift   default-route-openshift-image-registry.apps-crc.testing/mysql/mysql-openshift   latest   About an hour ago
imagestream.image.openshift.io/simple1           default-route-openshift-image-registry.apps-crc.testing/mysql/simple1

NAME                                       HOST/PORT                                PATH   SERVICES          PORT       TERMINATION   WILDCARD
route.route.openshift.io/mysql-openshift   mysql-openshift-mysql.apps-crc.testing          mysql-openshift   3306-tcp                 None
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc describe build simple1-1
Name:		simple1-1
Namespace:	mysql
Created:	6 minutes ago
Labels:		<span class="nv">app</span><span class="o">=</span>simple1
		app.kubernetes.io/component<span class="o">=</span>simple1
		app.kubernetes.io/instance<span class="o">=</span>simple1
		<span class="nv">buildconfig</span><span class="o">=</span>simple1
		openshift.io/build-config.name<span class="o">=</span>simple1
		openshift.io/build.start-policy<span class="o">=</span>Serial
Annotations:	openshift.io/build-config.name<span class="o">=</span>simple1
		openshift.io/build.number<span class="o">=</span>1
		openshift.io/build.pod-name<span class="o">=</span>simple1-1-build

Status:		Complete
Started:	Wed, 23 Feb 2022 10:47:46 IST
Duration:	5m44s
  FetchInputs:	  2s
  PullImages:	  4m50s
  Build:	  20s
  PushImage:	  6s

Build Config:	simple1
Build Pod:	simple1-1-build
Image Digest:	sha256:953bec8b4ee57661ae61df0ab8ff7d1ad568d3dead2a1f0f785d1e2cd25c71e0

Strategy:		Source
URL:			https://github.com/pradeepgadde/simple-openshift-app
Commit:			7695b44 <span class="o">(</span>Getting Started with OpenShift by sandervanvugt<span class="o">)</span>
Author/Committer:	Pradeep Gadde / GitHub
From Image:		DockerImage image-registry.openshift-image-registry.svc:5000/openshift/php@sha256:34f2d80adac5a8aeb65c045e3a8c1caea56d53704d8d5cddec588c9008ceb5ea
Pull Secret Name:	builder-dockercfg-n8v7b
Volumes:		&lt;none&gt;
Output to:		ImageStreamTag simple1:latest
Push Secret:		builder-dockercfg-n8v7b

Build trigger cause:	Image change
Image ID:		image-registry.openshift-image-registry.svc:5000/openshift/php@sha256:34f2d80adac5a8aeb65c045e3a8c1caea56d53704d8d5cddec588c9008ceb5ea
Image Name/Kind:	php:7.4-ubi8 / ImageStreamTag

Events:
  Type		Reason		Age			From			Message
  <span class="nt">----</span>		<span class="nt">------</span>		<span class="nt">----</span>			<span class="nt">----</span>			<span class="nt">-------</span>
  Normal	Scheduled	6m33s			default-scheduler	Successfully assigned mysql/simple1-1-build to crc-xxcfw-master-0
  Normal	AddedInterface	6m28s			multus			Add eth0 <span class="o">[</span>10.217.0.98/23] from openshift-sdn
  Normal	Pulled		6m27s			kubelet			Container image <span class="s2">"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:961b30e0ab62cf04d000c1eb8330e62ff2daf4545ee094d3c9acd4c443e3c5d2"</span> already present on machine
  Normal	Created		6m27s			kubelet			Created container git-clone
  Normal	Started		6m26s			kubelet			Started container git-clone
  Normal	BuildStarted	6m26s			build-controller	Build mysql/simple1-1 is now running
  Normal	Pulled		6m22s			kubelet			Container image <span class="s2">"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:961b30e0ab62cf04d000c1eb8330e62ff2daf4545ee094d3c9acd4c443e3c5d2"</span> already present on machine
  Normal	Created		6m13s			kubelet			Created container manage-dockerfile
  Normal	Started		6m9s			kubelet			Started container manage-dockerfile
  Normal	Pulled		6m8s			kubelet			Container image <span class="s2">"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:961b30e0ab62cf04d000c1eb8330e62ff2daf4545ee094d3c9acd4c443e3c5d2"</span> already present on machine
  Normal	Created		6m8s			kubelet			Created container sti-build
  Normal	Started		6m7s			kubelet			Started container sti-build
  Normal	BuildCompleted	48s			build-controller	Build mysql/simple1-1 completed successfully
  Warning	FailedMount	47s <span class="o">(</span>x3 over 49s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"build-proxy-ca-bundles"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"simple1-1-global-ca"</span> not registered
  Warning	FailedMount	47s <span class="o">(</span>x3 over 49s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"build-system-configs"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"simple1-1-sys-config"</span> not registered
  Warning	FailedMount	47s <span class="o">(</span>x3 over 49s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"build-ca-bundles"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"simple1-1-ca"</span> not registered
  Warning	FailedMount	47s <span class="o">(</span>x3 over 49s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"builder-dockercfg-n8v7b-pull"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"builder-dockercfg-n8v7b"</span> not registered
  Warning	FailedMount	47s <span class="o">(</span>x3 over 49s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"builder-dockercfg-n8v7b-push"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"builder-dockercfg-n8v7b"</span> not registered
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc explain buildconfig.spec.triggers
KIND:     BuildConfig
VERSION:  build.openshift.io/v1

RESOURCE: triggers &lt;<span class="o">[]</span>Object&gt;

DESCRIPTION:
     triggers determine how new Builds can be launched from a BuildConfig. If no
     triggers are defined, a new build can only occur as a result of an explicit
     client build creation.

     BuildTriggerPolicy describes a policy <span class="k">for </span>a single trigger that results <span class="k">in
     </span>a new Build.

FIELDS:
   bitbucket	&lt;Object&gt;
     BitbucketWebHook contains the parameters <span class="k">for </span>a Bitbucket webhook <span class="nb">type </span>of
     trigger

   generic	&lt;Object&gt;
     generic contains the parameters <span class="k">for </span>a Generic webhook <span class="nb">type </span>of trigger

   github	&lt;Object&gt;
     github contains the parameters <span class="k">for </span>a GitHub webhook <span class="nb">type </span>of trigger

   gitlab	&lt;Object&gt;
     GitLabWebHook contains the parameters <span class="k">for </span>a GitLab webhook <span class="nb">type </span>of trigger

   imageChange	&lt;Object&gt;
     imageChange contains parameters <span class="k">for </span>an ImageChange <span class="nb">type </span>of trigger

   <span class="nb">type</span>	&lt;string&gt; <span class="nt">-required-</span>
     <span class="nb">type </span>is the <span class="nb">type </span>of build trigger. Valid values:
     - GitHub GitHubWebHookBuildTriggerType represents a trigger that launches
     builds on GitHub webhook invocations

     - Generic GenericWebHookBuildTriggerType represents a trigger that launches
     builds on generic webhook invocations

     - GitLab GitLabWebHookBuildTriggerType represents a trigger that launches
     builds on GitLab webhook invocations

     - Bitbucket BitbucketWebHookBuildTriggerType represents a trigger that
     launches builds on Bitbucket webhook invocations

     - ImageChange ImageChangeBuildTriggerType represents a trigger that
     launches builds on availability of a new version of an image

     - ConfigChange ConfigChangeBuildTriggerType will trigger a build on an
     initial build config creation WARNING: In the future the behavior will
     change to trigger a build on any config change
</code></pre></div></div>

<p>Let us switch to the web console and look at the <code class="language-plaintext highlighter-rouge">mysql</code> project and applications running currently inside it.</p>

<p><img src="/assets/images/OS-26.png" alt="/assets/images/OS-26" /></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc expose service simple1
route.route.openshift.io/simple1 exposed
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get all
NAME                                  READY   STATUS      RESTARTS   AGE
pod/bnginx                            1/1     Running     2          88m
pod/mysql-openshift-f5b68568f-shwz7   1/1     Running     2          127m
pod/simple1-1-build                   0/1     Completed   0          57m
pod/simple1-7678fbccc9-dnzbd          1/1     Running     2          51m

NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>              AGE
service/mysql-openshift   ClusterIP   10.217.4.70    &lt;none&gt;        3306/TCP,33060/TCP   127m
service/simple1           ClusterIP   10.217.5.217   &lt;none&gt;        8080/TCP,8443/TCP    57m

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mysql-openshift   1/1     1            1           127m
deployment.apps/simple1           1/1     1            1           57m

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-openshift-68996c4c8c   0         0         0       127m
replicaset.apps/mysql-openshift-f5b68568f    1         1         1       127m
replicaset.apps/simple1-59c9b6d884           0         0         0       57m
replicaset.apps/simple1-7678fbccc9           1         1         1       51m

NAME                                     TYPE     FROM   LATEST
buildconfig.build.openshift.io/simple1   Source   Git    1

NAME                                 TYPE     FROM          STATUS     STARTED          DURATION
build.build.openshift.io/simple1-1   Source   Git@7695b44   Complete   57 minutes ago   5m44s

NAME                                             IMAGE REPOSITORY                                                                TAGS     UPDATED
imagestream.image.openshift.io/mysql-openshift   default-route-openshift-image-registry.apps-crc.testing/mysql/mysql-openshift   latest   2 hours ago
imagestream.image.openshift.io/simple1           default-route-openshift-image-registry.apps-crc.testing/mysql/simple1           latest   51 minutes ago

NAME                                       HOST/PORT                                PATH   SERVICES          PORT       TERMINATION   WILDCARD
route.route.openshift.io/mysql-openshift   mysql-openshift-mysql.apps-crc.testing          mysql-openshift   3306-tcp                 None
route.route.openshift.io/simple1           simple1-mysql.apps-crc.testing                  simple1           8080-tcp                 None
</code></pre></div></div>

<p><img src="/assets/images/OS-27.png" alt="" /></p>

<p><img src="/assets/images/OS-28.png" alt="" /></p>

<p><img src="/assets/images/OS-29.png" alt="" /></p>

<h2 id="manually-triggering-updates">Manually Triggering Updates</h2>

<p>Edit the <code class="language-plaintext highlighter-rouge">index.php</code> in the simple-app  git repository. Remove the PHP Test info section and modify the body to show greetings in English!</p>

<p>Use <code class="language-plaintext highlighter-rouge">oc start-build simple1</code> to trigger the build procedure again</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc start-build simple1
build.build.openshift.io/simple1-2 started
</code></pre></div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get build
NAME        TYPE     FROM          STATUS     STARTED             DURATION
simple1-1   Source   Git@7695b44   Complete   About an hour ago   5m44s
simple1-2   Source   Git@854c5cb   Running    18 seconds ago
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc describe build simple1-2
Name:		simple1-2
Namespace:	mysql
Created:	About a minute ago
Labels:		<span class="nv">app</span><span class="o">=</span>simple1
		app.kubernetes.io/component<span class="o">=</span>simple1
		app.kubernetes.io/instance<span class="o">=</span>simple1
		<span class="nv">buildconfig</span><span class="o">=</span>simple1
		openshift.io/build-config.name<span class="o">=</span>simple1
		openshift.io/build.start-policy<span class="o">=</span>Serial
Annotations:	openshift.io/build-config.name<span class="o">=</span>simple1
		openshift.io/build.number<span class="o">=</span>2
		openshift.io/build.pod-name<span class="o">=</span>simple1-2-build

Status:		Complete
Started:	Wed, 23 Feb 2022 11:57:06 IST
Duration:	1m39s
  FetchInputs:	  1s
  PullImages:	  1m18s
  Build:	  8s
  PushImage:	  1s

Build Config:	simple1
Build Pod:	simple1-2-build
Image Digest:	sha256:d4cce6a7d47b33f3cb8d071c210829a71b24f8a2e3c263fc65fdd884eeebbacb

Strategy:		Source
URL:			https://github.com/pradeepgadde/simple-openshift-app
Commit:			854c5cb <span class="o">(</span>To <span class="nb">test </span>manual trigger of  OpenShift Build<span class="o">)</span>
Author/Committer:	Pradeep Gadde / GitHub
From Image:		DockerImage image-registry.openshift-image-registry.svc:5000/openshift/php@sha256:34f2d80adac5a8aeb65c045e3a8c1caea56d53704d8d5cddec588c9008ceb5ea
Pull Secret Name:	builder-dockercfg-n8v7b
Volumes:		&lt;none&gt;
Output to:		ImageStreamTag simple1:latest
Push Secret:		builder-dockercfg-n8v7b

Build trigger cause:	Manually triggered

Events:
  Type		Reason		Age			From			Message
  <span class="nt">----</span>		<span class="nt">------</span>		<span class="nt">----</span>			<span class="nt">----</span>			<span class="nt">-------</span>
  Normal	Scheduled	113s			default-scheduler	Successfully assigned mysql/simple1-2-build to crc-xxcfw-master-0
  Normal	Started		111s			kubelet			Started container git-clone
  Normal	AddedInterface	111s			multus			Add eth0 <span class="o">[</span>10.217.0.59/23] from openshift-sdn
  Normal	Pulled		111s			kubelet			Container image <span class="s2">"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:961b30e0ab62cf04d000c1eb8330e62ff2daf4545ee094d3c9acd4c443e3c5d2"</span> already present on machine
  Normal	Created		111s			kubelet			Created container git-clone
  Normal	BuildStarted	110s			build-controller	Build mysql/simple1-2 is now running
  Normal	Created		108s			kubelet			Created container manage-dockerfile
  Normal	Pulled		108s			kubelet			Container image <span class="s2">"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:961b30e0ab62cf04d000c1eb8330e62ff2daf4545ee094d3c9acd4c443e3c5d2"</span> already present on machine
  Normal	Started		107s			kubelet			Started container manage-dockerfile
  Normal	Pulled		106s			kubelet			Container image <span class="s2">"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:961b30e0ab62cf04d000c1eb8330e62ff2daf4545ee094d3c9acd4c443e3c5d2"</span> already present on machine
  Normal	Created		106s			kubelet			Created container sti-build
  Normal	Started		106s			kubelet			Started container sti-build
  Normal	BuildCompleted	15s			build-controller	Build mysql/simple1-2 completed successfully
  Warning	FailedMount	15s <span class="o">(</span>x2 over 15s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"build-ca-bundles"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"simple1-2-ca"</span> not registered
  Warning	FailedMount	15s <span class="o">(</span>x2 over 15s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"build-system-configs"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"simple1-2-sys-config"</span> not registered
  Warning	FailedMount	15s <span class="o">(</span>x2 over 15s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"build-proxy-ca-bundles"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"simple1-2-global-ca"</span> not registered
  Warning	FailedMount	15s <span class="o">(</span>x2 over 15s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"builder-dockercfg-n8v7b-push"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"builder-dockercfg-n8v7b"</span> not registered
  Warning	FailedMount	15s <span class="o">(</span>x2 over 15s<span class="o">)</span>	kubelet			MountVolume.SetUp failed <span class="k">for </span>volume <span class="s2">"builder-dockercfg-n8v7b-pull"</span> : object <span class="s2">"mysql"</span>/<span class="s2">"builder-dockercfg-n8v7b"</span> not registere
</code></pre></div></div>

<p><img src="/assets/images/OS-30.png" alt="" /></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get all
NAME                                  READY   STATUS      RESTARTS   AGE
pod/bnginx                            1/1     Running     2          103m
pod/mysql-openshift-f5b68568f-shwz7   1/1     Running     2          143m
pod/simple1-1-build                   0/1     Completed   0          72m
pod/simple1-2-build                   0/1     Completed   0          3m32s
pod/simple1-56465c7f74-xp9m8          1/1     Running     0          115s

NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>              AGE
service/mysql-openshift   ClusterIP   10.217.4.70    &lt;none&gt;        3306/TCP,33060/TCP   143m
service/simple1           ClusterIP   10.217.5.217   &lt;none&gt;        8080/TCP,8443/TCP    72m

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mysql-openshift   1/1     1            1           143m
deployment.apps/simple1           1/1     1            1           72m

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-openshift-68996c4c8c   0         0         0       143m
replicaset.apps/mysql-openshift-f5b68568f    1         1         1       143m
replicaset.apps/simple1-56465c7f74           1         1         1       115s
replicaset.apps/simple1-59c9b6d884           0         0         0       72m
replicaset.apps/simple1-7678fbccc9           0         0         0       67m

NAME                                     TYPE     FROM   LATEST
buildconfig.build.openshift.io/simple1   Source   Git    2

NAME                                 TYPE     FROM          STATUS     STARTED             DURATION
build.build.openshift.io/simple1-1   Source   Git@7695b44   Complete   About an hour ago   5m44s
build.build.openshift.io/simple1-2   Source   Git@854c5cb   Complete   3 minutes ago       1m39s

NAME                                             IMAGE REPOSITORY                                                                TAGS     UPDATED
imagestream.image.openshift.io/mysql-openshift   default-route-openshift-image-registry.apps-crc.testing/mysql/mysql-openshift   latest   2 hours ago
imagestream.image.openshift.io/simple1           default-route-openshift-image-registry.apps-crc.testing/mysql/simple1           latest   About a minute ago

NAME                                       HOST/PORT                                PATH   SERVICES          PORT       TERMINATION   WILDCARD
route.route.openshift.io/mysql-openshift   mysql-openshift-mysql.apps-crc.testing          mysql-openshift   3306-tcp                 None
route.route.openshift.io/simple1           simple1-mysql.apps-crc.testing                  simple1           8080-tcp                 None
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>curl simple1-mysql.apps-crc.testing
&lt;html&gt;
	&lt;<span class="nb">head</span><span class="o">&gt;</span>
	&lt;title&gt;PHP Test&lt;/title&gt;
	&lt;meta http-equiv<span class="o">=</span><span class="s2">"Content-Type"</span> <span class="nv">content</span><span class="o">=</span><span class="s2">"text/html; charset=ISO-8859-1"</span>/&gt;
	&lt;/head&gt;

&lt;body&gt;
	&lt;h1&gt;PHP Test&lt;/h1&gt;
		&lt;p&gt;&lt;b&gt;An Example of PHP <span class="k">in </span>Action&lt;/b&gt;&lt;/p&gt;
		The Current Date and Time is: &lt;br /&gt;6:30 AM Wednesday, February 23 2022. &lt;/p&gt;

        &lt;h2&gt;Greetings from Pradeep Gadde!&lt;/h2&gt;
	&lt;/body&gt;
&lt;/html&gt;
</code></pre></div></div>

<p>These tests confirm the changes.</p>

<h3 id="understanding-routes">Understanding Routes</h3>

<p>Routes are an alternative to Kubernetes Ingress</p>

<p>Routes are OpenShift resources that allow for network access to pods from users and applications outside the OpenShift instance</p>

<p>A route connects a public IP address and DNS name to the internal service IP address</p>

<p>OpenShift uses a HAProxy based shared router service to implement the route.</p>

<p>When exposing a service, the DNS name is auto-generated as <code class="language-plaintext highlighter-rouge">routename-projectname-defaultDNSdomain</code></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pradeep@learnOpenShift<span class="nv">$ </span>oc get route
NAME              HOST/PORT                                PATH   SERVICES          PORT       TERMINATION   WILDCARD
mysql-openshift   mysql-openshift-mysql.apps-crc.testing          mysql-openshift   3306-tcp                 None
simple1           simple1-mysql.apps-crc.testing                  simple1           8080-tcp                 None
</code></pre></div></div>

<p>This completes my initial testing of RedHat OpenShift on CloudReady Container platform.  There is one more major topic to basic tests that is storage, which I havenâ€™t explored yet.</p>

<p>I will keep updating this post as I get a chance to test and learn.</p>

<p>Keep learning :rocket:</p>

<p>Best regards,</p>

<p>Pradeep</p>

<p><img src="/assets/images/RedHatOpenShift.png" alt="" /></p>]]></content><author><name>OpenShift</name></author><category term="OpenShift" /><category term="Automation" /><category term="redhat" /><category term="codereadycontainers" /><summary type="html"><![CDATA[RedHat OpenShift Getting Started with OpenShift.]]></summary></entry></feed>