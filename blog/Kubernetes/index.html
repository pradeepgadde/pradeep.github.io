<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Kubernetes - Pradeep Gadde</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Kubernetes";
    var mkdocs_page_input_path = "Kubernetes.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Pradeep Gadde</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../about/">About</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Kubernetes</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#learning-resources">Learning Resources</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lab-setup">Lab Setup</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#minikube">Minikube</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#download">Download</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#install">Install</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#hyperkit">Hyperkit</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#download_1">Download</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#install_1">Install</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubectl">Kubectl</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#download_2">Download</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#install_2">Install</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#all-in-one-minikube-cluster-macos">All in one Minikube Cluster (macOS)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#verify">Verify</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#multi-node-minikube-cluster-macos">Multi-node Minikube Cluster (macOS)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#verify_1">Verify</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#minikube-cluster-with-a-different-runtime-containerd">Minikube Cluster with a different runtime (containerd)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#kubernetes-architecture">Kubernetes Architecture</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#control-plane">Control Plane</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#worker-nodes">Worker Nodes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubernetes-api-resources">Kubernetes API Resources</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubernetes-resources-of-a-standard-application">Kubernetes resources of a standard application</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#namespace">Namespace</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#defaults">Defaults</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#create-a-new-namespace">Create a new namespace</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#life-cycle-of-a-pod">Life cycle of a Pod</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#create-a-deployment">Create a Deployment</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#view-cluster-events">View Cluster Events</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scaling-a-deployment">Scaling a Deployment</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#minikube-ssh-and-access-pods">Minikube SSH and Access Pods</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#services">Services</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#default-services">Default Services</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#nodeport-type-service">NodePort type service</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#access-nodeport-type-service">Access NodePort type Service</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#clusterip-service">ClusterIP Service</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#access-pods-with-clusterip">Access Pods with ClusterIP</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#loadbalancer-service">LoadBalancer Service</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#access-pods-with-loadbalancer-service">Access Pods with LoadBalancer Service</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deleting-resources">Deleting Resources</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#creating-resources-in-a-declarative-wayyaml">Creating Resources in a Declarative Way‚ÄîYAML</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#show-labels">Show Labels</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#apply-labels">Apply Labels</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#label-selector">Label Selector</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#describe-resources">Describe Resources</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubectl-explain">Kubectl Explain!</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scheduling">Scheduling</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#manual-scheduling">Manual Scheduling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#node-selector">Node Selector</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#node-affinity">Node Affinity</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taints-and-tolerations">Taints and Tolerations</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#resource-limits">Resource Limits</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#daemon-sets">Daemon Sets</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#static-pods">Static Pods</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#create-a-static-pod-in-the-worker-node">Create a Static Pod in the worker node</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#multiple-schedulers">Multiple Schedulers</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#logging-and-monitoring">Logging and Monitoring</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#verify_2">Verify</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examine-pod-logs">Examine Pod Logs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubectl-exec">Kubectl exec</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#application-life-cycle-management">Application Life Cycle Management</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#rolling-update">Rolling Update</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#recreate">Recreate</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#commands-and-arguments">Commands and Arguments</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#environment-variables">Environment Variables</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#configmaps">ConfigMaps</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#secrets">Secrets</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#multi-container-pods">Multi Container Pods</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#init-containers">Init Containers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rollout">Rollout</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#update-deployment-using-kubectl-set">Update Deployment (using Kubectl Set)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubectl-annotate">Kubectl Annotate</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rollback">Rollback</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#security">Security</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#certificates">Certificates</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#certificates-api">Certificates API</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kube-config">Kube Config</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#roles-and-rolebindings">Roles and RoleBindings</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cluster-roles-and-cluster-role-bindings">Cluster Roles and Cluster Role Bindings</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubectl-auth">Kubectl Auth</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#security-context">Security Context</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#storage">Storage</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#volumes">Volumes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#emptydir">emptyDir</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#hostpath">HostPath</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#config-maps">Config Maps</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#secrets_1">Secrets</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#persistentvolume">PersistentVolume</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#persistentvolumeclaim">PersistentVolumeClaim</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#storageclass">StorageClass</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#static-binding">Static Binding</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dynamic-binding">Dynamic Binding</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#networking">Networking</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#kindnet">KindNet</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#calico">Calico</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubernetes-ingress-controller">Kubernetes Ingress Controller</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#enable-ingress-addon">Enable Ingress Addon</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#deploy-a-hello-world-app">Deploy a Hello, World App!</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#create-an-ingress">Create an Ingress</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#create-a-second-deployment">Create a second Deployment</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#test-your-ingress">Test your Ingress</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubernetes-dns">Kubernetes DNS</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#create-a-simple-pod-to-use-as-a-test-environment">Create a simple Pod to use as a test environment</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Pradeep Gadde</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Kubernetes</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>

          <div role="main">
            <div class="section">
              
                <h1 id="kubernetes">Kubernetes</h1>
<p><img alt="kubernetes-logo" src="../kubernetes-logo.png" />    </p>
<h2 id="introduction">Introduction</h2>
<p>Google has created a resource in the form of a comic to understand why Kubernetes and  what problems can be addressed by Kubernetes. Here is the link to the same.</p>
<p>https://cloud.google.com/kubernetes-engine/kubernetes-comic</p>
<p>And here is an extract:
<img alt="Kubernetes-Comic" src="../kubernetes-comic.png" /></p>
<h2 id="learning-resources">Learning Resources</h2>
<p>While there are many resources out there, personally I have used these resources:</p>
<p><a href="https://www.edx.org/course/introduction-to-kubernetes">Introduction to Kubernetes by Linux Foundation</a></p>
<p>then, the most popular Udemy course on this subject: </p>
<p><a href="https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/">Certified Kubernetes Administrator (CKA) with Practice Tests by Mumshad Mannambeth</a>,</p>
<p>and the official Linux Foundation course 
  <a href="https://training.linuxfoundation.org/training/kubernetes-fundamentals-lfs258-cka-exam-bundle/">LFS258</a></p>
<p>Also, I got the opportunity to attend some of the O'Reilly Live Classes‚Äì</p>
<p><a href="https://github.com/bmuschko/cka-crash-course">Certified Kubernetes Administrator (CKA) Exam Prep, hosted by Benjamin Muschko</a>,</p>
<p>and</p>
<p><a href="https://github.com/sandervanvugt/cka">Certified Kubernetes Administrator (CKA) Crash Course, presented by Sander van Vugt</a>.</p>
<p>Of course, the official   <a href="https://kubernetes.io/docs">Kubernetes Documentation</a></p>
<h2 id="lab-setup">Lab Setup</h2>
<p>In this post, let's explore the world of Kubernetes right from our workstation. All the examples shown here are executed on a machine running macOS. </p>
<p>To get started, we need to install three packages: <code>minikube</code>, <code>hyperkit</code>, and <code>kubectl</code>.
If you already have some kind of virtualization software like Vmware Fusion or Virtualbox, hyperkit is not mandatory, but I prefer to use hyperkit (which seems to be the case with minikube also).</p>
<h3 id="minikube">Minikube</h3>
<h4 id="download">Download</h4>
<pre><code class="language-sh">pradeep@learnk8s$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 
</code></pre>
<h4 id="install">Install</h4>
<pre><code class="language-sh">pradeep@learnk8s$ sudo install minikube-darwin-amd64 /usr/local/bin/minikube
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ minikube version
minikube version: v1.25.1
commit: 3e64b11ed75e56e4898ea85f96b2e4af0301f43d
</code></pre>
<h3 id="hyperkit">Hyperkit</h3>
<h4 id="download_1">Download</h4>
<p>If not, install Brew using, https://brew.sh/</p>
<h4 id="install_1">Install</h4>
<p>if you have Brew package manager, run: </p>
<pre><code class="language-sh">pradeep@learnk8s$ brew install hyperkit
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ hyperkit -v
hyperkit: 0.20200908

Homepage: https://github.com/docker/hyperkit
License: BSD
</code></pre>
<h3 id="kubectl">Kubectl</h3>
<p>The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. </p>
<h4 id="download_2">Download</h4>
<p>https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/</p>
<h4 id="install_2">Install</h4>
<pre><code class="language-sh">pradeep@learnk8s$ brew install kubectl 
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl version
Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;23&quot;, GitVersion:&quot;v1.23.2&quot;, GitCommit:&quot;9d142434e3af351a628bffee3939e64c681afa4d&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2022-01-19T17:27:51Z&quot;, GoVersion:&quot;go1.17.6&quot;, Compiler:&quot;gc&quot;, Platform:&quot;darwin/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;23&quot;, GitVersion:&quot;v1.23.1&quot;, GitCommit:&quot;86ec240af8cbd1b60bcc4c03c20da9b98005b92e&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2021-12-16T11:34:54Z&quot;, GoVersion:&quot;go1.17.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
</code></pre>
<h3 id="all-in-one-minikube-cluster-macos">All in one Minikube Cluster (macOS)</h3>
<pre><code class="language-sh">pradeep@learnk8s$ minikube start
üòÑ  minikube v1.25.1 on Darwin 11.6.2
‚ú®  Automatically selected the hyperkit driver. Other choices: vmware, virtualbox, ssh
üëç  Starting control plane node minikube in cluster minikube
üî•  Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...
‚ùó  This VM is having trouble accessing https://k8s.gcr.io
üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
üê≥  Preparing Kubernetes v1.23.1 on Docker 20.10.12 ...
    ‚ñ™ kubelet.housekeeping-interval=5m
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: default-storageclass, storage-provisioner

‚ùó  /usr/local/bin/kubectl is version 1.21.2, which may have incompatibilites with Kubernetes 1.23.1.
    ‚ñ™ Want kubectl v1.23.1? Try 'minikube kubectl -- get pods -A'
üèÑ  Done! kubectl is now configured to use &quot;minikube&quot; cluster and &quot;default&quot; namespace by default

</code></pre>
<h4 id="verify">Verify</h4>
<pre><code class="language-sh">pradeep@learnk8s$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
No resources found in default namespace.
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS       AGE
kube-system   coredns-64897985d-nvzjq            1/1     Running   0              5m7s
kube-system   etcd-minikube                      1/1     Running   0              5m7s
kube-system   kube-apiserver-minikube            1/1     Running   0              5m7s
kube-system   kube-controller-manager-minikube   1/1     Running   0              5m7s
kube-system   kube-proxy-2ch8q                   1/1     Running   0              5m7s
kube-system   kube-scheduler-minikube            1/1     Running   0              5m7s
kube-system   storage-provisioner                1/1     Running   0                        5m7s
</code></pre>
<h3 id="multi-node-minikube-cluster-macos">Multi-node Minikube Cluster (macOS)</h3>
<pre><code class="language-sh">pradeep@learnk8s$ minikube start --nodes 2 -p k8s
üòÑ  [k8s] minikube v1.25.1 on Darwin 11.6.2
‚ú®  Automatically selected the hyperkit driver. Other choices: vmware, virtualbox, ssh
üëç  Starting control plane node k8s in cluster k8s
üî•  Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
‚ùó  This VM is having trouble accessing https://k8s.gcr.io
üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
üê≥  Preparing Kubernetes v1.23.1 on Docker 20.10.12 ...
    ‚ñ™ kubelet.housekeeping-interval=5m
    ‚ñ™ kubelet.cni-conf-dir=/etc/cni/net.mk
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîó  Configuring CNI (Container Networking Interface) ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass

üëç  Starting worker node k8s-m02 in cluster k8s
üî•  Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=192.168.177.17
‚ùó  This VM is having trouble accessing https://k8s.gcr.io
üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
üê≥  Preparing Kubernetes v1.23.1 on Docker 20.10.12 ...
    ‚ñ™ env NO_PROXY=192.168.177.17
üîé  Verifying Kubernetes components...

‚ùó  /usr/local/bin/kubectl is version 1.21.2, which may have incompatibilites with Kubernetes 1.23.1.
    ‚ñ™ Want kubectl v1.23.1? Try 'minikube kubectl -- get pods -A'
üèÑ  Done! kubectl is now configured to use &quot;k8s&quot; cluster and &quot;default&quot; namespace by default 
</code></pre>
<h4 id="verify_1">Verify</h4>
<pre><code class="language-sh">pradeep@learnk8s$ minikube status -p k8s
k8s
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

k8s-m02
type: Worker
host: Running
kubelet: Running
</code></pre>
<h3 id="minikube-cluster-with-a-different-runtime-containerd">Minikube Cluster with a different runtime (containerd)</h3>
<p>Read this post on Kubernetes Blog</p>
<p>https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube start --container-runtime=containerd
üòÑ  minikube v1.25.1 on Darwin 11.6.2
‚ú®  Automatically selected the hyperkit driver. Other choices: vmware, virtualbox, ssh
üëç  Starting control plane node minikube in cluster minikube
üíæ  Downloading Kubernetes v1.23.1 preload ...
    &gt; preloaded-images-k8s-v16-v1...: 571.57 MiB / 571.57 MiB  100.00% 3.17 MiB
üî•  Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...
‚ùó  This VM is having trouble accessing https://k8s.gcr.io
üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
üì¶  Preparing Kubernetes v1.23.1 on containerd 1.4.12 ...
    ‚ñ™ kubelet.housekeeping-interval=5m
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîó  Configuring bridge CNI (Container Networking Interface) ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass

‚ùó  /usr/local/bin/kubectl is version 1.21.2, which may have incompatibilites with Kubernetes 1.23.1.
    ‚ñ™ Want kubectl v1.23.1? Try 'minikube kubectl -- get pods -A'
üèÑ  Done! kubectl is now configured to use &quot;minikube&quot; cluster and &quot;default&quot; namespace by default

</code></pre>
<h2 id="kubernetes-architecture">Kubernetes Architecture</h2>
<p>https://kubernetes.io/docs/concepts/overview/components/</p>
<h3 id="control-plane">Control Plane</h3>
<ul>
<li>
<p>Etcd‚ÄîA distributed data store which persists cluster configuration</p>
</li>
<li>
<p>API Server‚ÄîCentral communication hub for all Kubernetes components
  This is also the endpoint used by Kubernetes clients, such as the kubectl CLI</p>
</li>
<li>
<p>Scheduler‚ÄîThe component responsible for assigning application components to worker nodes</p>
</li>
<li>
<p>Controller Manager‚ÄîHandles cluster-level operations, such as tracking worker node status, handling node failures, and replicating components</p>
</li>
</ul>
<p>For our examples we will use a single master configuration, but the master functions can be replicated and distributed for reliability</p>
<h3 id="worker-nodes">Worker Nodes</h3>
<ul>
<li>Kubelet‚ÄîDeploys and manages application components assigned to the local node by the scheduler.</li>
<li>Kube-proxy‚ÄîThe local networking component on the worker. Handles load-balancing between application components.</li>
<li>Container engine ‚Äì The container runtime used by the local worker node. </li>
</ul>
<p>For our examples we will use the most common runtime, Docker. </p>
<h3 id="kubernetes-api-resources">Kubernetes API Resources</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v2                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                               true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
endpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
csistoragecapacities                           storage.k8s.io/v1beta1                 true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment

</code></pre>
<h3 id="kubernetes-resources-of-a-standard-application">Kubernetes resources of a standard application</h3>
<p>As shown here, to deploy a standard application in Kubernetes, we might have to deploy a lot of these API resources.</p>
<p>https://github.com/kubernetes/community/blob/master/icons/examples/schemas/std-app.png </p>
<p><img alt="kubernetes-std-app" src="../kubernetes-std-app.png" />  </p>
<p>Reference   https://kubernetes.io/docs</p>
<ul>
<li>
<p>Pods
Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.
A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. </p>
</li>
<li>
<p>ReplicaSet
A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.</p>
</li>
<li>
<p>Deployments
A Deployment provides declarative updates for Pods and ReplicaSets.
You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. </p>
</li>
<li>
<p>Service
An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you don't need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.</p>
</li>
</ul>
<h3 id="namespace">Namespace</h3>
<h4 id="defaults">Defaults</h4>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   2m7s
kube-node-lease   Active   2m10s
kube-public       Active   2m10s
kube-system       Active   2m10s
</code></pre>
<p>As we saw at the beginning, when we setup a kubernetes cluster, all control plane and worker node compononets gets installed as pods in the system defined namespace <code>kube-system</code>.</p>
<h4 id="create-a-new-namespace">Create a new namespace</h4>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create namespace prod
namespace/prod created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get ns
NAME              STATUS   AGE
default           Active   5m4s
prod              Active   33s
kube-node-lease   Active   5m7s
kube-public       Active   5m7s
kube-system       Active   5m7s

</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run nginx --image=nginx -n prod
pod/nginx created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -n prod
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          79s
</code></pre>
<h3 id="life-cycle-of-a-pod">Life cycle of a Pod</h3>
<ol>
<li>kubectl client sends new pod request</li>
<li>Scheduler assigns new pod to worker node</li>
<li>API server instructs kubelet  to provision new Pod</li>
<li>kubelet requests that Docker create container</li>
<li>Docker retrieves nginx image</li>
<li>Docker creates container</li>
<li>Kubelet passes container details to K8s API</li>
<li>Pod configs stored in etcd</li>
<li>Network configs are created on kube-proxy</li>
</ol>
<h3 id="create-a-deployment">Create a Deployment</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create deployment demo --image=k8s.gcr.io/echoserver:1.4
deployment.apps/demo created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployments
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
demo   1/1     1            1           36s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get replicasets
NAME              DESIRED   CURRENT   READY   AGE
demo-75f9c7566f   1         1         1       31s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
demo-75f9c7566f-nmjw2   1/1     Running   0          26s
</code></pre>
<h3 id="view-cluster-events">View Cluster Events</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get events
LAST SEEN   TYPE     REASON                    OBJECT                       MESSAGE
5m15s       Normal   Scheduled                 pod/demo-75f9c7566f-nmjw2    Successfully assigned default/demo-75f9c7566f-nmjw2 to k8s-m02
5m14s       Normal   Pulling                   pod/demo-75f9c7566f-nmjw2    Pulling image &quot;k8s.gcr.io/echoserver:1.4&quot;
4m57s       Normal   Pulled                    pod/demo-75f9c7566f-nmjw2    Successfully pulled image &quot;k8s.gcr.io/echoserver:1.4&quot; in 16.898815167s
4m57s       Normal   Created                   pod/demo-75f9c7566f-nmjw2    Created container echoserver
4m57s       Normal   Started                   pod/demo-75f9c7566f-nmjw2    Started container echoserver
5m15s       Normal   SuccessfulCreate          replicaset/demo-75f9c7566f   Created pod: demo-75f9c7566f-nmjw2
5m15s       Normal   ScalingReplicaSet         deployment/demo              Scaled up replica set demo-75f9c7566f to 1
14m         Normal   Starting                  node/k8s-m02                 Starting kubelet.
14m         Normal   NodeHasSufficientMemory   node/k8s-m02                 Node k8s-m02 status is now: NodeHasSufficientMemory
14m         Normal   NodeHasNoDiskPressure     node/k8s-m02                 Node k8s-m02 status is now: NodeHasNoDiskPressure
14m         Normal   NodeHasSufficientPID      node/k8s-m02                 Node k8s-m02 status is now: NodeHasSufficientPID
14m         Normal   NodeAllocatableEnforced   node/k8s-m02                 Updated Node Allocatable limit across pods
14m         Normal   Starting                  node/k8s-m02
14m         Normal   RegisteredNode            node/k8s-m02                 Node k8s-m02 event: Registered Node k8s-m02 in Controller
13m         Normal   NodeReady                 node/k8s-m02                 Node k8s-m02 status is now: NodeReady
15m         Normal   NodeHasSufficientMemory   node/k8s                     Node k8s status is now: NodeHasSufficientMemory
15m         Normal   NodeHasNoDiskPressure     node/k8s                     Node k8s status is now: NodeHasNoDiskPressure
15m         Normal   NodeHasSufficientPID      node/k8s                     Node k8s status is now: NodeHasSufficientPID
14m         Normal   Starting                  node/k8s                     Starting kubelet.
14m         Normal   NodeHasSufficientMemory   node/k8s                     Node k8s status is now: NodeHasSufficientMemory
14m         Normal   NodeHasNoDiskPressure     node/k8s                     Node k8s status is now: NodeHasNoDiskPressure
14m         Normal   NodeHasSufficientPID      node/k8s                     Node k8s status is now: NodeHasSufficientPID
14m         Normal   NodeAllocatableEnforced   node/k8s                     Updated Node Allocatable limit across pods
14m         Normal   RegisteredNode            node/k8s                     Node k8s event: Registered Node k8s in Controller
14m         Normal   Starting                  node/k8s
14m         Normal   NodeReady                 node/k8s                     Node k8s status is now: NodeReady
</code></pre>
<h3 id="scaling-a-deployment">Scaling a Deployment</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl scale deployment demo --replicas=3
deployment.apps/demo scaled
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deploy
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
demo   3/3     3            3           11m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get rs
NAME              DESIRED   CURRENT   READY   AGE
demo-75f9c7566f   3         3         3       12m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
demo-75f9c7566f-fct76   1/1     Running   0          2m2s
demo-75f9c7566f-kz9kb   1/1     Running   0          2m3s
demo-75f9c7566f-nmjw2   1/1     Running   0          12m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE      NOMINATED NODE   READINESS GATES
demo-75f9c7566f-fct76   1/1     Running   0          2m10s   10.244.1.4   k8s-m02   &lt;none&gt;           &lt;none&gt;
demo-75f9c7566f-kz9kb   1/1     Running   0          2m11s   10.244.0.3   k8s       &lt;none&gt;           &lt;none&gt;
demo-75f9c7566f-nmjw2   1/1     Running   0          12m     10.244.1.3   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="minikube-ssh-and-access-pods">Minikube SSH and Access Pods</h3>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.1.4:8080
CLIENT VALUES:
client_address=192.168.177.27
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://10.244.1.4:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=10.244.1.4:8080
user-agent=curl/7.78.0
BODY:
-no body in request-$
</code></pre>
<pre><code class="language-sh">$ curl 10.244.0.3:8080
CLIENT VALUES:
client_address=10.244.0.1
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://10.244.0.3:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=10.244.0.3:8080
user-agent=curl/7.78.0
BODY:
-no body in request-$
</code></pre>
<pre><code class="language-sh">$ curl 10.244.1.3:8080
CLIENT VALUES:
client_address=192.168.177.27
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://10.244.1.3:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=10.244.1.3:8080
user-agent=curl/7.78.0
BODY:
-no body in request-$
</code></pre>
<h3 id="services">Services</h3>
<ul>
<li>ClusterIP: Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType.</li>
<li>NodePort: Exposes the Service on each Node's IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You'll be able to contact the NodePort Service, from outside the cluster, by requesting <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>. </li>
<li>LoadBalancer: Exposes the Service externally using a cloud provider's load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.</li>
</ul>
<h4 id="default-services">Default Services</h4>
<p>From the default namespace</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   33m
</code></pre>
<p>From all namespaces</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get services -A
NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP                  33m
kube-system   kube-dns     ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   33m
</code></pre>
<h4 id="nodeport-type-service">NodePort type service</h4>
<p>Expose the deployment as a NodePort type Service</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl expose deployment demo --type=NodePort --port=8080
service/demo exposed
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
demo         NodePort    10.101.87.60   &lt;none&gt;        8080:32298/TCP   4s
kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP          35m
</code></pre>
<p>Note the port number <code>32298</code> assigned to the service</p>
<h4 id="access-nodeport-type-service">Access NodePort type Service</h4>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get nodes -o wide
NAME      STATUS   ROLES                  AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
k8s       Ready    control-plane,master   38m   v1.23.1   192.168.177.27   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
k8s-m02   Ready    &lt;none&gt;                 37m   v1.23.1   192.168.177.28   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
</code></pre>
<p>From Minikube SSH session, with <code>k8s-m02</code> node IP, and NodePort <code>32298</code></p>
<pre><code class="language-sh">$ curl 192.168.177.28:32298
CLIENT VALUES:
client_address=10.244.1.1
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://192.168.177.28:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=192.168.177.28:32298
user-agent=curl/7.78.0
BODY:
-no body in request-$
</code></pre>
<p>with <code>k8s</code> node IP and NodePort <code>32298</code></p>
<pre><code class="language-sh">$ curl 192.168.177.27:32298
CLIENT VALUES:
client_address=10.244.0.1
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://192.168.177.27:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=192.168.177.27:32298
user-agent=curl/7.78.0
BODY:
-no body in request-$
</code></pre>
<h4 id="clusterip-service">ClusterIP Service</h4>
<p>Let's delete the service</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete svc demo
service &quot;demo&quot; deleted
</code></pre>
<p>and expose the same deployment as a ClusterIP (the default ServiceType). If we don't specify any type, its the ClusterIP.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl expose deployment demo  --port=8080
service/demo exposed
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
demo         ClusterIP   10.103.185.242   &lt;none&gt;        8080/TCP   5s
kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    45m
</code></pre>
<p>Note the ClusterIP assigned to this service <code>10.103.185.242</code>.</p>
<h4 id="access-pods-with-clusterip">Access Pods with ClusterIP</h4>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.103.185.242:8080
CLIENT VALUES:
client_address=192.168.177.27
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://10.103.185.242:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=10.103.185.242:8080
user-agent=curl/7.78.0
BODY:
-no body in request-$
</code></pre>
<h4 id="loadbalancer-service">LoadBalancer Service</h4>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete service demo
service &quot;demo&quot; deleted
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl expose deployment demo ‚Äìtype=LoadBalancer --port=8080
service/demo exposed
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get svc
NAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
demo         LoadBalancer   10.98.252.85   &lt;pending&gt;     8080:30646/TCP   3s
kubernetes   ClusterIP      10.96.0.1      &lt;none&gt;        443/TCP          50m
</code></pre>
<p>Open another terminal and initiate <code>minikube tunnel</code></p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube tunnel -p k8s
Password:
Status:
    machine: k8s
    pid: 95721
    route: 10.96.0.0/12 -&gt; 192.168.177.27
    minikube: Running
    services: [demo]
    errors:
        minikube: no errors
        router: no errors
        loadbalancer emulator: no errors
</code></pre>
<p>Return to the first terminal, Verify that <code>EXTERNAL-IP</code> is populated now</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get svc
NAME         TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)          AGE
demo         LoadBalancer   10.98.252.85   10.98.252.85   8080:30646/TCP   3m27s
kubernetes   ClusterIP      10.96.0.1      &lt;none&gt;         443/TCP          53m
</code></pre>
<p>NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.</p>
<h4 id="access-pods-with-loadbalancer-service">Access Pods with LoadBalancer Service</h4>
<pre><code class="language-sh">pradeep@learnk8s$ minikube service demo -p k8s
|-----------|------|-------------|-----------------------------|
| NAMESPACE | NAME | TARGET PORT |             URL             |
|-----------|------|-------------|-----------------------------|
| default   | demo |        8080 | http://192.168.177.27:30646 |
|-----------|------|-------------|-----------------------------|
üéâ  Opening service default/demo in default browser..
</code></pre>
<p><img alt="kubernetes-lb-svc" src="../kubernetes-lb-svc.png" /> </p>
<h3 id="deleting-resources">Deleting Resources</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete pod demo-75f9c7566f-fct76
pod &quot;demo-75f9c7566f-fct76&quot; deleted
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME                    READY   STATUS              RESTARTS   AGE
demo-75f9c7566f-b45x7   0/1     ContainerCreating   0          3s
demo-75f9c7566f-kz9kb   1/1     Running             0          55m
demo-75f9c7566f-nmjw2   1/1     Running             0          65m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/demo-75f9c7566f-b45x7   1/1     Running   0          2m29s
pod/demo-75f9c7566f-kz9kb   1/1     Running   0          57m
pod/demo-75f9c7566f-nmjw2   1/1     Running   0          67m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   77m

NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/demo   3/3     3            3           67m

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/demo-75f9c7566f   3         3         3       67m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete deployment demo
deployment.apps &quot;demo&quot; deleted
</code></pre>
<p>Deleting a deployment, deletes all the associated pods, replicaset as well.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   78m
</code></pre>
<h3 id="creating-resources-in-a-declarative-wayyaml">Creating Resources in a Declarative Way‚ÄîYAML</h3>
<pre><code class="language-yaml">pradeep@learnk8s$ kubectl run nginx --image=nginx --dry-run=client -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run nginx --image=nginx --dry-run=client -o yaml &gt; demopod.yaml
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f demopod.yaml
pod/nginx created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          50s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create deployment demo --image=nginx --replicas=3 --dry-run=client -o yaml &gt; demodeploy.yaml
</code></pre>
<pre><code class="language-yaml">pradeep@learnk8s$ cat demodeploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: demo
  name: demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demo
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: demo
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f demodeploy.yaml
deployment.apps/demo created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/demo-6c54f77c95-6g7zq   1/1     Running   0          71s
pod/demo-6c54f77c95-sb4c9   1/1     Running   0          71s
pod/demo-6c54f77c95-w2bsw   1/1     Running   0          71s
pod/nginx                   1/1     Running   0          5m45s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   90m

NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/demo   3/3     3            3           71s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/demo-6c54f77c95   3         3         3       71s
</code></pre>
<h3 id="show-labels">Show Labels</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods --show-labels
NAME                    READY   STATUS    RESTARTS   AGE     LABELS
demo-6c54f77c95-6g7zq   1/1     Running   0          3m32s   app=demo,pod-template-hash=6c54f77c95
demo-6c54f77c95-sb4c9   1/1     Running   0          3m32s   app=demo,pod-template-hash=6c54f77c95
demo-6c54f77c95-w2bsw   1/1     Running   0          3m32s   app=demo,pod-template-hash=6c54f77c95
nginx                   1/1     Running   0          8m6s    run=nginx
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get nodes --show-labels
NAME      STATUS   ROLES                  AGE   VERSION   LABELS
k8s       Ready    control-plane,master   92m   v1.23.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s,kubernetes.io/os=linux,minikube.k8s.io/commit=3e64b11ed75e56e4898ea85f96b2e4af0301f43d,minikube.k8s.io/name=k8s,minikube.k8s.io/updated_at=2022_02_04T05_49_32_0700,minikube.k8s.io/version=v1.25.1,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=
k8s-m02   Ready    &lt;none&gt;                 91m   v1.23.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m02,kubernetes.io/os=linux
</code></pre>
<h3 id="apply-labels">Apply Labels</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl label pods nginx new-label=awesome
pod/nginx labeled
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods --show-labels
NAME                    READY   STATUS    RESTARTS   AGE     LABELS
demo-6c54f77c95-6g7zq   1/1     Running   0          8m47s   app=demo,pod-template-hash=6c54f77c95
demo-6c54f77c95-sb4c9   1/1     Running   0          8m47s   app=demo,pod-template-hash=6c54f77c95
demo-6c54f77c95-w2bsw   1/1     Running   0          8m47s   app=demo,pod-template-hash=6c54f77c95
nginx                   1/1     Running   0          13m     new-label=awesome,run=nginx
</code></pre>
<h3 id="label-selector">Label Selector</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods --selector=app=demo
NAME                    READY   STATUS    RESTARTS   AGE
demo-6c54f77c95-6g7zq   1/1     Running   0          10m
demo-6c54f77c95-sb4c9   1/1     Running   0          10m
demo-6c54f77c95-w2bsw   1/1     Running   0          10m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods --selector=run=nginx
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          15m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods --selector=new-label=awesome
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          15m
</code></pre>
<h3 id="describe-resources">Describe Resources</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods nginx
Name:         nginx
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.28
Start Time:   Fri, 04 Feb 2022 07:13:50 +0530
Labels:       new-label=awesome
              run=nginx
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.6
IPs:
  IP:  10.244.1.6
Containers:
  nginx:
    Container ID:   docker://32bfdccc6c984f51a4c4092e4027ff7f1e53dd518938196c95d5427a44a90e40
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Fri, 04 Feb 2022 07:13:56 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9sstv (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-9sstv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  18m   default-scheduler  Successfully assigned default/nginx to k8s-m02
  Normal  Pulling    18m   kubelet            Pulling image &quot;nginx&quot;
  Normal  Pulled     18m   kubelet            Successfully pulled image &quot;nginx&quot; in 4.864287771s
  Normal  Created    18m   kubelet            Created container nginx
  Normal  Started    18m   kubelet            Started container nginx
</code></pre>
<h3 id="kubectl-explain">Kubectl Explain!</h3>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl explain pod.spec
KIND:     Pod
VERSION:  v1

RESOURCE: spec &lt;Object&gt;

DESCRIPTION:
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

     PodSpec is a description of a pod.

FIELDS:
   &lt;SNIP&gt;
   containers   &lt;[]Object&gt; -required-
     List of containers belonging to the pod. Containers cannot currently be
     added or removed. There must be at least one container in a Pod. Cannot be
     updated.
   &lt;SNIP&gt;
    nodeName    &lt;string&gt;
     NodeName is a request to schedule this pod onto a specific node. If it is
     non-empty, the scheduler simply schedules this pod onto that node, assuming
     that it fits resource requirements.
   &lt;SNIP&gt;  
</code></pre>
<h2 id="scheduling">Scheduling</h2>
<h3 id="manual-scheduling">Manual Scheduling</h3>
<p>Modify the Pod definition file, to includ the <code>nodeName</code> in the spec.</p>
<pre><code class="language-sh">pradeep@learnk8s$ cat manual-sched-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-manual
spec:
  nodeName: k8s
  containers:
  - image: nginx
    name: nginx
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f manual-sched-pod.yaml
pod/nginx-manual created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
demo-6c54f77c95-6g7zq   1/1     Running   0          20m
demo-6c54f77c95-sb4c9   1/1     Running   0          20m
demo-6c54f77c95-w2bsw   1/1     Running   0          20m
nginx                   1/1     Running   0          25m
nginx-manual            1/1     Running   0          48s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep manual
nginx-manual            1/1     Running   0          2m9s   10.244.0.5   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="node-selector">Node Selector</h3>
<p>You can constrain a Pod so that it can only run on particular set of Node(s). 
<code>nodeSelector</code> provides a very simple way to constrain pods to nodes with particular labels.
First, let‚Äôs label one of the nodes with <code>disktype=ssd</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl label nodes k8s-m02 disktype=ssd
node/k8s-m02 labeled
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get nodes --show-labels
NAME      STATUS   ROLES                  AGE     VERSION   LABELS
k8s       Ready    control-plane,master   3d11h   v1.23.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s,kubernetes.io/os=linux,minikube.k8s.io/commit=3e64b11ed75e56e4898ea85f96b2e4af0301f43d,minikube.k8s.io/name=k8s,minikube.k8s.io/updated_at=2022_02_07T17_03_56_0700,minikube.k8s.io/version=v1.25.1,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=
k8s-m02   Ready    &lt;none&gt;                 4m3s    v1.23.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m02,kubernetes.io/os=linux
pradeep@learnk8s$
</code></pre>
<p>Now, let's create a pod with <code>nodeSelector</code> spec set to the label that we assigned, so that the pod gets scheduled on this node.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-node-selector.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-node-selector
  labels:
    env: test
spec:
  containers:
  - name: nginx-node-selector
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
</code></pre>
<p>Finally, verify the node on which this pod is running.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep selector
nginx-node-selector     1/1     Running   0          17s    10.244.1.2    k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>As expected, this new pod is running on the node(<code>k8s-m02</code>) with the label specified by the <code>nodeSelector</code> pod spec.</p>
<p>If you describe this pod, you will see the configured Node-Selectors.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pod nginx-node-selector | grep Node-Selectors
Node-Selectors:              disktype=ssd
</code></pre>
<h3 id="node-affinity">Node Affinity</h3>
<p>There are currently two types of node affinity, called <code>requiredDuringSchedulingIgnoredDuringExecution</code> and <code>preferredDuringSchedulingIgnoredDuringExecution</code>.</p>
<p>You can think of them as "hard" and "soft" respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (similar to <code>nodeSelector</code> but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee. </p>
<p>The <code>IgnoredDuringExecution</code> part of the names means that, similar to how <code>nodeSelector</code> works, if labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod continues to run on the node. </p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-node-affinity.yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

  containers:
  - name: with-node-affinity
    image: nginx
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-node-affinity.yaml
pod/with-node-affinity created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep affinity
with-node-affinity      1/1     Running   0          9s     10.244.1.3    k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="taints-and-tolerations">Taints and Tolerations</h3>
<p>Taints  allow a node to repel a set of pods.</p>
<p>Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.</p>
<p>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe nodes | grep Taint
Taints:             &lt;none&gt;
Taints:             &lt;none&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl taint nodes k8s key1=value1:NoSchedule

node/k8s tainted
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe nodes | grep Taint
Taints:             key1=value1:NoSchedule
Taints:             &lt;none&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl taint nodes k8s-m02 key2=value2:NoSchedule

node/k8s-m02 tainted
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe nodes | grep Taint
Taints:             key1=value1:NoSchedule
Taints:             key2=value2:NoSchedule
</code></pre>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-toleration.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-taint-demo
  labels:
    env: test
spec:
  containers:
  - name: nginx-taint-demo
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: &quot;key1&quot;
    operator: &quot;Equal&quot;
    value: &quot;value1&quot;
    effect: &quot;NoSchedule&quot;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-toleration.yaml
pod/nginx-taint-demo created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep taint-demo
nginx-taint-demo        1/1     Running   0          9m3s    10.244.0.15   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-toleration-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-taint-demo-2
  labels:
    env: test
spec:
  containers:
  - name: nginx-taint-demo-2
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: &quot;key2&quot;
    operator: &quot;Equal&quot;
    value: &quot;value2&quot;
    effect: &quot;NoSchedule&quot;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-toleration-2.yaml
pod/nginx-taint-demo-2 created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep taint-demo
nginx-taint-demo        1/1     Running   0          11m     10.244.0.15   k8s       &lt;none&gt;           &lt;none&gt;
nginx-taint-demo-2      1/1     Running   0          9m35s   10.244.2.5    k8s-m02   &lt;none&gt;    &lt;none&gt;
</code></pre>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-no-toleration.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-no-tolerate
  labels:
    env: test
spec:
  containers:
  - name: nginx-no-tolerate
    image: nginx
    imagePullPolicy: IfNotPresent
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-no-toleration.yaml
pod/nginx-no-tolerate created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep no-tolerate
nginx-no-tolerate       0/1     Pending   0          10m     &lt;none&gt;        &lt;none&gt;    &lt;none&gt;          &lt;none&gt;
</code></pre>
<p>The Pod got created but is in <code>Pending</code> state, it is not <code>Running</code> yet. Let's find out why?!</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods nginx-no-tolerate
Name:         nginx-no-tolerate
Namespace:    default
Priority:     0
Node:         &lt;none&gt;
Labels:       env=test
Annotations:  &lt;none&gt;
Status:       Pending
IP:
IPs:          &lt;none&gt;
Containers:
  nginx-no-tolerate:
    Image:        nginx
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6mz6d (ro)
Conditions:
  Type           Status
  PodScheduled   False
Volumes:
  kube-api-access-6mz6d:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  43s (x11 over 11m)  default-scheduler  0/2 nodes are available: 1 node(s) had taint {key1: value1}, that the pod didn't tolerate, 1 node(s) had taint {key2: value2}, that the pod didn't tolerate.

</code></pre>
<p>Look at the Reason: <code>FailedScheduling</code>, none of the nodes (0/2) are available because both have some taint that our pod couldn't tolerate!</p>
<p>Now, let's delete the Taint on one of the nodes and try creating the Pod again.</p>
<p>To untaint a node, just add a <code>-</code> at the end of the taint that you plan to remove.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl taint node k8s  key1=value1:NoSchedule-
node/k8s untainted
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe nodes | grep Taint
Taints:             &lt;none&gt;
Taints:             key2=value2:NoSchedule
</code></pre>
<p>Now the <code>nginx-no-tolerate</code> pod changes to <code>Running</code> state and is scheduled on the <code>k8s</code> node which does not have any Taints at the moment.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep no-tolerate
nginx-no-tolerate       1/1     Running   0          16h   10.244.0.16   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="resource-limits">Resource Limits</h3>
<p>Earlier, we have created a namespace called <code>prod</code>. Let's describe it to see if any resource quota applied to it.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe namespaces prod
Name:         prod
Labels:       kubernetes.io/metadata.name=prod
Annotations:  &lt;none&gt;
Status:       Active

No resource quota.

No LimitRange resource.
</code></pre>
<p>Let's define a quota using the example shown here:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create resourcequota -h
Create a resource quota with the specified name, hard limits, and optional scopes.

Aliases:
quota, resourcequota

Examples:
  # Create a new resource quota named my-quota
  kubectl create quota my-quota
--hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10

  # Create a new resource quota named best-effort
  kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3 -n prod
resourcequota/my-quota created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe ns prod
Name:         prod
Labels:       kubernetes.io/metadata.name=prod
Annotations:  &lt;none&gt;
Status:       Active

Resource Quotas
  Name:     my-quota
  Resource  Used  Hard
  --------  ---   ---
  cpu       0     1
  memory    0     1G
  pods      0     2
  services  0     3

No LimitRange resource.
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get quota -n prod
NAME       AGE     REQUEST                                            LIMIT
my-quota   4m40s   cpu: 0/1, memory: 0/1G, pods: 0/2, services: 0/3
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe quota -n prod
Name:       my-quota
Namespace:  prod
Resource    Used  Hard
--------    ----  ----
cpu         0     1
memory      0     1G
pods        0     2
services    0     3
</code></pre>
<p>Let's try create a pod in this namespace and see what happens? </p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run nginx-without-cpu --image=nginx -n prod
Error from server (Forbidden): pods &quot;nginx-without-cpu&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
pradeep@learnk8s$
</code></pre>
<p>Pod creation failed with an error (Forbidden).
Let's try create a deployment in this namespace and see what happens?</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create deployment test-quota --image=nginx --replicas=3 -n prod
deployment.apps/test-quota created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deploy -n prod
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
test-quota   0/3     0            0           47s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get events -n prod
LAST SEEN   TYPE      REASON              OBJECT                             MESSAGE
104s        Warning   FailedCreate        replicaset/test-quota-666dfd598f   Error creating: pods &quot;test-quota-666dfd598f-bgsxd&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
103s        Warning   FailedCreate        replicaset/test-quota-666dfd598f   Error creating: pods &quot;test-quota-666dfd598f-w9p4z&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
103s        Warning   FailedCreate        replicaset/test-quota-666dfd598f   Error creating: pods &quot;test-quota-666dfd598f-swwgq&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
103s        Warning   FailedCreate        replicaset/test-quota-666dfd598f   Error creating: pods &quot;test-quota-666dfd598f-tjlsb&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
103s        Warning   FailedCreate        replicaset/test-quota-666dfd598f   Error creating: pods &quot;test-quota-666dfd598f-ll2m9&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
103s        Warning   FailedCreate        replicaset/test-quota-666dfd598f   Error creating: pods &quot;test-quota-666dfd598f-c72mn&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
103s        Warning   FailedCreate        replicaset/test-quota-666dfd598f   Error creating: pods &quot;test-quota-666dfd598f-hqzf4&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
103s        Warning   FailedCreate        replicaset/test-quota-666dfd598f   Error creating: pods &quot;test-quota-666dfd598f-bdh4w&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
102s        Warning   FailedCreate        replicaset/test-quota-666dfd598f   Error creating: pods &quot;test-quota-666dfd598f-gggw2&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
21s         Warning   FailedCreate        replicaset/test-quota-666dfd598f   (combined from similar events): Error creating: pods &quot;test-quota-666dfd598f-b5rh6&quot; is forbidden: failed quota: my-quota: must specify cpu,memory
104s        Normal    ScalingReplicaSet   deployment/test-quota              Scaled up replica set test-quota-666dfd598f to 3
</code></pre>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-quota.yaml
apiVersion: v1
kind: Pod
metadata:
  name: quota-mem-cpu-demo
  namespace: prod
spec:
  containers:
  - name: quota-mem-cpu-demo-ctr
    image: nginx
    resources:
      limits:
        memory: &quot;800Mi&quot;
        cpu: &quot;800m&quot;
      requests:
        memory: &quot;600Mi&quot;
        cpu: &quot;400m&quot;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-quota.yaml
pod/quota-mem-cpu-demo created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -n prod
NAME                 READY   STATUS    RESTARTS   AGE
quota-mem-cpu-demo   1/1     Running   0          9s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe ns prod
Name:         prod
Labels:       kubernetes.io/metadata.name=prod
Annotations:  &lt;none&gt;
Status:       Active

Resource Quotas
  Name:     my-quota
  Resource  Used   Hard
  --------  ---    ---
  cpu       400m   1
  memory    600Mi  1G
  pods      1      2
  services  0      3

No LimitRange resource.
</code></pre>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-quota-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: quota-mem-cpu-demo-2
  namespace: prod
spec:
  containers:
  - name: quota-mem-cpu-demo-ctr-2
    image: nginx
    resources:
      limits:
        memory: &quot;800Mi&quot;
        cpu: &quot;800m&quot;
      requests:
        memory: &quot;600Mi&quot;
        cpu: &quot;400m&quot;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-quota-2.yaml
Error from server (Forbidden): error when creating &quot;pod-quota-2.yaml&quot;: pods &quot;quota-mem-cpu-demo-2&quot; is forbidden: exceeded quota: my-quota, requested: memory=600Mi, used: memory=600Mi, limited: memory=1G
</code></pre>
<h3 id="daemon-sets">Daemon Sets</h3>
<p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get daemonsets.apps -A
NAMESPACE     NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   kindnet      2         2         2       2            2           &lt;none&gt;                   5d12h
kube-system   kube-proxy   2         2         2       2            2           kubernetes.io/os=linux   5d12h
</code></pre>
<p>As seen above, there are two daemonsets in the <code>kube-system</code> namespace. One is named <code>kindnet</code> and the other is <code>kube-proxy</code>. If you look at the <code>DESIRED</code> column, both of them have a value of <code>2</code> which is equal to the number of nodes in this (minikube) cluster.</p>
<p>Let's look at those pods now.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe daemonsets.apps kindnet -n kube-system
Name:           kindnet
Selector:       app=kindnet
Node-Selector:  &lt;none&gt;
Labels:         app=kindnet
                k8s-app=kindnet
                tier=node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=kindnet
                    k8s-app=kindnet
                    tier=node
  Service Account:  kindnet
  Containers:
   kindnet-cni:
    Image:      kindest/kindnetd:v20210326-1e038dc5
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:      (v1:status.hostIP)
      POD_IP:       (v1:status.podIP)
      POD_SUBNET:  10.244.0.0/16
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
  Volumes:
   cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.mk
    HostPathType:  DirectoryOrCreate
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:
Events:            &lt;none&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe daemonsets.apps kube-proxy -n kube-system
Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=kube-proxy
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy:v1.23.1
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:               HostPath (bare host directory volume)
    Path:               /lib/modules
    HostPathType:
  Priority Class Name:  system-node-critical
Events:                 &lt;none&gt;
</code></pre>
<p>As seen in this output, there are two pods, one pod on each node. The pod names follow the convention of <code>&lt;daemonsetname&gt;-&lt;randomidentifier&gt;</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -A -o wide -o wide| grep kube-proxy
kube-system   kube-proxy-fszkr              1/1     Running   1 (2d ago)     5d12h   192.168.177.28   k8s-m02   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-m747v              1/1     Running   1              5d12h   192.168.177.27   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Similarly, the pods from the other daemonset‚Äîkindnet.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -A -o wide -o wide| grep kindnet
kube-system   kindnet-jpxdd                 1/1     Running   1              5d12h   192.168.177.27   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-p77mb                 1/1     Running   1 (2d ago)     5d12h   192.168.177.28   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>A simple way to create a DaemonSet is to first generate a YAML file for a Deployment and remove the replicas, strategy fields. Change the kind from Deployment to DaemonSet.</p>
<p>Here is an example (modfied from the demodeployment.yaml):</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat demodaemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: demo-ds
  name: demo-ds
spec:
  selector:
    matchLabels:
      app: demo-ds
  template:
    metadata:
      labels:
        app: demo-ds
    spec:
      containers:
      - image: nginx
        name: nginx-ds
        resources: {}

</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f demodaemonset.yaml
daemonset.apps/demo-ds created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get ds
NAME      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
demo-ds   2         2         0       2            0           &lt;none&gt;          6s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep demo-ds
demo-ds-gtcf7           1/1     Running   0          28s    10.244.1.5    k8s-m02   &lt;none&gt;           &lt;none&gt;
demo-ds-kkw4g           1/1     Running   0          28s    10.244.0.11   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="static-pods">Static Pods</h3>
<p><em>Static Pods</em> are managed directly by the kubelet daemon on a specific node, without the API server observing them. Unlike Pods that are managed by the control plane; instead, the kubelet watches each static Pod (and restarts it if it fails).</p>
<p>The Pod names will be suffixed with the node hostname with a leading hyphen.</p>
<p>Manifests are standard Pod definitions in JSON or YAML format in a specific directory. Use the staticPodPath: <code>&lt;the directory&gt;</code> field in the kubelet configuration file, which periodically scans the directory and creates/deletes static Pods as YAML/JSON files appear/disappear there. </p>
<p>Let's explore our minikube environment to see if any Static Pods are there.</p>
<p>First SSH to the minikube node and search for the <code>kubelet</code> process. The result shows all configuration parameteres used by the <code>kubelet</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ps -aux | grep kubelet
root        4244 11.6  4.5 1946788 100436 ?      Ssl  Feb08 238:40 /var/lib/minikube/binaries/v1.23.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --cni-conf-dir=/etc/cni/net.mk --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=k8s --housekeeping-interval=5m --kubeconfig=/etc/kubernetes/kubelet.conf --network-plugin=cni --node-ip=192.168.177.27
root      412294 15.3 13.5 1042556 296888 ?      Ssl  01:40  10:27 kube-apiserver --advertise-address=192.168.177.27 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-account-signing-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key
docker    429542  0.0  0.0   3348   448 pts/0    S+   02:48   0:00 grep kubelet
</code></pre>
<p>As seen above, the kubelet config settings are present in <code>--config=/var/lib/kubelet/config.yaml</code>. You can explore this file to get the <code>staticPodPath</code>.</p>
<pre><code class="language-sh">$ more /var/lib/kubelet/config.yaml | grep static
staticPodPath: /etc/kubernetes/manifests
</code></pre>
<p>Now that we know the <code>staticPodPath</code>, lets see what manifests are currently defined.</p>
<pre><code class="language-sh">$ ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
$
</code></pre>
<p>There are four manifest files defined in this location. Looking at the name of the files, you can relate them to the Kubernetes core components.</p>
<p>Let us re-visit the <code>kube-system</code> namespace and get all the pods running there.</p>
<pre><code class="language-sh">$ exit
logout
pradeep@learnk8s$ kubectl get pods -n kube-system
NAME                          READY   STATUS    RESTARTS        AGE
coredns-64897985d-r9tzv       1/1     Running   8 (114m ago)    6d2h
etcd-k8s                      1/1     Running   2 (114m ago)    6d2h
kindnet-jpxdd                 1/1     Running   5 (8h ago)      6d2h
kindnet-p77mb                 1/1     Running   50 (118m ago)   6d2h
kube-apiserver-k8s            1/1     Running   3 (8h ago)      6d2h
kube-controller-manager-k8s   1/1     Running   4 (113m ago)    6d2h
kube-proxy-fszkr              1/1     Running   1 (2d15h ago)   6d2h
kube-proxy-m747v              1/1     Running   1               6d2h
kube-scheduler-k8s            1/1     Running   2 (8h ago)      6d2h
storage-provisioner           1/1     Running   49 (81m ago)    6d2h
</code></pre>
<p>Pay special attention to the <code>-k8s</code> string in the name of the Pods: <code>etcd-k8s</code>, <code>kube-apiserver-k8s</code>, <code>kube-controller-manager-k8</code>, and <code>kube-scheduler-k8s</code>. As mentioned earlier, The StaticPod names will be suffixed with the node hostname (<code>k8s</code> in our case) with a leading hyphen.</p>
<h4 id="create-a-static-pod-in-the-worker-node">Create a Static Pod in the worker node</h4>
<p>Now that we have seen the static pods deployed by the system during cluster setup, let us manually deploy an <code>nginx</code> pod in the second node (<code>k8s-m02</code>) of our minikube cluster <code>k8s</code>.</p>
<p>For this first, we need to SSH to that node: <code>k8s-m02</code>. To do this, add <code>-n k8s-m02</code> option to the minikube ssh command that we have been using so far.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a show eth0
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether be:02:3c:97:9d:85 brd ff:ff:ff:ff:ff:ff
    inet 192.168.177.28/24 brd 192.168.177.255 scope global dynamic eth0
       valid_lft 47485sec preferred_lft 47485sec

</code></pre>
<p>This confirms that we logged in to the <code>k8s-m02</code> node which has the IP address: 192.168.177.28. </p>
<p>Create a new file called <code>nginx.yaml</code> in the staicPodPath location, that is <code>/etc/kubernetes/manifests/</code>.</p>
<pre><code class="language-sh">$ sudo vi /etc/kubernetes/manifests/nginx.yaml
$ cat /etc/kubernetes/manifests/nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
$ exit
logout
</code></pre>
<p>By the presence of this file in the <code>k8s-m02</code> node, the kubelet creates this Pod automatically now.</p>
<p>To verify,</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
demo-6c54f77c95-mgz7f   1/1     Running   1          5d22h
demo-6c54f77c95-q679r   1/1     Running   1          4d22h
demo-6c54f77c95-qqzbf   1/1     Running   1          4d22h
demo-6c54f77c95-vjgc2   1/1     Running   1          4d22h
demo-6c54f77c95-wv78b   1/1     Running   1          5d22h
demo-ds-gtcf7           1/1     Running   0          14h
demo-ds-kkw4g           1/1     Running   0          14h
nginx-k8s-m02           1/1     Running   0          18s
nginx-manual            1/1     Running   1          4d21h
nginx-no-tolerate       1/1     Running   1          4d14h
nginx-node-selector     1/1     Running   0          2d15h
nginx-taint-demo        1/1     Running   1          4d14h
with-node-affinity      1/1     Running   0          2d15h
</code></pre>
<p>Our nginx static pod is successfully Running on <code>k8s-m02</code> node, as seen above, as pod named <code>nginx-k8s-m02</code>. </p>
<p>Another way to confirm that this Pod is indeed running on the <code>k8s-m02</code> node is </p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep m02
demo-ds-gtcf7           1/1     Running   0          14h     10.244.1.5    k8s-m02   &lt;none&gt;           &lt;none&gt;
nginx-k8s-m02           1/1     Running   0          18m     10.244.1.6    k8s-m02   &lt;none&gt;           &lt;none&gt;
nginx-node-selector     1/1     Running   0          2d15h   10.244.1.2    k8s-m02   &lt;none&gt;           &lt;none&gt;
with-node-affinity      1/1     Running   0          2d15h   10.244.1.3    k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="multiple-schedulers">Multiple Schedulers</h3>
<p>Kubernetes ships with a default scheduler (<code>kube-scheduler</code>) that we discussed earlier. If the default scheduler does not suit your needs you can implement your own scheduler. Moreover, you can even run multiple schedulers simultaneously alongside the default scheduler and instruct Kubernetes what scheduler to use for each of your pods.</p>
<p>We can use the staticPod manifest file located at /etc/kubernetes/manifests/kube-scheduler.yaml to create our own scheduler.</p>
<p>First, let us take a look at the definition of the default scheduler.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ cat /etc/kubernetes/manifests/
etcd.yaml                     kube-apiserver.yaml           kube-controller-manager.yaml  kube-scheduler.yaml

$ sudo cat /etc/kubernetes/manifests/kube-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=false
    image: k8s.gcr.io/kube-scheduler:v1.23.1
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
status: {}
$
</code></pre>
<p>The default scheduler listens on port <code>10259</code>, so for our custom scheduler, we have to choose another port, for example <code>10282</code>.</p>
<p>Here is the definition of the new scheduler called  <code>my-scheduler</code>, which is listening on the secure-port <code>10282</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ sudo cat /etc/kubernetes/manifests/my-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: my-scheduler
    tier: control-plane
  name: my-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=false
    - --secure-port=10282
    image: k8s.gcr.io/kube-scheduler:v1.23.1
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10282
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: my-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10282
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
status: {}
$
</code></pre>
<p>Verify that both schedulers <code>kube-scheduler-k8s</code> and <code>my-scheduler-k8s</code> are in running state.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods  -n kube-system
NAME                          READY   STATUS    RESTARTS        AGE
coredns-64897985d-r9tzv       1/1     Running   8 (3h2m ago)    6d3h
etcd-k8s                      1/1     Running   2 (3h2m ago)    6d3h
kindnet-jpxdd                 1/1     Running   5 (9h ago)      6d3h
kindnet-p77mb                 1/1     Running   50 (3h6m ago)   6d3h
kube-apiserver-k8s            1/1     Running   3 (9h ago)      6d3h
kube-controller-manager-k8s   1/1     Running   4 (3h1m ago)    6d3h
kube-proxy-fszkr              1/1     Running   1 (2d16h ago)   6d3h
kube-proxy-m747v              1/1     Running   1               6d3h
kube-scheduler-k8s            1/1     Running   0               14m
my-scheduler-k8s              1/1     Running   0               2m14s
storage-provisioner           1/1     Running   49 (149m ago)   6d3h
</code></pre>
<p>Now, it is time to deploy a Pod that makes use of this new scheduler.
We just need to add one line for the <code>schedulerName</code> in the Pod spec.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-my-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx-my-scheduler
  name: nginx-my-scheduler
spec:
  schedulerName: my-scheduler
  containers:
  - image: nginx
    name: nginx-my-scheduler
    resources: {}
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-my-scheduler.yaml
pod/nginx-my-scheduler created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
demo-6c54f77c95-mgz7f   1/1     Running   1          6d
demo-6c54f77c95-q679r   1/1     Running   1          5d
demo-6c54f77c95-qqzbf   1/1     Running   1          5d
demo-6c54f77c95-vjgc2   1/1     Running   1          5d
demo-6c54f77c95-wv78b   1/1     Running   1          6d
demo-ds-gtcf7           1/1     Running   0          16h
demo-ds-kkw4g           1/1     Running   0          16h
nginx-k8s-m02           1/1     Running   0          137m
nginx-manual            1/1     Running   1          4d23h
nginx-my-scheduler      0/1     Pending   0          2s
nginx-no-tolerate       1/1     Running   1          4d17h
nginx-node-selector     1/1     Running   0          2d17h
nginx-taint-demo        1/1     Running   1          4d17h
with-node-affinity      1/1     Running   0          2d17h
</code></pre>
<p>:memo:  <strong>NOTE:</strong> There seems to be some issue with this, the Pod is in Pending state, I could not find any events. In the recent Kubernetes version 1.23, there seems to be some changes related to Scheduler definition. I will have to park this aside for some time and continue investigating.</p>
<h2 id="logging-and-monitoring">Logging and Monitoring</h2>
<p>To monitor cluster components, we need to deploy <code>metrics-server</code>.
Metrics Server is a cluster-wide aggregator of resource usage data. Resource usage metrics, such as container CPU and memory usage, are available in Kubernetes through the Metrics API. These metrics can be accessed either directly by the user with the kubectl top command, or by a controller in the cluster, for example Horizontal Pod Autoscaler, to make decisions.</p>
<p>Through the Metrics API, you can get the amount of resource currently used by a given node or a given pod.</p>
<p>The minikube tool includes a set of built-in addons that can be enabled, disabled and opened in the local Kubernetes environment.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube addons list -p k8s
|-----------------------------|---------|--------------|--------------------------------|
|         ADDON NAME          | PROFILE |    STATUS    |           MAINTAINER           |
|-----------------------------|---------|--------------|--------------------------------|
| ambassador                  | k8s     | disabled     | third-party (ambassador)       |
| auto-pause                  | k8s     | disabled     | google                         |
| csi-hostpath-driver         | k8s     | disabled     | kubernetes                     |
| dashboard                   | k8s     | disabled     | kubernetes                     |
| default-storageclass        | k8s     | enabled ‚úÖ   | kubernetes                     |
| efk                         | k8s     | disabled     | third-party (elastic)          |
| freshpod                    | k8s     | disabled     | google                         |
| gcp-auth                    | k8s     | disabled     | google                         |
| gvisor                      | k8s     | disabled     | google                         |
| helm-tiller                 | k8s     | disabled     | third-party (helm)             |
| ingress                     | k8s     | disabled     | unknown (third-party)          |
| ingress-dns                 | k8s     | disabled     | google                         |
| istio                       | k8s     | disabled     | third-party (istio)            |
| istio-provisioner           | k8s     | disabled     | third-party (istio)            |
| kubevirt                    | k8s     | disabled     | third-party (kubevirt)         |
| logviewer                   | k8s     | disabled     | unknown (third-party)          |
| metallb                     | k8s     | disabled     | third-party (metallb)          |
| metrics-server              | k8s     | disabled     | kubernetes                     |
| nvidia-driver-installer     | k8s     | disabled     | google                         |
| nvidia-gpu-device-plugin    | k8s     | disabled     | third-party (nvidia)           |
| olm                         | k8s     | disabled     | third-party (operator          |
|                             |         |              | framework)                     |
| pod-security-policy         | k8s     | disabled     | unknown (third-party)          |
| portainer                   | k8s     | disabled     | portainer.io                   |
| registry                    | k8s     | disabled     | google                         |
| registry-aliases            | k8s     | disabled     | unknown (third-party)          |
| registry-creds              | k8s     | disabled     | third-party (upmc enterprises) |
| storage-provisioner         | k8s     | enabled ‚úÖ   | google                         |
| storage-provisioner-gluster | k8s     | disabled     | unknown (third-party)          |
| volumesnapshots             | k8s     | disabled     | kubernetes                     |
|-----------------------------|---------|--------------|--------------------------------|
üí°  To see addons list for other profiles use: `minikube addons -p name list`
</code></pre>
<p>Let us enable the <code>metric-server</code> addon now in our cluster.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube addons enable metrics-server -p k8s
    ‚ñ™ Using image k8s.gcr.io/metrics-server/metrics-server:v0.4.2
üåü  The 'metrics-server' addon is enabled
</code></pre>
<h4 id="verify_2">Verify</h4>
<pre><code class="language-sh">pradeep@learnk8s$ minikube addons list -p k8s
|-----------------------------|---------|--------------|--------------------------------|
|         ADDON NAME          | PROFILE |    STATUS    |           MAINTAINER           |
|-----------------------------|---------|--------------|--------------------------------|
| ambassador                  | k8s     | disabled     | third-party (ambassador)       |
| auto-pause                  | k8s     | disabled     | google                         |
| csi-hostpath-driver         | k8s     | disabled     | kubernetes                     |
| dashboard                   | k8s     | disabled     | kubernetes                     |
| default-storageclass        | k8s     | enabled ‚úÖ   | kubernetes                     |
| efk                         | k8s     | disabled     | third-party (elastic)          |
| freshpod                    | k8s     | disabled     | google                         |
| gcp-auth                    | k8s     | disabled     | google                         |
| gvisor                      | k8s     | disabled     | google                         |
| helm-tiller                 | k8s     | disabled     | third-party (helm)             |
| ingress                     | k8s     | disabled     | unknown (third-party)          |
| ingress-dns                 | k8s     | disabled     | google                         |
| istio                       | k8s     | disabled     | third-party (istio)            |
| istio-provisioner           | k8s     | disabled     | third-party (istio)            |
| kubevirt                    | k8s     | disabled     | third-party (kubevirt)         |
| logviewer                   | k8s     | disabled     | unknown (third-party)          |
| metallb                     | k8s     | disabled     | third-party (metallb)          |
| metrics-server              | k8s     | enabled ‚úÖ   | kubernetes                     |
| nvidia-driver-installer     | k8s     | disabled     | google                         |
| nvidia-gpu-device-plugin    | k8s     | disabled     | third-party (nvidia)           |
| olm                         | k8s     | disabled     | third-party (operator          |
|                             |         |              | framework)                     |
| pod-security-policy         | k8s     | disabled     | unknown (third-party)          |
| portainer                   | k8s     | disabled     | portainer.io                   |
| registry                    | k8s     | disabled     | google                         |
| registry-aliases            | k8s     | disabled     | unknown (third-party)          |
| registry-creds              | k8s     | disabled     | third-party (upmc enterprises) |
| storage-provisioner         | k8s     | enabled ‚úÖ   | google                         |
| storage-provisioner-gluster | k8s     | disabled     | unknown (third-party)          |
| volumesnapshots             | k8s     | disabled     | kubernetes                     |
|-----------------------------|---------|--------------|--------------------------------|
üí°  To see addons list for other profiles use: `minikube addons -p name list`
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pod,svc,rs,deploy -n kube-system
NAME                                  READY   STATUS    RESTARTS         AGE
pod/coredns-64897985d-r9tzv           1/1     Running   8 (5h34m ago)    6d6h
pod/etcd-k8s                          1/1     Running   2 (5h34m ago)    6d6h
pod/kindnet-jpxdd                     1/1     Running   5 (11h ago)      6d6h
pod/kindnet-p77mb                     1/1     Running   50 (5h38m ago)   6d6h
pod/kube-apiserver-k8s                1/1     Running   3 (11h ago)      6d6h
pod/kube-controller-manager-k8s       1/1     Running   4 (5h34m ago)    6d6h
pod/kube-proxy-fszkr                  1/1     Running   1 (2d19h ago)    6d6h
pod/kube-proxy-m747v                  1/1     Running   1                6d6h
pod/kube-scheduler-k8s                1/1     Running   0                166m
pod/metrics-server-6b76bd68b6-7nbnx   1/1     Running   0                2m49s
pod/my-scheduler-k8s                  1/1     Running   0                89m
pod/storage-provisioner               1/1     Running   51 (53m ago)     6d6h

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns         ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP,53/TCP,9153/TCP   6d6h
service/metrics-server   ClusterIP   10.106.249.50   &lt;none&gt;        443/TCP                  2m49s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-64897985d           1         1         1       6d6h
replicaset.apps/metrics-server-6b76bd68b6   1         1         1       2m50s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns          1/1     1            1           6d6h
deployment.apps/metrics-server   1/1     1            1           2m50s
</code></pre>
<p>As seen above, <code>metric-server</code> is deployed as a Deployment, and exposed as ClusterIP service.</p>
<p>Use <code>kubectl top node</code> command to see CPU and memory utilization of each node in the cluster.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl top node
NAME      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
k8s       334m         16%    1635Mi          76%
k8s-m02   86m          4%     981Mi           45%
</code></pre>
<p>Use <code>kubectl top pod -A</code> command to see CPU and memory utilization of each node in the cluster.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl top pod -A
NAMESPACE     NAME                              CPU(cores)   MEMORY(bytes)
default       demo-6c54f77c95-mgz7f             0m           3Mi
default       demo-6c54f77c95-q679r             0m           3Mi
default       demo-6c54f77c95-qqzbf             0m           3Mi
default       demo-6c54f77c95-vjgc2             0m           3Mi
default       demo-6c54f77c95-wv78b             0m           3Mi
default       demo-ds-gtcf7                     0m           3Mi
default       demo-ds-kkw4g                     0m           3Mi
default       nginx-k8s-m02                     0m           3Mi
default       nginx-manual                      0m           3Mi
default       nginx-no-tolerate                 0m           7Mi
default       nginx-node-selector               0m           9Mi
default       nginx-taint-demo                  0m           3Mi
default       with-node-affinity                0m           3Mi
kube-system   coredns-64897985d-r9tzv           0m           12Mi
kube-system   etcd-k8s                          0m           44Mi
kube-system   kindnet-jpxdd                     0m           8Mi
kube-system   kindnet-p77mb                     0m           7Mi
kube-system   kube-apiserver-k8s                0m           235Mi
kube-system   kube-controller-manager-k8s       0m           42Mi
kube-system   kube-proxy-fszkr                  0m           16Mi
kube-system   kube-proxy-m747v                  0m           22Mi
kube-system   kube-scheduler-k8s                0m           14Mi
kube-system   metrics-server-6b76bd68b6-7nbnx   0m           4Mi
kube-system   my-scheduler-k8s                  0m           13Mi
kube-system   storage-provisioner               0m           8Mi
prod          quota-mem-cpu-demo                0m           3Mi
</code></pre>
<p>You can also use the <code>--sory-by</code> field to display in descending order.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl top pod -A --sort-by=memory
NAMESPACE     NAME                              CPU(cores)   MEMORY(bytes)
kube-system   kube-apiserver-k8s                0m           235Mi
kube-system   etcd-k8s                          0m           44Mi
kube-system   kube-controller-manager-k8s       0m           42Mi
kube-system   kube-proxy-m747v                  0m           22Mi
kube-system   kube-proxy-fszkr                  0m           16Mi
kube-system   kube-scheduler-k8s                0m           14Mi
kube-system   my-scheduler-k8s                  0m           13Mi
kube-system   coredns-64897985d-r9tzv           0m           12Mi
default       nginx-node-selector               0m           9Mi
kube-system   kindnet-jpxdd                     0m           8Mi
kube-system   storage-provisioner               0m           8Mi
default       nginx-no-tolerate                 0m           7Mi
kube-system   kindnet-p77mb                     0m           7Mi
kube-system   metrics-server-6b76bd68b6-7nbnx   0m           4Mi
default       demo-ds-kkw4g                     0m           3Mi
default       nginx-taint-demo                  0m           3Mi
default       demo-6c54f77c95-mgz7f             0m           3Mi
default       nginx-manual                      0m           3Mi
default       with-node-affinity                0m           3Mi
default       demo-ds-gtcf7                     0m           3Mi
default       demo-6c54f77c95-wv78b             0m           3Mi
default       demo-6c54f77c95-q679r             0m           3Mi
default       demo-6c54f77c95-qqzbf             0m           3Mi
default       demo-6c54f77c95-vjgc2             0m           3Mi
default       nginx-k8s-m02                     0m           3Mi
prod          quota-mem-cpu-demo                0m           3Mi
</code></pre>
<h4 id="examine-pod-logs">Examine Pod Logs</h4>
<p>Use the <code>kubectl logs &lt;pod&gt;</code> name to look at the logs of the affected container</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl logs nginx-manual
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2022/02/07 11:34:31 [notice] 1#1: using the &quot;epoll&quot; event method
2022/02/07 11:34:31 [notice] 1#1: nginx/1.21.6
2022/02/07 11:34:31 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6)
2022/02/07 11:34:31 [notice] 1#1: OS: Linux 4.19.202
2022/02/07 11:34:31 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2022/02/07 11:34:31 [notice] 1#1: start worker processes
2022/02/07 11:34:31 [notice] 1#1: start worker process 32
2022/02/07 11:34:31 [notice] 1#1: start worker process 33
</code></pre>
<h4 id="kubectl-exec">Kubectl exec</h4>
<p>If the container image includes debugging utilities, as is the case with images built from Linux and Windows OS base images, you can run commands inside a specific container with <code>kubectl exec</code>.</p>
<p>Here is an example to login to the <code>nginx-manual</code> container and execute commands from the <code>shell</code> mode.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it nginx-manual -- sh
# uname -a
Linux nginx-manual 4.19.202 #1 SMP Thu Dec 23 10:44:17 UTC 2021 x86_64 GNU/Linux
# exit 0
</code></pre>
<h2 id="application-life-cycle-management">Application Life Cycle Management</h2>
<p>I ran into some issue with my cluster and had to rebuild from scratch. Here is how my new cluster looks like:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get nodes -o wide
NAME      STATUS   ROLES                  AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
k8s       Ready    control-plane,master   4m42s   v1.23.1   192.168.177.29   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
k8s-m02   Ready    &lt;none&gt;                 4m21s   v1.23.1   192.168.177.30   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -A
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE
kube-system   coredns-64897985d-l9zmg       1/1     Running   0          4m34s
kube-system   etcd-k8s                      1/1     Running   1          4m45s
kube-system   kindnet-4d4xb                 1/1     Running   0          50s
kube-system   kindnet-z27gn                 1/1     Running   0          4m30s
kube-system   kube-apiserver-k8s            1/1     Running   1          4m45s
kube-system   kube-controller-manager-k8s   1/1     Running   1          4m45s
kube-system   kube-proxy-n75w8              1/1     Running   0          4m34s
kube-system   kube-proxy-v2mq5              1/1     Running   0          4m30s
kube-system   kube-scheduler-k8s            1/1     Running   1          4m45s
kube-system   storage-provisioner           1/1     Running   0          4m43s

</code></pre>
<p>Let us now create a deployment called <code>kodekloud</code> with image <code>webapp-color</code> and four replicas.
Here is the Docker image that we are going to use: https://hub.docker.com/r/kodekloud/webapp-color/tags?page=1&amp;ordering=-last_updated. It has many versions of the same image (like v1, v2, and v3).</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create deployment kodekloud --image=kodekloud/webapp-color --replicas=4
deployment.apps/kodekloud created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP           NODE      NOMINATED NODE   READINESS GATES
kodekloud-589c9f4b47-6fbqg   1/1     Running   0          47s   10.244.1.8   k8s-m02   &lt;none&gt;           &lt;none&gt;
kodekloud-589c9f4b47-c75mz   1/1     Running   0          47s   10.244.1.7   k8s-m02   &lt;none&gt;           &lt;none&gt;
kodekloud-589c9f4b47-s54tz   1/1     Running   0          47s   10.244.0.7   k8s       &lt;none&gt;           &lt;none&gt;
kodekloud-589c9f4b47-z2sqs   1/1     Running   0          47s   10.244.0.6   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployment
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kodekloud   4/4     4            4           53s
</code></pre>
<p>Verify the current Image being used by the deployment</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployment kodekloud | grep Image
    Image:        kodekloud/webapp-color
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.1.8:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #16a085;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-589c9f4b47-6fbqg!&lt;/h1&gt;





&lt;/div&gt;$ curl 10.244.1.7:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #16a085;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-589c9f4b47-c75mz!&lt;/h1&gt;




&lt;/div&gt;$ curl 10.244.0.7:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #2980b9;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-589c9f4b47-s54tz!&lt;/h1&gt;




&lt;/div&gt;$ curl 10.244.0.6:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #be2edd;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-589c9f4b47-z2sqs!&lt;/h1&gt;




&lt;/div&gt;$
</code></pre>
<h3 id="rolling-update">Rolling Update</h3>
<p>Also, find out the strategy used by this deployment</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployment kodekloud | grep Strategy
StrategyType:           RollingUpdate
RollingUpdateStrategy:  25% max unavailable, 25% max surge
</code></pre>
<p>With <code>RollingUpdate</code> strategy, Pods are upgraded few at a time.</p>
<p>Now update the deployment to use a different version of the application <code>kodekloud/webapp-color:v2</code>.</p>
<p>For this, we need not delete or re-create the existing deployment. We can <strong>edit</strong> the deployment and modify the image name. Save with Esc + wq.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl edit deployments kodekloud
</code></pre>
<pre><code class="language-yaml"># Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &quot;1&quot;
  creationTimestamp: &quot;2022-02-15T07:13:38Z&quot;
  generation: 1
  labels:
    app: kodekloud
  name: kodekloud
  namespace: default
  resourceVersion: &quot;1592&quot;
  uid: e5728c54-8768-4d5c-b1b0-c3e33cf62062
spec:
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: kodekloud
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: kodekloud
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        imagePullPolicy: Always
        name: webapp-color
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
-- INSERT --
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl edit deployments kodekloud
deployment.apps/kodekloud edited
</code></pre>
<p>The pods should be recreated with new image. Looking at the AGE column, we can conclude that all four pods got re-created few seconds ago. If you notice carefully, the replicates is having a different Identifier now (changed from 589c9f4b47 to 8477b7849).</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
kodekloud-8477b7849-4lgg2   1/1     Running   0          66s
kodekloud-8477b7849-b7grh   1/1     Running   0          44s
kodekloud-8477b7849-ndf4w   1/1     Running   0          66s
kodekloud-8477b7849-ww5qn   1/1     Running   0          47s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployments.apps
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kodekloud   4/4     4            4           16m
</code></pre>
<p>If we look at the Events section of this deployment, we can clearly see that the deployment-controller has Scaled Up the new replica set kodekloud-8477b7849 gradually from 1 to 4 and at the same time, old replica set kodekloud-589c9f4b47 got Scaled down from 4 to 0. This is becuase of the <code>RollingUpdateStrategy:  25% max unavailable, 25% max surge</code>.
For this current strategy, only one Pod can be down during the upgrade process (25%, meaning 1 out of total 4 pods in the deployment).</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployments.apps kodekloud
Name:                   kodekloud
Namespace:              default
CreationTimestamp:      Tue, 15 Feb 2022 12:43:38 +0530
Labels:                 app=kodekloud
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=kodekloud
Replicas:               4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=kodekloud
  Containers:
   webapp-color:
    Image:        kodekloud/webapp-color:v2
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   kodekloud-8477b7849 (4/4 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  23m    deployment-controller  Scaled up replica set kodekloud-589c9f4b47 to 4
  Normal  ScalingReplicaSet  8m22s  deployment-controller  Scaled up replica set kodekloud-8477b7849 to 1
  Normal  ScalingReplicaSet  8m22s  deployment-controller  Scaled down replica set kodekloud-589c9f4b47 to 3
  Normal  ScalingReplicaSet  8m22s  deployment-controller  Scaled up replica set kodekloud-8477b7849 to 2
  Normal  ScalingReplicaSet  8m4s   deployment-controller  Scaled down replica set kodekloud-589c9f4b47 to 2
  Normal  ScalingReplicaSet  8m3s   deployment-controller  Scaled up replica set kodekloud-8477b7849 to 3
  Normal  ScalingReplicaSet  8m     deployment-controller  Scaled down replica set kodekloud-589c9f4b47 to 1
  Normal  ScalingReplicaSet  8m     deployment-controller  Scaled up replica set kodekloud-8477b7849 to 4
  Normal  ScalingReplicaSet  7m55s  deployment-controller  Scaled down replica set kodekloud-589c9f4b47 to 0
</code></pre>
<p>Verify the Image being used by the deployment now. It says, <code>v2</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$kubectl describe deployment kodekloud| grep Image
    Image:        kodekloud/webapp-color:v2
</code></pre>
<p>Let's collect the new IP address of the Pods and verify the application from the minikube node.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME                        READY   STATUS    RESTARTS   AGE     IP            NODE      NOMINATED NODE   READINESS GATES
kodekloud-8477b7849-4lgg2   1/1     Running   0          4m2s    10.244.0.8    k8s       &lt;none&gt;           &lt;none&gt;
kodekloud-8477b7849-b7grh   1/1     Running   0          3m40s   10.244.0.9    k8s       &lt;none&gt;           &lt;none&gt;
kodekloud-8477b7849-ndf4w   1/1     Running   0          4m2s    10.244.1.9    k8s-m02   &lt;none&gt;           &lt;none&gt;
kodekloud-8477b7849-ww5qn   1/1     Running   0          3m43s   10.244.1.10   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>As seen here, the application is now running the v2 version as indicated by the new headers:</p>
<pre><code class="language-html">&lt;h2&gt;
    Application Version: v2
&lt;/h2&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.0.8:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #16a085;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-8477b7849-4lgg2!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v2
  &lt;/h2&gt;


&lt;/div&gt;$ curl 10.244.0.9:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #16a085;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-8477b7849-b7grh!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v2
  &lt;/h2&gt;


&lt;/div&gt;$ curl 10.244.1.9:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #16a085;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-8477b7849-ndf4w!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v2
  &lt;/h2&gt;


&lt;/div&gt;$ curl 10.244.1.10:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #16a085;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-8477b7849-ww5qn!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v2
  &lt;/h2&gt;


&lt;/div&gt;$ 
</code></pre>
<p>Another test is to verify the color of the web-app, it says <code>green</code>.</p>
<pre><code class="language-sh">$ curl 10.244.0.8:8080/color
green$ curl 10.244.0.9:8080/color
green$ curl 10.244.1.9:8080/color
green$ curl 10.244.1.10:8080/color
green$
</code></pre>
<h3 id="recreate">Recreate</h3>
<p>There is another deployment strategy called <code>Recreate</code>.
Let us edit the deployment again to use another version, v3, this time and change the strategy from <code>RollingUpdate</code> to <code>Recreate</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl edit deployment kodekloud
deployment.apps/kodekloud edited
</code></pre>
<pre><code class="language-yaml"># Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &quot;2&quot;
  creationTimestamp: &quot;2022-02-15T07:13:38Z&quot;
  generation: 2
  labels:
    app: kodekloud
  name: kodekloud
  namespace: default
  resourceVersion: &quot;2408&quot;
  uid: e5728c54-8768-4d5c-b1b0-c3e33cf62062
spec:
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: kodekloud
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: kodekloud
    spec:
      containers:
      - image: kodekloud/webapp-color:v3
        imagePullPolicy: Always
        name: webapp-color
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
-- INSERT --
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployments.apps
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kodekloud   4/4     4            4           33m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
kodekloud-676c6b9fcd-6bm6z   1/1     Running   0          53s   10.244.1.12   k8s-m02   &lt;none&gt;           &lt;none&gt;
kodekloud-676c6b9fcd-dggk7   1/1     Running   0          53s   10.244.0.10   k8s       &lt;none&gt;           &lt;none&gt;
kodekloud-676c6b9fcd-m2q7x   1/1     Running   0          53s   10.244.0.11   k8s       &lt;none&gt;           &lt;none&gt;
kodekloud-676c6b9fcd-p8tnl   1/1     Running   0          53s   10.244.1.11   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>If we look at the latest events of the deployment, the <em>deployment-controller  Scaled down replica set kodekloud-8477b7849 to 0 at a time and Scaled up new replica set kodekloud-676c6b9fcd to 4</em>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployments.apps
Name:               kodekloud
Namespace:          default
CreationTimestamp:  Tue, 15 Feb 2022 12:43:38 +0530
Labels:             app=kodekloud
Annotations:        deployment.kubernetes.io/revision: 3
Selector:           app=kodekloud
Replicas:           4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:       Recreate
MinReadySeconds:    0
Pod Template:
  Labels:  app=kodekloud
  Containers:
   webapp-color:
    Image:        kodekloud/webapp-color:v3
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   kodekloud-676c6b9fcd (4/4 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  35m    deployment-controller  Scaled up replica set kodekloud-589c9f4b47 to 4
  Normal  ScalingReplicaSet  20m    deployment-controller  Scaled up replica set kodekloud-8477b7849 to 1
  Normal  ScalingReplicaSet  20m    deployment-controller  Scaled down replica set kodekloud-589c9f4b47 to 3
  Normal  ScalingReplicaSet  20m    deployment-controller  Scaled up replica set kodekloud-8477b7849 to 2
  Normal  ScalingReplicaSet  20m    deployment-controller  Scaled down replica set kodekloud-589c9f4b47 to 2
  Normal  ScalingReplicaSet  20m    deployment-controller  Scaled up replica set kodekloud-8477b7849 to 3
  Normal  ScalingReplicaSet  20m    deployment-controller  Scaled down replica set kodekloud-589c9f4b47 to 1
  Normal  ScalingReplicaSet  20m    deployment-controller  Scaled up replica set kodekloud-8477b7849 to 4
  Normal  ScalingReplicaSet  19m    deployment-controller  Scaled down replica set kodekloud-589c9f4b47 to 0
  Normal  ScalingReplicaSet  2m58s  deployment-controller  Scaled down replica set kodekloud-8477b7849 to 0
  Normal  ScalingReplicaSet  2m27s  deployment-controller  Scaled up replica set kodekloud-676c6b9fcd to 4
</code></pre>
<p>Just to verify, let us connect to one of the pods from the new version and check the output.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.1.12:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #e74c3c;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-676c6b9fcd-6bm6z!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v3
  &lt;/h2&gt;


&lt;/div&gt;$
</code></pre>
<p>From this,  it is clear that the application got upgraded to latest version.</p>
<p>Also, there is another test to verify the color used by the web-app. All four pods returned <code>red</code>.</p>
<pre><code class="language-sh">$ curl 10.244.1.12:8080/color
red$ curl 10.244.1.11:8080/color
red$ curl 10.244.0.11:8080/color
red$ curl 10.244.0.10:8080/color
red$
</code></pre>
<h3 id="commands-and-arguments">Commands and Arguments</h3>
<p>Let us make use of help, to understand the usage of commands and arguments.
As shown here, with the help of <code>--command</code> option we can pass a different command and custom arguments to our container.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run -h
Create and run a particular image in a pod.

Examples:
&lt;SNIP&gt;

  # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command
  kubectl run nginx --image=nginx -- &lt;arg1&gt; &lt;arg2&gt; ... &lt;argN&gt;

  # Start the nginx pod using a different command and custom arguments
  kubectl run nginx --image=nginx --command -- &lt;cmd&gt; &lt;arg1&gt; ... &lt;argN&gt;

Options:
&lt;SNIP&gt;
</code></pre>
<p>As an example, let us try to create a Ubuntu container that just prints date.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run ubuntu-date --image=ubuntu --command date
pod/ubuntu-date created
</code></pre>
<p>Look at the <code>Command</code> header under <code>Containers</code> section.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ kubectl describe pods ubuntu-date
Name:         ubuntu-date
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Tue, 15 Feb 2022 13:55:40 +0530
Labels:       run=ubuntu-date
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.21
IPs:
  IP:  10.244.1.21
Containers:
  ubuntu-date:
    Container ID:  docker://3c6c897fe4ad67e0a90540ec086901eb63a5f8ab1d4afae85b7d0a734e363a9c
    Image:         ubuntu
    Image ID:      docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      date
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 15 Feb 2022 13:55:45 +0530
      Finished:     Tue, 15 Feb 2022 13:55:45 +0530
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-87bh2 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  kube-api-access-87bh2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age              From               Message
  ----    ------     ----             ----               -------
  Normal  Scheduled  9s               default-scheduler  Successfully assigned default/ubuntu-date to k8s-m02
  Normal  Pulled     4s               kubelet            Successfully pulled image &quot;ubuntu&quot; in 4.705982332s
  Normal  Created    4s               kubelet            Created container ubuntu-date
  Normal  Started    4s               kubelet            Started container ubuntu-date
  Normal  Pulling    3s (x2 over 8s)  kubelet            Pulling image &quot;ubuntu&quot;
</code></pre>
<p>The pod is Terminated with status <code>Completed</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods | grep date
ubuntu-date                 0/1     CrashLoopBackOff   3 (55s ago)   111s
</code></pre>
<p>Let us create another container that uses an argument as well, in addition to the command.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run ubuntu-sleep --image=ubuntu --command sleep 5000
pod/ubuntu-sleep created
</code></pre>
<pre><code class="language-yaml">pradeep@learnk8s$ kubectl describe pods ubuntu-sleep
Name:         ubuntu-sleep
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Tue, 15 Feb 2022 13:59:46 +0530
Labels:       run=ubuntu-sleep
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.22
IPs:
  IP:  10.244.1.22
Containers:
  ubuntu-sleep:
    Container ID:  docker://4a12725e6951fd785eadb5bb692b25755ecfac7c664d2eb97305f46369f9d898
    Image:         ubuntu
    Image ID:      docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sleep
      5000
    State:          Running
      Started:      Tue, 15 Feb 2022 13:59:56 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gpjg5 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-gpjg5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  26s   default-scheduler  Successfully assigned default/ubuntu-sleep to k8s-m02
  Normal  Pulling    25s   kubelet            Pulling image &quot;ubuntu&quot;
  Normal  Pulled     17s   kubelet            Successfully pulled image &quot;ubuntu&quot; in 8.387291792s
  Normal  Created    17s   kubelet            Created container ubuntu-sleep
  Normal  Started    16s   kubelet            Started container ubuntu-sleep
</code></pre>
<p>Let us take a look at the definition of this Pod with command and arguments in YAML.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods ubuntu-sleep -o yaml &gt; pod-command-arg.yaml
</code></pre>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-command-arg.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &quot;2022-02-15T08:29:46Z&quot;
  labels:
    run: ubuntu-sleep
  name: ubuntu-sleep
  namespace: default
  resourceVersion: &quot;5745&quot;
  uid: 971c6c08-da5e-4360-901b-7b8ca65a693a
spec:
  containers:
  - command:
    - sleep
    - &quot;5000&quot;
    image: ubuntu
    imagePullPolicy: Always
    name: ubuntu-sleep
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-gpjg5
      readOnly: true
  dnsPolicy: ClusterFirst
  &lt;SNIP&gt;
</code></pre>
<p>Now let us use the same kodecloud/web-app image and try to pass the color argument while initializing the pod.</p>
<p>Looking at the source code of this container    <a href="https://github.com/kodekloudhub/webapp-color/blob/master/app.py">kodekloudhub/webapp-color</a>, we can find out that, A color can be specified in two ways: either as a command line argument or as an environment variable.</p>
<pre><code class="language-python">from flask import Flask
from flask import render_template
import socket
import random
import os
import argparse

app = Flask(__name__)

color_codes = {
    &quot;red&quot;: &quot;#e74c3c&quot;,
    &quot;green&quot;: &quot;#16a085&quot;,
    &quot;blue&quot;: &quot;#2980b9&quot;,
    &quot;blue2&quot;: &quot;#30336b&quot;,
    &quot;pink&quot;: &quot;#be2edd&quot;,
    &quot;darkblue&quot;: &quot;#130f40&quot;
}

SUPPORTED_COLORS = &quot;,&quot;.join(color_codes.keys())

# Get color from Environment variable
COLOR_FROM_ENV = os.environ.get('APP_COLOR')
# Generate a random color
COLOR = random.choice([&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;blue2&quot;, &quot;darkblue&quot;, &quot;pink&quot;])


@app.route(&quot;/&quot;)
def main():
    # return 'Hello'
    return render_template('hello.html', name=socket.gethostname(), color=color_codes[COLOR])


if __name__ == &quot;__main__&quot;:

    print(&quot; This is a sample web application that displays a colored background. \n&quot;
          &quot; A color can be specified in two ways. \n&quot;
          &quot;\n&quot;
          &quot; 1. As a command line argument with --color as the argument. Accepts one of &quot; + SUPPORTED_COLORS + &quot; \n&quot;
          &quot; 2. As an Environment variable APP_COLOR. Accepts one of &quot; + SUPPORTED_COLORS + &quot; \n&quot;
          &quot; 3. If none of the above then a random color is picked from the above list. \n&quot;
          &quot; Note: Command line argument precedes over environment variable.\n&quot;
          &quot;\n&quot;
          &quot;&quot;)

    # Check for Command Line Parameters for color
    parser = argparse.ArgumentParser()
    parser.add_argument('--color', required=False)
    args = parser.parse_args()

    if args.color:
        print(&quot;Color from command line argument =&quot; + args.color)
        COLOR = args.color
        if COLOR_FROM_ENV:
            print(&quot;A color was set through environment variable -&quot; + COLOR_FROM_ENV + &quot;. However, color from command line argument takes precendence.&quot;)
    elif COLOR_FROM_ENV:
        print(&quot;No Command line argument. Color from environment variable =&quot; + COLOR_FROM_ENV)
        COLOR = COLOR_FROM_ENV
    else:
        print(&quot;No command line argument or environment variable. Picking a Random Color =&quot; + COLOR)

    # Check if input color is a supported one
    if COLOR not in color_codes:
        print(&quot;Color not supported. Received '&quot; + COLOR + &quot;' expected one of &quot; + SUPPORTED_COLORS)
        exit(1)

    # Run Flask Application
    app.run(host=&quot;0.0.0.0&quot;, port=8080)
</code></pre>
<p>Let us try the first method that we just discussed: as a command argument.
Create a pod using <code>kodekloud/webapp-color:v3</code> but pass the --color argument to change the default color from <code>red</code> to <code>blue</code>. Earlier, when we ran this image (with defaults), the color was <code>red</code>.</p>
<p>This can also be confirmed by looking at the Dockerfile definition.</p>
<p><img alt="" src="../kodekloud-webapp-color.png" /> </p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run kodekloud-change-color --image=kodekloud/webapp-color:v3 --dry-run=client -o yaml -- &quot;--color&quot; &quot;blue&quot; &gt; pod-change-color.yaml
</code></pre>
<p>Now we can see we are passing the <code>--color</code> and <code>blue</code> as arguements to this container.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-change-color.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: kodekloud-change-color
  name: kodekloud-change-color
spec:
  containers:
  - args:
    - --color
    - blue
    image: kodekloud/webapp-color:v3
    name: kodekloud-change-color
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
</code></pre>
<p>Let us create a pod from this definition file.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-change-color.yaml
pod/kodekloud-change-color created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods kodekloud-change-color
Name:         kodekloud-change-color
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Tue, 15 Feb 2022 16:31:54 +0530
Labels:       run=kodekloud-change-color
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.29
IPs:
  IP:  10.244.1.29
Containers:
  kodekloud-change-color:
    Container ID:  docker://3c6bf9642cb90fcc2f969a3043f3879d9a42917d7e15ed26905b2a340461609e
    Image:         kodekloud/webapp-color:v3
    Image ID:      docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Args:
      --color
      blue
    State:          Running
      Started:      Tue, 15 Feb 2022 16:31:57 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6l7n4 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-6l7n4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12s   default-scheduler  Successfully assigned default/kodekloud-change-color to k8s-m02
  Normal  Pulled     10s   kubelet            Container image &quot;kodekloud/webapp-color:v3&quot; already present on machine
  Normal  Created    10s   kubelet            Created container kodekloud-change-color
  Normal  Started    9s    kubelet            Started container kodekloud-change-color
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep color
kodekloud-change-color      1/1     Running            0                40s    10.244.1.29   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Verify the color of the webapp now, it should say <code>blue</code> instead of the default <code>red</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s

                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.1.29:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #2980b9;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-change-color!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v3
  &lt;/h2&gt;


&lt;/div&gt;$
$ curl 10.244.1.29:8080/color
blue$ exit
logout
</code></pre>
<p>It is confirmed the arguments that we passed worked! ("background: #2980b9 indicates blue, as defined in the color_code, "blue": "#2980b9").</p>
<h3 id="environment-variables">Environment Variables</h3>
<p>Now, it is time to try the other metod of passing arguments, via environment variables.
From the <code>kubectl run</code> examples, we can see that environment variables can be set with the <code>--env</code> option.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run -h
Create and run a particular image in a pod.

Examples:
  &lt;SNIP&gt;
  # Start a hazelcast pod and set environment variables &quot;DNS_DOMAIN=cluster&quot; and &quot;POD_NAMESPACE=default&quot; in the
container
  kubectl run hazelcast --image=hazelcast/hazelcast --env=&quot;DNS_DOMAIN=cluster&quot; --env=&quot;POD_NAMESPACE=default&quot;
  &lt;SNIP&gt;
</code></pre>
<p>Let us redploy the same image <code>kodekloud/webapp-color:v3</code> but this time changing the color of the app with environment variable.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run kodekloud-env-color --image=kodekloud/webapp-color:v3 --env=APP_COLOR=pink
pod/kodekloud-env-color created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep env
kodekloud-env-color         1/1     Running            0              12s     10.244.1.30   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.1.30:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #be2edd;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-env-color!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v3
  &lt;/h2&gt;


&lt;/div&gt;$ curl 10.244.1.30:8080/color
pink$
$ exit
logout
</code></pre>
<p>The curl tests confirm that the application is using the <code>pink</code> color now, which is passed with the environment variable, <code>APP_COLOR</code>.</p>
<p>If we look at the Pod description, <code>Environment</code> section, <code>APP_COLOR:  pink</code> is seen as expected.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pod kodekloud-env-color
Name:         kodekloud-env-color
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Tue, 15 Feb 2022 19:22:07 +0530
Labels:       run=kodekloud-env-color
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.30
IPs:
  IP:  10.244.1.30
Containers:
  kodekloud-env-color:
    Container ID:   docker://376016abd3ab8f65faa794cb746831f37c3ac16c0436598b08f9365bdd6176d4
    Image:          kodekloud/webapp-color:v3
    Image ID:       docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Tue, 15 Feb 2022 19:22:08 +0530
    Ready:          True
    Restart Count:  0
    Environment:
      APP_COLOR:  pink
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xj9jz (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-xj9jz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  2m26s  default-scheduler  Successfully assigned default/kodekloud-env-color to k8s-m02
  Normal  Pulled     2m25s  kubelet            Container image &quot;kodekloud/webapp-color:v3&quot; already present on machine
  Normal  Created    2m25s  kubelet            Created container kodekloud-env-color
  Normal  Started    2m25s  kubelet            Started container kodekloud-env-color
</code></pre>
<p>Save the current running pod as a YAML file.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods kodekloud-env-color -o yaml &gt; pod-env-variable.yaml
</code></pre>
<p>As this is a running Pod, the YAML output shows a lot of other details on Status, which is not required for now (to change the environment variable).</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-env-variable.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &quot;2022-02-15T13:52:07Z&quot;
  labels:
    run: kodekloud-env-color
  name: kodekloud-env-color
  namespace: default
  resourceVersion: &quot;15077&quot;
  uid: bd7c01ae-553e-4f77-b589-6af6560dcc88
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: pink
    image: kodekloud/webapp-color:v3
    imagePullPolicy: IfNotPresent
    name: kodekloud-env-color
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-xj9jz
      readOnly: true
  dnsPolicy: ClusterFirst
  &lt;SNIP&gt;
</code></pre>
<p>Create another copy of this YAML file and modify the definition to change the color fo the App to <code>darkblue</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ cp pod-env-variable.yaml pod-env-variable-2.yaml
pradeep@learnk8s$ vi pod-env-variable-2.yaml
</code></pre>
<p>The modified version looks like this.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-env-variable-2.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &quot;2022-02-15T13:52:07Z&quot;
  labels:
    run: kodekloud-env-color-2
  name: kodekloud-env-color-2
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: darkblue
    image: kodekloud/webapp-color:v3
    imagePullPolicy: IfNotPresent
    name: kodekloud-env-color-2
    resources: {}
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-env-variable-2.yaml
pod/kodekloud-env-color-2 created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep env
kodekloud-env-color         1/1     Running            0                14m     10.244.1.30   k8s-m02   &lt;none&gt;           &lt;none&gt;
kodekloud-env-color-2       1/1     Running            0                16s     10.244.1.31   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>We can verify that this new pod is using the <code>darkblue</code> color.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.1.31:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #130f40;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-env-color-2!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v3
  &lt;/h2&gt;


&lt;/div&gt;$ curl 10.244.1.31:8080/color
darkblue$
$ curl 10.244.1.31:8080/color
darkblue$
$ exit
logout
</code></pre>
<h3 id="configmaps">ConfigMaps</h3>
<p>As seen in the explanation, ConfigMaps hold the configuration data for pods to consume. For example, we could store the color of the Webapp from the previous example, in the form of a ConfigMap.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl explain configmaps
KIND:     ConfigMap
VERSION:  v1

DESCRIPTION:
     ConfigMap holds configuration data for pods to consume.
</code></pre>
<p>Before we create any configmaps, let us verify if we have any configmaps already in our cluster.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get configmaps
NAME               DATA   AGE
kube-root-ca.crt   1      7h16m
</code></pre>
<p>There is one configmap named <code>kube-root-ca.crt</code> already present with one DATA.
To see what is stored in this configmap, we can describe it and look at the Annotations.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe configmaps
Name:         kube-root-ca.crt
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  kubernetes.io/description:
                Contains a CA bundle that can be used to verify the kube-apiserver when using internal endpoints such as the internal service IP or kubern...

Data
====
ca.crt:
----
-----BEGIN CERTIFICATE-----
MIIDBjCCAe6gAwIBAgIBATANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p
a3ViZUNBMB4XDTIxMDYyODA0NTM1NloXDTMxMDYyNzA0NTM1NlowFTETMBEGA1UE
AxMKbWluaWt1YmVDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKZu
VYaX2I0BbQ7le2fCQiYL0UaoadaYm0UtRoYHCkGovRH4MBJlJipYp7IVZ+4oOgqZ
VtBC2o9oRzXnWEM6pJyTsrMrEhtDyTxAeCLY5iVwkLRCxrqxvsCVeGsTaF+0SMCy
PUXFaC20jrDVAxAUXs26Le0Fl2BjOv9k9K2hEScRuC0ogkF/oL+aC3BoGpdFmPVG
S+R18kS1UyBGpUWkktyXhtEAmZVdHn+PGeMW2W3cuHyF4OEqQiXE5xcXt6TQG64F
qgmE5xISVDp1/6VjlJeiVMqlyqZVm7dEKdtwcx6p8UKqnr7nL14STkzv6pDywk5v
egCFa7sP+amk41K6VMECAwEAAaNhMF8wDgYDVR0PAQH/BAQDAgKkMB0GA1UdJQQW
MBQGCCsGAQUFBwMCBggrBgEFBQcDATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQW
BBSL+jGni3bexvU9wL8lBR0FeLmCQDANBgkqhkiG9w0BAQsFAAOCAQEAd/sJdAwa
6hOYM52vQj0CiYfNqFAs1rK7klxzpPVa07sYIO8n8FjyzGTead+FxzeuxnOBxibu
y/447esB/RPE3d7hv+piQqT9FD7H/lskpHyIvffz4ai15P3DFtKQeY464bxVnynQ
eK4dvzySrqxDs5Q5mMC1PQn9ap7VvIRnz1wEr6hlHMqJ31G58rmnZ5V+eJBBUYvG
a39/4Q1PIMyk98T6QmJDTuLQngD8QJagcRMm4D2mzdieomt9jmIsrd5jINMhsC6d
pL/PMobEfOxGsZxSKW9RC4/mabcuo+Dty+xAN/cYlOrq6zGSatvQXu/60iDSx8un
D9XARLPvbRrYnA==
-----END CERTIFICATE-----


BinaryData
====

Events:  &lt;none&gt;
</code></pre>
<p>We can create configmaps in multiple different ways. Let us use the help option to see some examples.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl  create configmap -h
Create a config map based on a file, directory, or specified literal value.

 A single config map may package one or more key/value pairs.

 When creating a config map based on a file, the key will default to the basename of the file, and the value will
default to the file content.  If the basename is an invalid key, you may specify an alternate key.

 When creating a config map based on a directory, each file whose basename is a valid key in the directory will be
packaged into the config map.  Any directory entries except regular files are ignored (e.g. subdirectories, symlinks,
devices, pipes, etc).

Aliases:
configmap, cm

Examples:
  # Create a new config map named my-config based on folder bar
  kubectl create configmap my-config --from-file=path/to/bar

  # Create a new config map named my-config with specified keys instead of file basenames on disk
  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt

  # Create a new config map named my-config with key1=config1 and key2=config2
  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2

  # Create a new config map named my-config from the key=value pairs in the file
  kubectl create configmap my-config --from-file=path/to/bar

  # Create a new config map named my-config from an env file
  kubectl create configmap my-config --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
  &lt;SNIP&gt;
</code></pre>
<p>Using this examples, let us create a configmap named <code>webapp-color</code> using <code>--from-literal</code> option to set the color to <code>green</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create configmap webapp-color --from-literal=APP_COLOR=green
configmap/webapp-color created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get cm
NAME               DATA   AGE
kube-root-ca.crt   1      7h26m
webapp-color       1      5s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe cm webapp-color
Name:         webapp-color
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
APP_COLOR:
----
green

BinaryData
====

Events:  &lt;none&gt;
</code></pre>
<p>To use this configmap in a Pod definition, we need to make use of <code>-envFrom</code> and <code>-configMapRef</code> options in the Pod Spec.</p>
<pre><code class="language-yam">pradeep@learnk8s$ cat pod-config-map.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kodekloud-cm
spec:
  containers:
    - name: kodekloud-cm
      image: kodekloud/webapp-color:v3
      envFrom:
      - configMapRef:
          name: webapp-color
  restartPolicy: Never
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-config-map.yaml
pod/kodekloud-cm created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep cm
kodekloud-cm                1/1     Running            0                106s    10.244.0.14   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Verify that app color is changed to <code>green</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.0.14:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #16a085;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-cm!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v3
  &lt;/h2&gt;


&lt;/div&gt;$ curl 10.244.0.14:8080/color
green$
$ exit
logout
</code></pre>
<p>If we describe this pod, we can see the following section.
<em>Environment Variables from:
      webapp-color  ConfigMap  Optional: false</em></p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pod kodekloud-cm
Name:         kodekloud-cm
Namespace:    default
Priority:     0
Node:         k8s/192.168.177.29
Start Time:   Tue, 15 Feb 2022 20:09:51 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.0.14
IPs:
  IP:  10.244.0.14
Containers:
  kodekloud-cm:
    Container ID:   docker://53b0a746b4c95c65bbdce09f416ca6762ec9ce78d2a9618187a86d80465ec349
    Image:          kodekloud/webapp-color:v3
    Image ID:       docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Tue, 15 Feb 2022 20:09:53 +0530
    Ready:          True
    Restart Count:  0
    Environment Variables from:
      webapp-color  ConfigMap  Optional: false
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sq9q4 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-sq9q4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason        Age               From               Message
  ----     ------        ----              ----               -------
  Normal   Scheduled     10h               default-scheduler  Successfully assigned default/kodekloud-cm to k8s
  Normal   Pulled        10h               kubelet            Container image &quot;kodekloud/webapp-color:v3&quot; already present on machine
  Normal   Created       10h               kubelet            Created container kodekloud-cm
  Normal   Started       10h               kubelet            Started container kodekloud-cm
  Warning  NodeNotReady  9h (x2 over 10h)  node-controller    Node is not ready
</code></pre>
<h3 id="secrets">Secrets</h3>
<p>Secret holds secret data of a certain type.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl explain secrets
KIND:     Secret
VERSION:  v1

DESCRIPTION:
     Secret holds secret data of a certain type. The total bytes of the values
     in the Data field must be less than MaxSecretSize bytes.
&lt;SNIP&gt;
</code></pre>
<p>Verify if there are any secrets present in our cluster already!</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get secrets
NAME                  TYPE                                  DATA   AGE
default-token-b2xs6   kubernetes.io/service-account-token   3      17h
</code></pre>
<p>There is one secret of type <code>service-account-token</code>.</p>
<p>Describe it to see additional details of the secret.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe secrets
Name:         default-token-b2xs6
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  kubernetes.io/service-account.name: default
              kubernetes.io/service-account.uid: 5500622d-56e8-47c9-9440-7882c1d35512

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1111 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IkJGbThhZWVoY01TV3VDZ2hMU1RMenFlY0o2ckdsY2N3ZjB2ZFl5QWEtQm8ifQ.eyJpc3MiOiJrdWJlck5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VddC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tYjJ4czYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMjaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjU1MDA2MjJkLTU2ZTgtNDdjOS05NDQwLTc4ODJjMWQzNTUxMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.NGKbj5mw1EsZm4E14zDDpS2vNgL_L0WuGgP7Ex6k2TrXqKaO4wxX-ca3US1iSaUeYOIM4DLnXjDbXbP5J6YV-8ke--Yxhkks8iC_3tA9k1Q0YvK0RXS0T4WsL9i12sD44i-9LoJpL4Zpu3qJO-s4V5Plg9ifC69NpKoEu3CoOTMGmOX7DqDmGotogl5BJHflZekweX8GMOGP5WAv1FkUcROWyhj8wVMEFOzZIS8O8ssF67wHukWuUZ3IYDpCHy3QjQSKOWgeGGfFSDUXfW_mEQCE1ryl_c_-3TK7kGtlqN4l8aKrd2iBF-1MP4mdbq9WKDGBJDlZlGbHgPOZe2eBMw
</code></pre>
<p>How to create secrets?  Secrets can be of three types in Kubernetes: Docker Registry, Generic TLS.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create secret -h
Create a secret using specified subcommand.

Available Commands:
  docker-registry Create a secret for use with a Docker registry
  generic         Create a secret from a local file, directory, or literal value
  tls             Create a TLS secret

Usage:
  kubectl create secret [flags] [options]

Use &quot;kubectl &lt;command&gt; --help&quot; for more information about a given command.
Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create secret generic -h
Create a secret based on a file, directory, or specified literal value.

 A single secret may package one or more key/value pairs.

 When creating a secret based on a file, the key will default to the basename of the file, and the value will default to
the file content. If the basename is an invalid key or you wish to chose your own, you may specify an alternate key.

 When creating a secret based on a directory, each file whose basename is a valid key in the directory will be packaged
into the secret. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes,
etc).

Examples:
  # Create a new secret named my-secret with keys for each file in folder bar
  kubectl create secret generic my-secret --from-file=path/to/bar

  # Create a new secret named my-secret with specified keys instead of names on disk
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa
--from-file=ssh-publickey=path/to/id_rsa.pub

  # Create a new secret named my-secret with key1=supersecret and key2=topsecret
  kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret

  # Create a new secret named my-secret using a combination of a file and a literal
  kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret

  # Create a new secret named my-secret from env files
  kubectl create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env
  &lt;SNIP&gt;
</code></pre>
<p>Create a secret called <code>webapp-color-secret</code> and store the webapp color <code>blue</code> in it.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create secret generic webapp-color-secret --from-literal=APP_COLOR=blue
secret/webapp-color-secret created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get secrets
NAME                  TYPE                                  DATA   AGE
default-token-b2xs6   kubernetes.io/service-account-token   3      18h
webapp-color-secret   Opaque                                1      5s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe secrets webapp-color-secret
Name:         webapp-color-secret
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Type:  Opaque

Data
====
APP_COLOR:  4 bytes
</code></pre>
<p>It is time to use this secret in a Pod definition. We use the secrets in the same way we used configmaps. Instead of <code>configMapRef</code>, it would be <code>secretRef</code> now in the <code>envFrom</code> section.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-secret.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kodekloud-secret
spec:
  containers:
    - name: kodekloud-secret
      image: kodekloud/webapp-color:v3
      envFrom:
      - secretRef:
          name: webapp-color-secret
  restartPolicy: Never
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-secret.yaml
pod/kodekloud-secret created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide| grep secret
kodekloud-secret            1/1     Running            0              114s   10.244.1.32   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>To verify the color, login to the minikube node.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.1.32:8080
&lt;!doctype html&gt;
&lt;title&gt;Hello from Flask&lt;/title&gt;
&lt;body style=&quot;background: #2980b9;&quot;&gt;&lt;/body&gt;
&lt;div style=&quot;color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;&quot;&gt;

  &lt;h1&gt;Hello from kodekloud-secret!&lt;/h1&gt;



  &lt;h2&gt;
    Application Version: v3
  &lt;/h2&gt;


&lt;/div&gt;$ curl 10.244.1.32:8080/color
blue$ exit
logout
</code></pre>
<p>We can see that the app is using the <code>blue</code> color as defined in the secret.</p>
<p>As another final confirmation, we can describe this pod, to see 
<em>Environment Variables from:
  webapp-color-secret  Secret  Optional: false</em></p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods kodekloud-secret
Name:         kodekloud-secret
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Wed, 16 Feb 2022 06:47:47 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.32
IPs:
  IP:  10.244.1.32
Containers:
  kodekloud-secret:
    Container ID:   docker://799356d294bd0e1e6d37143ae8b12babcd9d50410d879ed099032dd4ceaec68a
    Image:          kodekloud/webapp-color:v3
    Image ID:       docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Wed, 16 Feb 2022 06:47:49 +0530
    Ready:          True
    Restart Count:  0
    Environment Variables from:
      webapp-color-secret  Secret  Optional: false
    Environment:           &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xw2ml (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-xw2ml:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  18s   default-scheduler  Successfully assigned default/kodekloud-secret to k8s-m02
  Normal  Pulled     17s   kubelet            Container image &quot;kodekloud/webapp-color:v3&quot; already present on machine
  Normal  Created    16s   kubelet            Created container kodekloud-secret
  Normal  Started    16s   kubelet            Started container kodekloud-secret
</code></pre>
<h3 id="multi-container-pods">Multi Container Pods</h3>
<p>As per Kubernetes documentation, The primary reason that Pods can have multiple containers is to support helper applications that assist a primary application. Typical examples of helper applications are data pullers, data pushers, and proxies. Helper and primary applications often need to communicate with each other. Typically this is done through a shared filesystem, as shown in this example.</p>
<p>We have not yet discussed volumes, but for now, it is sufficient to know that it is a storage component.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat multi-container-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-containers
spec:

  restartPolicy: Never

  volumes:
  - name: common-data
    emptyDir: {}

  containers:

  - name: nginx
    image: nginx
    volumeMounts:
    - name: common-data
      mountPath: /usr/share/nginx/html

  - name: ubuntu
    image: ubuntu
    volumeMounts:
    - name: common-data
      mountPath: /ubuntu-data
    command: [&quot;/bin/sh&quot;]
    args: [&quot;-c&quot;, &quot;echo Hello from the Ubuntu container which is visible in the nginx container as they are sharing the same storage common-data &gt; /ubuntu-data/index.html&quot;]
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f multi-container-pod.yaml
pod/multi-containers created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep multi
multi-containers            1/2     NotReady    0              8m3s   10.244.1.34   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>If you notice, though we have defined two containers (nginx and ubuntu) in this Pod definition, there is only a single IP (10.244.1.34 in this case). This is important to understand. All containers in a pod share the same network resources.</p>
<p>Also, READY column shows <code>1/2</code> meaning, there are two containers but only one is Running. Let us find out more on this by describing it.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods multi-containers
Name:         multi-containers
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Wed, 16 Feb 2022 07:19:50 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.34
IPs:
  IP:  10.244.1.34
Containers:
  nginx:
    Container ID:   docker://1cb2560ba88bf7f5a100c74246f10adecf6ce026d9f6031e43947c32ba021d3c
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Wed, 16 Feb 2022 07:20:09 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /usr/share/nginx/html from common-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7jtgb (ro)
  ubuntu:
    Container ID:  docker://d896bd674b06da660ee2e179d104af43344628dcf49edeb90630dc33fa63197e
    Image:         ubuntu
    Image ID:      docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      /bin/sh
    Args:
      -c
      echo Hello from the Ubuntu container which is visible in the nginx container as they are sharing the same storage common-data &gt; /ubuntu-data/index.html
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 16 Feb 2022 07:20:20 +0530
      Finished:     Wed, 16 Feb 2022 07:20:20 +0530
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /ubuntu-data from common-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7jtgb (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  common-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  &lt;unset&gt;
  kube-api-access-7jtgb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m8s   default-scheduler  Successfully assigned default/multi-containers to k8s-m02
  Normal  Pulling    3m6s   kubelet            Pulling image &quot;nginx&quot;
  Normal  Pulled     2m49s  kubelet            Successfully pulled image &quot;nginx&quot; in 17.095268831s
  Normal  Created    2m49s  kubelet            Created container nginx
  Normal  Started    2m49s  kubelet            Started container nginx
  Normal  Pulling    2m49s  kubelet            Pulling image &quot;ubuntu&quot;
  Normal  Pulled     2m38s  kubelet            Successfully pulled image &quot;ubuntu&quot; in 10.692871602s
  Normal  Created    2m38s  kubelet            Created container ubuntu
  Normal  Started    2m38s  kubelet            Started container ubuntu
</code></pre>
<p>We can see that the nginx container is Running but the ubuntu container is terminated with Reason as Completed. That means, Ubuntu container has finished its task. The task that we have given to this container is to echo a message and write that to a file. That's all.</p>
<p>To continue the verification, login to the nginx container. Remember the <code>kubectl exec</code> command?!</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it multi-containers -c nginx -- /bin/bash

root@multi-containers:/# curl localhost
Hello from the Ubuntu container which is visible in the nginx container as they are sharing the same storage common-data
root@multi-containers:/# exit
exit
</code></pre>
<p>The one difference that you might have noticed is the use of <code>-c</code> option here. When there are multiple containers in a pod, we need to be specific about which container we want to work with, by specifying its name. In this case , we want to login to the shell (bash) of the nginx container, and henc the <code>-c nginx</code> option. Also, we specify the command (/bin/bash) that we want to execute after the two dashes followed by a space (<code>--</code>).</p>
<p>We logged in to the nginx container, but the data written by the ubuntu container is visible here as both of them are using the common storage volume called <code>common-data</code>. </p>
<h3 id="init-containers">Init Containers</h3>
<p>Earlier, we saw that a Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started.</p>
<p>Init containers are exactly like regular containers, except:
Init containers always run to completion.
Each init container must complete successfully before the next one starts.</p>
<p>If init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.</p>
<p>Here is an example with one InitContainer called <code>wait-for-service</code> which is basically looking for the presence of a service called <code>my-service</code>. It does this check by performing <code>nslookup</code>. Till the kebernetes service called <code>my-service</code> is present, the name resolution will not succeed and this initContainer will not be finishing. Once the service is created, the initContainer job is done and it proceeds to the main container, in this example, it is the <code>ubuntu</code> container.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ more pod-with-init-container.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-init-container
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: ['sh', '-c', 'echo The ubuntu container is running! &amp;&amp; sleep 3600']
  initContainers:
  - name: wait-for-service
    image: busybox:1.28
    command: ['sh', '-c', &quot;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&quot;]
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-with-init-container.yaml
pod/pod-with-init-container created
</code></pre>
<p>Once we created the pod with initContainer, we can see the state as <code>Init:0/1</code> meaning there is one init container but it is not yet ready.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep init
pod-with-init-container     0/1     Init:0/1           0              7s    &lt;none&gt;        k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Waited for some more time, still it is in <code>Init:0/1</code> state.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep init
pod-with-init-container     0/1     Init:0/1           0                96s   10.244.0.15   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Let us describe this pod to get additional details.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods pod-with-init-container
Name:         pod-with-init-container
Namespace:    default
Priority:     0
Node:         k8s/192.168.177.29
Start Time:   Wed, 16 Feb 2022 07:48:13 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Pending
IP:
IPs:          &lt;none&gt;
Init Containers:
  wait-for-service:
    Container ID:
    Image:         busybox:1.28
    Image ID:
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
      -c
      until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
apiVersion: v1
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro)
Containers:
  ubuntu:
    Container ID:
    Image:         ubuntu
    Image ID:
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
      -c
      echo The ubuntu container is running! &amp;&amp; sleep 3600
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro)
Conditions:
  Type              Status
  Initialized       False
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  kube-api-access-7gp7r:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  17s   default-scheduler  Successfully assigned default/pod-with-init-container to k8s
  Normal  Pulling    14s   kubelet            Pulling image &quot;busybox:1.28&quot;
</code></pre>
<p>As seen above,  Init Container is in Waiting state and overall Pod status is Pending.</p>
<p>To make the InitiContainer successful, let us create a service called <code>myservice</code>.
Here is the sample definition of the service.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat init-container-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
</code></pre>
<p>Create the myservice from the YAML file.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f init-container-service.yaml
service/myservice created
</code></pre>
<p>Once the service is created, verify if there is any change in the main Pod status. It seems there is a change.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep init
pod-with-init-container     0/1     PodInitializing    0                108s   10.244.0.15   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>It is not yet in running state, it might take some more time. In the meanwhile, let us inspect the logs of the InitContainer that we created <code>wait-for-service</code>.</p>
<p>Remember the <code>kubectl logs</code> command?! and the <code>-c</code> option to specify a container name when there are multiple containers in a pod!</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl logs pod-with-init-container -c wait-for-service
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'myservice.default.svc.cluster.local'
waiting for myservice
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

waiting for myservice
nslookup: can't resolve 'myservice.default.svc.cluster.local'
Server:    10.96.0.10
nslookup: can't resolve 'myservice.default.svc.cluster.local'
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

&lt;SNIP&gt;

nslookup: can't resolve 'myservice.default.svc.cluster.local'
waiting for myservice
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      myservice.default.svc.cluster.local
Address 1: 10.111.136.80 myservice.default.svc.cluster.local
</code></pre>
<p>A lot of similar output is omitted for brevity, but we can clearly see that this container is continuosly checking for the resolution of <code>myservice.default.svc.cluster.local</code> but nslookup: can't resolve it. </p>
<p>Finally, after we created the <code>myservice</code> Service, name resolution was successful ( the last line). It (<code>myservice.default.svc.cluster.local</code>) is resolved to the IP address of 10.111.136.80.</p>
<p>This is nothing but the ClusterIP assigned to the <code>myservice</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP   19h
myservice    ClusterIP   10.111.136.80   &lt;none&gt;        80/TCP    9m13s
</code></pre>
<p>Going back to the main Pod, we can see it is in Running State.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep init
pod-with-init-container     1/1     Running            0               12m   10.244.0.15   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>If we describe it one more time, Init Container is in Terminated State with Reason as Completed. And the Ubuntu container is Running.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods pod-with-init-container
Name:         pod-with-init-container
Namespace:    default
Priority:     0
Node:         k8s/192.168.177.29
Start Time:   Wed, 16 Feb 2022 07:48:13 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.0.15
IPs:
  IP:  10.244.0.15
Init Containers:
  wait-for-service:
    Container ID:  docker://8b5668914aeef2c9c43eebf3ebf7b7289eff4efa96166198fc0c6db8cdfd358b
    Image:         busybox:1.28
    Image ID:      docker-pullable://busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
      -c
      until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 16 Feb 2022 07:48:52 +0530
      Finished:     Wed, 16 Feb 2022 07:49:52 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro)
Containers:
  ubuntu:
    Container ID:  docker://60caee7150e608901a2fda93fdc6758c8aa5208ec69320ed8c7982d57efde9fb
    Image:         ubuntu
    Image ID:      docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
      -c
      echo The ubuntu container is running! &amp;&amp; sleep 3600
    State:          Running
      Started:      Wed, 16 Feb 2022 07:59:33 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-7gp7r:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned default/pod-with-init-container to k8s
  Normal  Pulling    12m   kubelet            Pulling image &quot;busybox:1.28&quot;
  Normal  Pulled     11m   kubelet            Successfully pulled image &quot;busybox:1.28&quot; in 35.840326348s
  Normal  Created    11m   kubelet            Created container wait-for-service
  Normal  Started    11m   kubelet            Started container wait-for-service
  Normal  Pulling    10m   kubelet            Pulling image &quot;ubuntu&quot;
  Normal  Pulled     50s   kubelet            Successfully pulled image &quot;ubuntu&quot; in 9m39.673051136s
  Normal  Created    50s   kubelet            Created container ubuntu
  Normal  Started    50s   kubelet            Started container ubuntu
</code></pre>
<blockquote>
<p>:new: :date: </p>
</blockquote>
<h3 id="rollout">Rollout</h3>
<p>Let us take a look at the current deployments.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployment
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kodekloud   4/4     4            4           13d
</code></pre>
<p>To manage the rollout of a resource like deployments, kubernetes provides the <code>rollout</code> option, with which we can view rollout history, see the status of the rollout and even undo a previous rollout.</p>
<p>Here is the detailed help information.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout -h
Manage the rollout of a resource.

 Valid resource types include:

  *  deployments
  *  daemonsets
  *  statefulsets

Examples:
  # Rollback to the previous deployment
  kubectl rollout undo deployment/abc

  # Check the rollout status of a daemonset
  kubectl rollout status daemonset/foo

Available Commands:
  history     View rollout history
  pause       Mark the provided resource as paused
  restart     Restart a resource
  resume      Resume a paused resource
  status      Show the status of the rollout
  undo        Undo a previous rollout

Usage:
  kubectl rollout SUBCOMMAND [options]

Use &quot;kubectl &lt;command&gt; --help&quot; for more information about a given command.
Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).
</code></pre>
<p>Let us use this option to view the history of our <code>kodekloud</code> deployment.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment kodekloud
deployment.apps/kodekloud
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
3         &lt;none&gt;
4         &lt;none&gt;
</code></pre>
<p>We do see that there are multiple revisions to this <code>kodekloud</code> deployment, but we do not see any <code>CHANGE-CAUSE</code>.</p>
<p>Even in the description, we do see only one line related to <code>deployment.kubernetes.io/revision</code>. Pay attention to the <code>Annotations</code> section.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployments.apps
Name:               kodekloud
Namespace:          default
CreationTimestamp:  Tue, 15 Feb 2022 12:43:38 +0530
Labels:             app=kodekloud
Annotations:        deployment.kubernetes.io/revision: 4
Selector:           app=kodekloud
Replicas:           4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:       Recreate
MinReadySeconds:    0
Pod Template:
  Labels:  app=kodekloud
  Containers:
   webapp-color:
    Image:        kodekloud/webapp-color:v2
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   kodekloud-8477b7849 (4/4 replicas created)
Events:          &lt;none&gt;
</code></pre>
<p>Create a new deployment </p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat nginx-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80
````
```sh
pradeep@learnk8s$ kubectl apply -f nginx-deployment.yaml
deployment.apps/nginx-deployment created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployments.apps
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
kodekloud          4/4     4            4           13d
nginx-deployment   3/3     3            3           31s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.20
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-7b96fbf5d8 (3/3 replicas created)
Events:
  Type     Reason                 Age   From                   Message
  ----     ------                 ----  ----                   -------
  Warning  ReplicaSetCreateError  33s   deployment-controller  Failed to create new replica set &quot;nginx-deployment-7b96fbf5d8&quot;: Unauthorized
  Normal   ScalingReplicaSet      33s   deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 3
</code></pre>
<h3 id="update-deployment-using-kubectl-set">Update Deployment (using Kubectl Set)</h3>
<p>Earlier we have used <code>kubectl edit</code> to make changes to an existing deployment.</p>
<p>Here we will explore another option <code>kubectl set</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl set -h
Configure application resources.

 These commands help you make changes to existing application resources.

Available Commands:
  env            Update environment variables on a pod template
  image          Update the image of a pod template
  resources      Update resource requests/limits on objects with pod templates
  selector       Set the selector on a resource
  serviceaccount Update the service account of a resource
  subject        Update the user, group, or service account in a role binding or cluster role binding

Usage:
  kubectl set SUBCOMMAND [options]

Use &quot;kubectl &lt;command&gt; --help&quot; for more information about a given command.
Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).
</code></pre>
<p>Let us set the image for our <code>nginx-deployment</code> to a new version <code>1.21</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl set image deployment/nginx-deployment nginx=nginx:1.21
deployment.apps/nginx-deployment image updated
</code></pre>
<p>We can see the rollout status like this, using the <code>kubectl rollout status</code> command.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout status deployment/nginx-deployment

deployment &quot;nginx-deployment&quot; successfully rolled out
</code></pre>
<p>Describe the deployment </p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.21
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-5778cd94ff (3/3 replicas created)
Events:
  Type     Reason                 Age    From                   Message
  ----     ------                 ----   ----                   -------
  Warning  ReplicaSetCreateError  10m    deployment-controller  Failed to create new replica set &quot;nginx-deployment-7b96fbf5d8&quot;: Unauthorized
  Normal   ScalingReplicaSet      10m    deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      3m18s  deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      3m12s  deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      3m12s  deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      3m10s  deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      3m10s  deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      3m3s   deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 0
</code></pre>
<h3 id="kubectl-annotate">Kubectl Annotate</h3>
<p>Annotations are like labels (key/value pairs), they store arbitrary string values.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl annotate -h
Update the annotations on one or more resources.

 All Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are
key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and
system extensions may use annotations to store their own data.

 Attempting to set an annotation that already exists will fail unless --overwrite is set. If --resource-version is
specified and does not match the current resource version on the server the command will fail.

Use &quot;kubectl api-resources&quot; for a complete list of supported resources.

Examples:
  # Update pod 'foo' with the annotation 'description' and the value 'my frontend'
  # If the same annotation is set multiple times, only the last value will be applied
  kubectl annotate pods foo description='my frontend'

  # Update a pod identified by type and name in &quot;pod.json&quot;
  kubectl annotate -f pod.json description='my frontend'

  # Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any
existing value
  kubectl annotate --overwrite pods foo description='my frontend running nginx'

  # Update all pods in the namespace
  kubectl annotate pods --all description='my frontend running nginx'

  # Update pod 'foo' only if the resource is unchanged from version 1
  kubectl annotate pods foo description='my frontend running nginx' --resource-version=1

  # Update pod 'foo' by removing an annotation named 'description' if it exists
  # Does not require the --overwrite flag
  kubectl annotate pods foo description-

Options:
      --all=false: Select all resources, in the namespace of the specified resource types.
  -A, --all-namespaces=false: If true, check the specified action in all namespaces.
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in
the template. Only applies to golang and jsonpath output formats.
      --dry-run='none': Must be &quot;none&quot;, &quot;server&quot;, or &quot;client&quot;. If client strategy, only print the object that would be
sent, without sending it. If server strategy, submit server-side request without persisting the resource.
      --field-manager='kubectl-annotate': Name of the manager used to track field ownership.
      --field-selector='': Selector (field query) to filter on, supports '=', '==', and '!='.(e.g. --field-selector
key1=value1,key2=value2). The server only supports a limited number of field queries per type.
  -f, --filename=[]: Filename, directory, or URL to files identifying the resource to update the annotation
  -k, --kustomize='': Process the kustomization directory. This flag can't be used together with -f or -R.
      --list=false: If true, display the annotations for a given resource.
      --local=false: If true, annotation will NOT contact api-server but run locally.
  -o, --output='': Output format. One of:
json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.
      --overwrite=false: If true, allow annotations to be overwritten, otherwise reject annotation updates that
overwrite existing annotations.
  -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage
related manifests organized within the same directory.
      --resource-version='': If non-empty, the annotation update will only succeed if this is the current
resource-version for the object. Only valid when specifying a single resource.
  -l, --selector='': Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l
key1=value1,key2=value2).
      --show-managed-fields=false: If true, keep the managedFields when printing objects in JSON or YAML format.
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

Usage:
  kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]
[options]

Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=&quot;image updated to 1.21&quot;
deployment.apps/nginx-deployment annotated
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         image updated to 1.21
</code></pre>
<p>Look at the Annotations section of the description.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: image updated to 1.21
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.21
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-5778cd94ff (3/3 replicas created)
Events:
  Type     Reason                 Age    From                   Message
  ----     ------                 ----   ----                   -------
  Warning  ReplicaSetCreateError  14m    deployment-controller  Failed to create new replica set &quot;nginx-deployment-7b96fbf5d8&quot;: Unauthorized
  Normal   ScalingReplicaSet      14m    deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      6m57s  deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      6m51s  deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      6m51s  deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      6m49s  deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      6m49s  deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      6m42s  deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 0
</code></pre>
<p>We can see the details of a particular revision using the <code>--revision</code>  option.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment --revision=1
deployment.apps/nginx-deployment with revision #1
Pod Template:
  Labels:   app=nginx
    pod-template-hash=7b96fbf5d8
  Containers:
   nginx:
    Image:  nginx:1.20
    Port:   80/TCP
    Host Port:  0/TCP
    Environment:    &lt;none&gt;
    Mounts: &lt;none&gt;
  Volumes:  &lt;none&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment --revision=2
deployment.apps/nginx-deployment with revision #2
Pod Template:
  Labels:   app=nginx
    pod-template-hash=5778cd94ff
  Annotations:  kubernetes.io/change-cause: image updated to 1.21
  Containers:
   nginx:
    Image:  nginx:1.21
    Port:   80/TCP
    Host Port:  0/TCP
    Environment:    &lt;none&gt;
    Mounts: &lt;none&gt;
  Volumes:  &lt;none&gt;
</code></pre>
<h3 id="rollback">Rollback</h3>
<p>If you've decided to undo the current rollout and rollback to the previous revision, you can do so.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout undo deployment/nginx-deployment
deployment.apps/nginx-deployment rolled back
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout status deployment nginx-deployment
deployment &quot;nginx-deployment&quot; successfully rolled out
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         image updated to 1.21
3         &lt;none&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 3
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.20
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-7b96fbf5d8 (3/3 replicas created)
Events:
  Type     Reason                 Age                From                   Message
  ----     ------                 ----               ----                   -------
  Warning  ReplicaSetCreateError  20m                deployment-controller  Failed to create new replica set &quot;nginx-deployment-7b96fbf5d8&quot;: Unauthorized
  Normal   ScalingReplicaSet      13m                deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      12m                deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 0
  Normal   ScalingReplicaSet      50s                deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      48s                deployment-controller  Scaled down replica set nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      48s                deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      46s                deployment-controller  Scaled down replica set nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      45s (x2 over 20m)  deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      43s                deployment-controller  Scaled down replica set nginx-deployment-5778cd94ff to 0
</code></pre>
<p>Let us annotate revision 3.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl annotate deployments.apps nginx-deployment kubernetes.io/change-cause=&quot;image rolledback to 1.20&quot;
deployment.apps/nginx-deployment annotated
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
2         image updated to 1.21
3         image rolledback to 1.20
</code></pre>
<p>What happens if we rollback the deployment again?</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout undo deployment/nginx-deployment
deployment.apps/nginx-deployment rolled back
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
3         image rolledback to 1.20
4         image updated to 1.21
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment --revision=4
deployment.apps/nginx-deployment with revision #4
Pod Template:
  Labels:   app=nginx
    pod-template-hash=5778cd94ff
  Annotations:  kubernetes.io/change-cause: image updated to 1.21
  Containers:
   nginx:
    Image:  nginx:1.21
    Port:   80/TCP
    Host Port:  0/TCP
    Environment:    &lt;none&gt;
    Mounts: &lt;none&gt;
  Volumes:  &lt;none&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 4
                        kubernetes.io/change-cause: image updated to 1.21
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.21
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-5778cd94ff (3/3 replicas created)
Events:
  Type     Reason                 Age                    From                   Message
  ----     ------                 ----                   ----                   -------
  Warning  ReplicaSetCreateError  31m                    deployment-controller  Failed to create new replica set &quot;nginx-deployment-7b96fbf5d8&quot;: Unauthorized
  Normal   ScalingReplicaSet      23m                    deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      23m                    deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      23m                    deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 0
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled down replica set nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled down replica set nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      11m (x2 over 31m)      deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      11m                    deployment-controller  Scaled down replica set nginx-deployment-5778cd94ff to 0
  Normal   ScalingReplicaSet      2m27s (x2 over 24m)    deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      2m25s (x2 over 23m)    deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      2m25s (x2 over 23m)    deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      2m20s (x3 over 2m23s)  deployment-controller  (combined from similar events): Scaled down replica set nginx-deployment-7b96fbf5d8 to 0
</code></pre>
<p>Let us update the deployment one more time, finally to the latest version.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl set image deployment/nginx-deployment nginx=nginx:latest
deployment.apps/nginx-deployment image updated
</code></pre>
<p>Check the rollout history, and note that the existing annotation continued to the latest revision.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
3         image rolledback to 1.20
4         image updated to 1.21
5         image updated to 1.21
</code></pre>
<p>Change the annotation</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl annotate deployments.apps nginx-deployment kubernetes.io/change-cause=&quot;image updated to the latest&quot;
deployment.apps/nginx-deployment annotated
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
3         image rolledback to 1.20
4         image updated to 1.21
5         image updated to the latest
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 01 Mar 2022 06:05:50 +0530
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 5
                        kubernetes.io/change-cause: image updated to the latest
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:latest
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-67dffbbbb (3/3 replicas created)
Events:
  Type     Reason                 Age                  From                   Message
  ----     ------                 ----                 ----                   -------
  Warning  ReplicaSetCreateError  40m                  deployment-controller  Failed to create new replica set &quot;nginx-deployment-7b96fbf5d8&quot;: Unauthorized
  Normal   ScalingReplicaSet      32m                  deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      32m                  deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 3
  Normal   ScalingReplicaSet      32m                  deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 0
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 1
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled down replica set nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled down replica set nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      20m (x2 over 40m)    deployment-controller  Scaled up replica set nginx-deployment-7b96fbf5d8 to 3
  Normal   ScalingReplicaSet      20m                  deployment-controller  Scaled down replica set nginx-deployment-5778cd94ff to 0
  Normal   ScalingReplicaSet      11m (x2 over 33m)    deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 1
  Normal   ScalingReplicaSet      11m (x2 over 33m)    deployment-controller  Scaled up replica set nginx-deployment-5778cd94ff to 2
  Normal   ScalingReplicaSet      11m (x2 over 33m)    deployment-controller  Scaled down replica set nginx-deployment-7b96fbf5d8 to 2
  Normal   ScalingReplicaSet      2m52s (x9 over 11m)  deployment-controller  (combined from similar events): Scaled down replica set nginx-deployment-5778cd94ff to 0
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME                               READY   STATUS                   RESTARTS         AGE
kodekloud-8477b7849-blzrs          1/1     Running                  1                13d
kodekloud-8477b7849-m65m8          1/1     Running                  1                13d
kodekloud-8477b7849-p7psw          1/1     Running                  0                32h
kodekloud-8477b7849-vf9zs          1/1     Running                  0                32h
kodekloud-cm                       0/1     ContainerStatusUnknown   1                13d
nginx-deployment-67dffbbbb-7ttxq   1/1     Running                  0                3m35s
nginx-deployment-67dffbbbb-8dtsf   1/1     Running                  0                3m38s
nginx-deployment-67dffbbbb-sz5bb   1/1     Running                  0                3m40s
pod-with-hostpath-volume           0/1     ContainerCreating        1                8d
pod-with-init-container            1/1     Running                  45 (4m46s ago)   12d
security-context-demo-cap          1/1     Running                  15 (5m16s ago)   9d
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
kodekloud-589c9f4b47          0         0         0       13d
kodekloud-676c6b9fcd          0         0         0       13d
kodekloud-8477b7849           4         4         4       13d
nginx-deployment-5778cd94ff   0         0         0       34m
nginx-deployment-67dffbbbb    3         3         3       3m52s
nginx-deployment-7b96fbf5d8   0         0         0       41m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment --revision=5
deployment.apps/nginx-deployment with revision #5
Pod Template:
  Labels:   app=nginx
    pod-template-hash=67dffbbbb
  Annotations:  kubernetes.io/change-cause: image updated to the latest
  Containers:
   nginx:
    Image:  nginx:latest
    Port:   80/TCP
    Host Port:  0/TCP
    Environment:    &lt;none&gt;
    Mounts: &lt;none&gt;
  Volumes:  &lt;none&gt;
</code></pre>
<h2 id="security">Security</h2>
<h3 id="certificates">Certificates</h3>
<p>Take a look at the description of the <code>kube-apiserver</code> pod in the <code>kube-system</code> namespace. Pay attention to the <code>kube-apiserver</code> command arguments. It is a long list!!</p>
<p>Primarily, there are many pairs of certfile and keyfile for apiserver, etcd, kubectl etc. In Minikube setup, most of these are located in <code>/var/lib/minikube/certs/</code>. It would be different when the cluster is setup with <code>kubeadm</code> which we have not discussed yet.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe -n kube-system pods kube-apiserver-k8s
Name:                 kube-apiserver-k8s
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 k8s/192.168.177.29
Start Time:           Tue, 15 Feb 2022 12:28:03 +0530
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.177.29:8443
                      kubernetes.io/config.hash: cca804e910f3a6e748c66a6963d63fdd
                      kubernetes.io/config.mirror: cca804e910f3a6e748c66a6963d63fdd
                      kubernetes.io/config.seen: 2022-02-15T06:58:02.427574419Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   192.168.177.29
IPs:
  IP:           192.168.177.29
Controlled By:  Node/k8s
Containers:
  kube-apiserver:
    Container ID:  docker://0ec9bc91aa13e593b1518fac7a4f9f39c7e16a0e478c2362336b8c050f9c085c
    Image:         k8s.gcr.io/kube-apiserver:v1.23.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-apiserver@sha256:f54681a71cce62cbc1b13ebb3dbf1d880f849112789811f98b6aebd2caa2f255
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      kube-apiserver
      --advertise-address=192.168.177.29
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/var/lib/minikube/certs/ca.crt
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt
      --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt
      --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt
      --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt
      --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --secure-port=8443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/var/lib/minikube/certs/sa.pub
      --service-account-signing-key-file=/var/lib/minikube/certs/sa.key
      --service-cluster-ip-range=10.96.0.0/12
      --tls-cert-file=/var/lib/minikube/certs/apiserver.crt
      --tls-private-key-file=/var/lib/minikube/certs/apiserver.key
    State:          Running
      Started:      Tue, 15 Feb 2022 12:27:50 +0530
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:        250m
    Liveness:     http-get https://192.168.177.29:8443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://192.168.177.29:8443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://192.168.177.29:8443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  &lt;none&gt;
    Mounts:
      /etc/ssl/certs from ca-certs (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
      /var/lib/minikube/certs from k8s-certs (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/minikube/certs
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    &lt;none&gt;
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                  From     Message
  ----     ------     ----                 ----     -------
  Warning  Unhealthy  178m (x5 over 30h)   kubelet  Liveness probe failed: Get &quot;https://192.168.177.29:8443/livez&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  177m (x17 over 30h)  kubelet  Readiness probe failed: Get &quot;https://192.168.177.29:8443/readyz&quot;: net/http: TLS handshake timeout
  Warning  Unhealthy  170m (x7 over 27h)   kubelet  Readiness probe failed: Get &quot;https://192.168.177.29:8443/readyz&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  170m (x6 over 27h)   kubelet  Liveness probe failed: Get &quot;https://192.168.177.29:8443/livez&quot;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  170m (x8 over 30h)   kubelet  Readiness probe failed: Get &quot;https://192.168.177.29:8443/readyz&quot;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
</code></pre>
<p>We can login to the minikube node and use the <code>openssl x509</code> command to view the actual certificate. Here is an example. Pass the certificate path as <code>-in</code> argument and use <code>-text</code> to display the certificate in plain-text.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ openssl x509 -in /var/lib/minikube/certs/apiserver.crt -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 2 (0x2)
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN = minikubeCA
        Validity
            Not Before: Feb 14 06:54:36 2022 GMT
            Not After : Feb 14 06:54:36 2025 GMT
        Subject: O = system:masters, CN = minikube
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                RSA Public-Key: (2048 bit)
                Modulus:
                    00:c7:e1:19:9b:17:43:df:ff:31:4e:fe:66:7c:4b:
                    c4:1a:63:e6:5d:a6:a2:85:1c:11:b9:5a:72:42:50:
                    8b:25:a4:f0:89:eb:24:bf:3b:a6:e6:79:26:b8:18:
                    ed:9b:7b:76:01:68:a4:b1:1b:39:d8:b8:36:14:21:
                    44:b5:26:57:a3:a6:d3:55:e2:8c:32:5b:55:71:1e:
                    47:3e:56:b6:e8:92:86:af:aa:90:d1:4a:5a:36:ac:
                    a7:4f:a4:6c:09:a6:16:3b:e7:76:bc:41:18:89:7e:
                    be:87:df:c7:a9:ee:b7:da:34:43:ae:9f:37:cd:5d:
                    8d:e2:71:5c:e6:4c:e4:60:46:8c:b1:ef:7d:90:4b:
                    51:c3:e3:7f:a7:84:fe:06:28:1a:28:18:fd:9a:00:
                    b0:a7:d9:c9:b1:61:c9:d7:81:2d:c1:5d:5b:d2:f3:
                    f6:13:e4:d8:7f:d6:5c:c0:39:56:b1:14:04:f6:b7:
                    ea:9b:50:d7:aa:4d:f2:20:89:8b:8b:bc:81:b0:91:
                    4f:9b:f2:b9:69:b5:ce:80:67:a4:9e:f3:ba:17:03:
                    f6:89:ee:22:0e:8d:65:61:ef:16:96:67:dc:d7:4f:
                    ea:aa:36:7a:0c:59:53:2d:2a:fd:01:0f:93:15:fa:
                    8d:42:94:da:f1:0d:c8:8e:6b:15:b3:8b:4f:de:1d:
                    03:2b
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage:
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Authority Key Identifier:
                keyid:8B:FA:31:A7:8B:76:DE:C6:F5:3D:C0:BF:25:05:1D:05:78:B9:82:40

            X509v3 Subject Alternative Name:
                DNS:minikubeCA, DNS:control-plane.minikube.internal, DNS:kubernetes.default.svc.cluster.local, DNS:kubernetes.default.svc, DNS:kubernetes.default, DNS:kubernetes, DNS:localhost, IP Address:192.168.177.29, IP Address:10.96.0.1, IP Address:127.0.0.1, IP Address:10.0.0.1
    Signature Algorithm: sha256WithRSAEncryption
         8e:94:63:81:ad:57:80:84:2d:89:8b:3c:af:7c:13:d1:6c:49:
         53:53:61:cc:cb:bc:9d:63:93:9b:4e:b2:0e:a0:e3:9d:22:e4:
         4e:a9:de:75:88:05:23:46:bb:75:4c:be:ff:ba:68:e3:19:d0:
         15:b2:6a:01:5d:5b:ea:d0:a2:2d:53:80:99:25:e9:4f:f0:1a:
         65:47:c3:e4:8e:06:6c:db:23:55:57:64:f3:0d:5a:4a:e8:63:
         b2:b6:57:00:13:85:29:fe:e0:de:06:d6:e3:ec:f3:96:1d:5c:
         e7:03:8f:46:d9:bf:6b:f5:dd:1a:41:db:15:23:14:36:42:c3:
         c7:34:28:2e:a3:c4:e8:99:29:6c:28:9b:40:35:aa:58:0e:4a:
         b4:fd:0b:b4:11:a6:c5:f4:10:97:9b:c8:1c:ec:ea:a0:77:7c:
         c2:b1:70:c6:7b:85:34:8a:36:b0:ca:35:6a:7c:1c:e9:4e:08:
         9c:f9:be:de:41:ce:84:5e:51:60:52:e0:63:89:a7:18:1f:23:
         3e:f2:8e:0c:d6:9d:d2:38:04:cd:cc:2c:2e:70:c8:57:99:2b:
         3e:ba:08:1f:86:f4:0f:39:63:55:71:33:bc:49:ac:44:cf:e6:
         4f:27:dd:78:45:88:13:a7:57:d1:a3:09:76:cb:06:00:4b:84:
         df:ac:cb:0e
-----BEGIN CERTIFICATE-----
MIID3DCCAsSgAwIBAgIBAjANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p
a3ViZUNBMB4XDTIyMDIxNDA2NTQzNloXDTI1MDIxNDA2NTQzNlowLDEXMBUGA1UE
ChMOc3lzdGVtOm1hc3RlcnMxETAPBgNVBAMTCG1pbmlrdWJlMIIBIjANBgkqhkiG
9w0BAQEFAAOCAQ8AMIIBCgKCAQEAx+EZmxdD3/8xTv5mfEvEGmPmXaaihRwRuVpy
QlCLJaTwieskvzum5nkmuBjtm3t2AWigsRs52Lg2FCFEtSZXo6bTVeKMMltVcR5H
Pla26JKGr6qQ0UpaNqynT6RsCaYWO+d2vEEYiX6+h9/Hqe632jRDrp83zV2N4nFc
5kzkYEaMse99kEtRw+N/p4T+BigaKBj9mgCwp9nJsWHJ14EtwV1b0vP2E+TYf9Zc
wDlWsRQE9rfqm1DXqk3yIImLi7yBsJFPm/K5abXOgGeknvO6FwP2ie4iDo1lYe8W
lmfc10/qqjZ6DFlTLSr9AQ+TFfqNQpTa8Q3IjmsVs4tP3h0DKwIDAQABo4IBHjCC
ARowDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEFBQcD
AjAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFIv6MadLdt7G9T3AvyUFHQV4uYJA
MIG5BgNVHREEgbEwgb6CCm1pbmlrdWJlQ0GCH2NvbnRyb2wtcGxhbmUubWluaWt1
YmUuaW50ZXJuYWyCJGt1YmVybmV0ZXMuZGVmYXVsdC5zdmMuY2x1c3Rlci5sb2Nh
bIIWa3ViZXJuZXRlcy5kZWZhdWx0LnN2Y4ISa3ViZXJuZXRlcy5kZWZhdWx0ggpr
dWJlcm5ldGVzgglsb2NhbGhvc3SHBMCosR2HBApgAAGHBH8AAAGHBAoAAAEwDQYJ
KoZIhvcNAQELBQADggEBAI6UY4GtV4CELYmLPK98E9FsSVNTYczLvJ1jk5tOsg6g
450i5E6p3nWIBSNGu3VMvv+6aOMZ0BWyagFdW+rQoi1TgJkl6U/wGmVHw+SOBmzb
I1VXZPMNWkroY7K2VwAThSn+4N4G1uPs85YdXOcDj0bZv2v13Roh2xUjFDZCw8c0
KC6jxOiZKWwom0A1qlgOSrT9C7QRpsX0EJtbyBzs6qB3fMKxcMZ7hTSKNrDKNWp8
HOlOCJz5vt5BzoReUWBS4GOJpxgfIz7yjgzWndI4BM3MLC5wyFeZKz66CB+G9A85
Y1VxM7xJrETP5k8n3XhFiBOnV9GjCXbLBgBLhN+syw4=
-----END CERTIFICATE-----
$
</code></pre>
<p>We can see that the  <code>Issuer: CN = minikubeCA</code> ,  <code>Subject: O = system:masters, CN = minikube</code> and <code>Subject Alternative Name: DNS:minikubeCA, DNS:control-plane.minikube.internal, DNS:kubernetes.default.svc.cluster.local, DNS:kubernetes.default.svc, DNS:kubernetes.default, DNS:kubernetes, DNS:localhost, IP Address:192.168.177.29, IP Address:10.96.0.1, IP Address:127.0.0.1, IP Address:10.0.0.1</code>.</p>
<p>Similarly, let us take a look at the description of the <code>etcd</code> pod in the <code>kube-system</code> namespace and pay attention to the <code>etcd</code> command arguments. Again a long list, but for now, just look for cert and key files.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe -n kube-system pods etcd-k8s
Name:                 etcd-k8s
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 k8s/192.168.177.29
Start Time:           Tue, 15 Feb 2022 12:28:03 +0530
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.177.29:2379
                      kubernetes.io/config.hash: 553a1d887eba16384375f475194d677c
                      kubernetes.io/config.mirror: 553a1d887eba16384375f475194d677c
                      kubernetes.io/config.seen: 2022-02-15T06:58:02.427571167Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   192.168.177.29
IPs:
  IP:           192.168.177.29
Controlled By:  Node/k8s
Containers:
  etcd:
    Container ID:  docker://7ad2e9b1c5fb22610382e026ecea82c4def63655f4771b398416b6dfd7b88374
    Image:         k8s.gcr.io/etcd:3.5.1-0
    Image ID:      docker-pullable://k8s.gcr.io/etcd@sha256:64b9ea357325d5db9f8a723dcf503b5a449177b17ac87d69481e126bb724c263
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      etcd
      --advertise-client-urls=https://192.168.177.29:2379
      --cert-file=/var/lib/minikube/certs/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/minikube/etcd
      --initial-advertise-peer-urls=https://192.168.177.29:2380
      --initial-cluster=k8s=https://192.168.177.29:2380
      --key-file=/var/lib/minikube/certs/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://192.168.177.29:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://192.168.177.29:2380
      --name=k8s
      --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/var/lib/minikube/certs/etcd/peer.key
      --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
      --proxy-refresh-interval=70000
      --snapshot-count=10000
      --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
    State:          Running
      Started:      Tue, 15 Feb 2022 12:27:49 +0530
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  &lt;none&gt;
    Mounts:
      /var/lib/minikube/certs/etcd from etcd-certs (rw)
      /var/lib/minikube/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/minikube/certs/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/minikube/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    &lt;none&gt;
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                   From     Message
  ----     ------     ----                  ----     -------
  Warning  Unhealthy  3h18m (x12 over 28h)  kubelet  Liveness probe failed: Get &quot;http://127.0.0.1:2381/health&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)
</code></pre>
<p>ETCD can have its own Certificate Authority (CA). Let us verify if both api-server and etcd are using the same CA certificate or different.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ openssl x509 -in /var/lib/minikube/certs/etcd/ca.crt -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 0 (0x0)
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN = etcd-ca
        Validity
            Not Before: Feb 15 06:54:39 2022 GMT
            Not After : Feb 13 06:54:39 2032 GMT
        Subject: CN = etcd-ca
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                RSA Public-Key: (2048 bit)
                Modulus:
                    00:c2:8c:1f:12:73:c7:89:bc:f9:8d:14:c2:5e:61:
                    2f:ae:d4:f0:48:89:f7:cc:bc:be:a6:19:9d:a8:4d:
                    85:99:59:d6:96:57:35:9e:6c:62:39:31:12:e6:ee:
                    32:f9:a4:aa:ce:91:8b:5f:1d:7d:99:cc:b5:fc:c7:
                    14:75:f0:a2:43:4f:fd:e6:e7:b3:ab:11:c2:5f:db:
                    a8:72:b8:80:2f:66:5f:98:21:3b:06:af:b9:09:69:
                    94:1a:06:33:96:2f:1c:c7:f8:b2:ca:bd:87:d9:13:
                    36:c5:f6:de:aa:6a:81:c2:d4:94:2b:9e:63:dc:56:
                    27:b4:32:31:1d:49:ab:69:0e:dc:d1:14:d3:bb:f1:
                    43:80:19:31:73:29:7e:7b:d4:3b:2d:cf:14:7f:3b:
                    3c:84:4b:21:a4:2d:a6:59:79:bb:6f:1d:dc:5c:d5:
                    44:fd:3f:bd:34:b4:33:38:0d:cc:76:48:e1:de:53:
                    02:2d:54:79:44:22:64:f5:a6:39:1e:87:24:6c:91:
                    3c:5f:eb:7f:1f:84:38:e0:96:19:1b:46:9d:fc:e6:
                    79:98:c7:2e:6e:2a:2e:63:f0:28:42:57:16:14:45:
                    f3:de:bf:32:8e:d9:49:e5:ab:a5:06:6e:0d:d2:9c:
                    8b:65:40:46:17:43:3e:d2:46:53:bf:22:89:72:d0:
                    6f:49
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment, Certificate Sign
            X509v3 Basic Constraints: critical
                CA:TRUE
            X509v3 Subject Key Identifier:
                E3:7B:E5:78:30:93:55:DB:08:E4:EB:D0:2E:0C:30:6C:DB:17:40:40
            X509v3 Subject Alternative Name:
                DNS:etcd-ca
    Signature Algorithm: sha256WithRSAEncryption
         1a:29:48:3a:dc:24:57:35:20:b7:21:31:ed:c5:12:88:d5:79:
         7e:42:ca:21:12:92:64:49:5e:eb:2e:60:ce:16:71:5f:43:76:
         09:97:3d:6c:19:15:16:b0:6d:fc:c9:84:14:e7:5b:c2:c8:d4:
         24:77:bf:fd:87:3d:e4:c9:e4:39:49:ba:f6:41:bb:a0:9c:97:
         ef:71:b0:46:a1:86:dc:00:6b:19:26:39:32:26:c4:0c:70:4e:
         bf:1b:6b:93:55:54:d7:97:89:07:5e:e9:3b:63:a5:49:4e:6c:
         21:aa:75:8b:b1:a9:94:68:bf:1c:2e:cf:84:09:b0:52:03:62:
         72:54:b0:e2:c8:63:88:31:c0:1e:de:38:89:39:25:92:df:b9:
         1d:56:fb:c5:3b:71:fa:4e:70:e7:ec:1b:c5:fc:39:bb:71:90:
         ab:d3:36:c1:80:c5:30:6f:4d:8b:7c:8a:ee:24:15:f5:fc:5c:
         63:47:51:a0:9f:eb:30:ee:4e:95:a7:10:41:10:44:37:1e:19:
         0d:37:65:f5:94:66:4a:93:5e:fb:df:f3:24:28:17:4e:7e:7f:
         4f:d0:97:3a:24:b2:95:27:42:6f:42:0d:32:c7:b6:a6:a2:0f:
         66:df:91:e9:af:c7:66:a9:eb:01:d4:74:ae:2c:1f:72:b8:40:
         5e:15:d6:bc
-----BEGIN CERTIFICATE-----
MIIC9TCCAd2gAwIBAgIBADANBgkqhkiG9w0BAQsFADASMRAwDgYDVQQDEwdldGNk
LWNhMB4XDTIyMDIxNTA2NTQzOVoXDTMyMDIxMzA2NTQzOVowEjEQMA4GA1UEAxMH
ZXRjZC1jYTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMKMHxJzx4m8
+Y0Uwl5hL67U8EiJ98y8vqYZnahNhZlZ1pZXNZ5sYjkxEubuMvmkqs6Ri18dfZnM
tfzHFHXwokNP/ebns6sRwl/bqHK4gC9mX5ghOwabuQlplBoGM5YvHMf4ssq9h9kT
NsX23qpqgcLUlCueY9xWJ7QyMR1Jq2kO3NEU07vxQ4AZMXMpfnvUOy3PFH87PIRL
IaQtpll5u28d3FzVRP0/vTS0MzgNzHZI4d5TAi1UeUQiZPWmOR6HJGyRPF/rfx+E
OOCWGRtGnfzmeZjHLm4qLmPwKEJXFhRF896/Mo7ZSeWrpQZuDdKci2VARhdDPtJG
U78iiXLQb0kCAwEAAaNWMFQwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB/wQFMAMB
Af8wHQYDVR0OBBYEFON75Xgwk1XbCOTr0C4MMGzbF0BAMBIGA1UdEQQLMAmCB2V0
Y2QtY2EwDQYJKoZIhvcNAQELBQADggEBABopSDrfJFc1ILchMe3FEojVeX5CyiES
kmRJXusuYM4WcV9DdgmXPWwZFRawbfzJhBTnW8LI1CR3v/2HPeTJ5DlJuvZBu6Cc
l+9xsEahhtwAaxkmOTImxAxwTr8ba5NVVNeXiQde6TtjpUlObCGqdYuxqZRovxwu
z4QJsFIDYnJUsOLIY4gxwB7eOIk5JZLfuR1W+8U7cfpOcOfsG8X8ObtxkKvTNsGA
xTBvTYt8iu4kFfX8XGNHUaCf6zDuTpWnEEEQRDceGQ03ZfWUZkqTXvvf8yQoF05+
f0/QlzokspUnQm9CDTLHtqaiD2bfkemvx2ap6wHUdK4sH3K4QF4V1rw=
-----END CERTIFICATE-----
$
</code></pre>
<p>We can see it is a different Issuer: CN = etcd-ca, and Subject: CN = etcd-ca, X509v3 Subject Alternative Name: DNS:etcd-ca.</p>
<p>If we look at the ETCD server certificate subject details, we can see the CN as <code>k8s</code> and subject alternative names of <code>k8s</code>, <code>localhost</code> and the node IP address <code>192.168.177.29</code>.</p>
<pre><code class="language-sh">$ openssl x509 -in /var/lib/minikube/certs/etcd/server.crt -text | grep -e Subject -e DNS
        Subject: CN = k8s
        Subject Public Key Info:
            X509v3 Subject Alternative Name:
                DNS:k8s, DNS:localhost, IP Address:192.168.177.29, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1
$
</code></pre>
<p>Also, we can see that this certificate is valid for one year.</p>
<pre><code class="language-sh">openssl x509 -in /var/lib/minikube/certs/etcd/server.crt -text | grep -A 2 Validity
        Validity
            Not Before: Feb 15 06:54:39 2022 GMT
            Not After : Feb 15 06:54:39 2023 GMT
$
</code></pre>
<p>Where as the <code>minikubeCA</code> certificate is valid for ten years.</p>
<pre><code class="language-sh">$ openssl x509 -in /var/lib/minikube/certs/ca.crt -text | grep -A 2 Validity
        Validity
            Not Before: Jun 28 04:53:56 2021 GMT
            Not After : Jun 27 04:53:56 2031 GMT
</code></pre>
<h4 id="certificates-api">Certificates API</h4>
<p>Let us create a new user called <code>pradeep</code> and generate a key pair. Once the keypair is generated, created a certificate signing request to be submitted to the Kubernetes Cluster CA.</p>
<pre><code class="language-sh">$ sudo adduser -G wheel pradeep
Changing password for pradeep
New password:
Bad password: similar to username
Retype password:
passwd: password for pradeep changed by root
$ su - pradeep
Password:
$ mkdir .certs
$ cd .certs/
$ openssl genrsa -out pradeep.key 2048
Generating RSA private key, 2048 bit long modulus (2 primes)
........+++++
......................................................................................................+++++
e is 65537 (0x010001)
$ openssl req -new -key pradeep.key -out pradeep.csr -subj &quot;/CN=pradeep&quot;
$ exit
logout
</code></pre>
<p>We need to convert this CSR file to base64 format and trim newline character, which needs to be passed in the CSR request.</p>
<pre><code class="language-sh">$ ls
pradeep.csr  pradeep.key
$ cat pradeep.csr | base64 | tr -d &quot;\n&quot;
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=$
$
</code></pre>
<p>Create an YAML manifest for the CertificateSigningRequest API resource.</p>
<pre><code class="language-sh">pradeep@learnk8s$ cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: pradeep
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  expirationSeconds: 86400  # one day
  usages:
  - client auth
  signerName: kubernetes.io/kube-apiserver-client
EOF
certificatesigningrequest.certificates.k8s.io/pradeep created
</code></pre>
<p>Verify that the new request is in Pending state.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get csr
NAME      AGE   SIGNERNAME                            REQUESTOR       REQUESTEDDURATION   CONDITION
pradeep   4s    kubernetes.io/kube-apiserver-client   minikube-user   24h                 Pending
</code></pre>
<p>As we know this is a valid request, we can go ahead and approve this.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl certificate approve pradeep
certificatesigningrequest.certificates.k8s.io/pradeep approved
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get csr
NAME      AGE   SIGNERNAME                            REQUESTOR       REQUESTEDDURATION   CONDITION
pradeep   66s   kubernetes.io/kube-apiserver-client   minikube-user   24h                 Approved,Issued
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe csr
Name:         pradeep
Labels:       &lt;none&gt;
Annotations:  kubectl.kubernetes.io/last-applied-configuration={&quot;apiVersion&quot;:&quot;certificates.k8s.io/v1&quot;,&quot;kind&quot;:&quot;CertificateSigningRequest&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;pradeep&quot;},&quot;spec&quot;:{&quot;expirationSeconds&quot;:86400,&quot;request&quot;:&quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=&quot;,&quot;signerName&quot;:&quot;kubernetes.io/kube-apiserver-client&quot;,&quot;usages&quot;:[&quot;client auth&quot;]}}

CreationTimestamp:   Thu, 17 Feb 2022 11:17:16 +0530
Requesting User:     minikube-user
Signer:              kubernetes.io/kube-apiserver-client
Requested Duration:  24h
Status:              Approved,Issued
Subject:
         Common Name:    pradeep
         Serial Number:
Events:  &lt;none&gt;
</code></pre>
<h3 id="kube-config">Kube Config</h3>
<p>Kubectl command uses a  configuration file that contains <code>cluster</code> and <code>context</code> information along with <code>users</code>.
This configuration file can be viewed directly using the common commands like <code>cat</code>, or <code>kubectl config view</code> command. Usually the configuraiton is stored in a filed called <code>config</code> in the hidden directory <code>.kube</code> inside the user home directory (<code>/Users/pradeep/</code> in this case).</p>
<p>Here we can see the same output using both methods.</p>
<p>Using the <code>kubectl config view</code> command:</p>
<pre><code class="language-yaml">pradeep@learnk8s$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /Users/pradeep/.minikube/ca.crt
    extensions:
    - extension:
        last-update: Tue, 15 Feb 2022 12:28:03 IST
        provider: minikube.sigs.k8s.io
        version: v1.25.1
      name: cluster_info
    server: https://192.168.177.29:8443
  name: k8s
contexts:
- context:
    cluster: k8s
    extensions:
    - extension:
        last-update: Tue, 15 Feb 2022 12:28:03 IST
        provider: minikube.sigs.k8s.io
        version: v1.25.1
      name: context_info
    namespace: default
    user: k8s
  name: k8s
current-context: k8s
kind: Config
preferences: {}
users:
- name: k8s
  user:
    client-certificate: /Users/pradeep/.minikube/profiles/k8s/client.crt
    client-key: /Users/pradeep/.minikube/profiles/k8s/client.key
</code></pre>
<p>Using the standard <code>cat</code> command with full path <code>Users/pradeep/.kube/config</code>:</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat /Users/pradeep/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /Users/pradeep/.minikube/ca.crt
    extensions:
    - extension:
        last-update: Tue, 15 Feb 2022 12:28:03 IST
        provider: minikube.sigs.k8s.io
        version: v1.25.1
      name: cluster_info
    server: https://192.168.177.29:8443
  name: k8s
contexts:
- context:
    cluster: k8s
    extensions:
    - extension:
        last-update: Tue, 15 Feb 2022 12:28:03 IST
        provider: minikube.sigs.k8s.io
        version: v1.25.1
      name: context_info
    namespace: default
    user: k8s
  name: k8s
current-context: k8s
kind: Config
preferences: {}
users:
- name: k8s
  user:
    client-certificate: /Users/pradeep/.minikube/profiles/k8s/client.crt
    client-key: /Users/pradeep/.minikube/profiles/k8s/client.key
</code></pre>
<p>The three important sections from this file are:
Cluster:  <code>k8s</code>
Context: <code>k8s</code>
User: <code>k8s</code></p>
<p>When we setup Minikube, all of this work is done for us automatically.
We can either <code>get</code> or <code>set</code> or even <code>delete</code> all of these three settings with the <code>kubectl config</code> command as seen here.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config -h
Modify kubeconfig files using subcommands like &quot;kubectl config set current-context my-context&quot;

 The loading order follows these rules:

  1.  If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes
place.
  2.  If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for
your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When
a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the
last file in the list.
  3.  Otherwise, ${HOME}/.kube/config is used and no merging takes place.

Available Commands:
  current-context Display the current-context
  delete-cluster  Delete the specified cluster from the kubeconfig
  delete-context  Delete the specified context from the kubeconfig
  delete-user     Delete the specified user from the kubeconfig
  get-clusters    Display clusters defined in the kubeconfig
  get-contexts    Describe one or many contexts
  get-users       Display users defined in the kubeconfig
  rename-context  Rename a context from the kubeconfig file
  set             Set an individual value in a kubeconfig file
  set-cluster     Set a cluster entry in kubeconfig
  set-context     Set a context entry in kubeconfig
  set-credentials Set a user entry in kubeconfig
  unset           Unset an individual value in a kubeconfig file
  use-context     Set the current-context in a kubeconfig file
  view            Display merged kubeconfig settings or a specified kubeconfig file

Usage:
  kubectl config SUBCOMMAND [options]

Use &quot;kubectl &lt;command&gt; --help&quot; for more information about a given command.
Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).
</code></pre>
<p>Here is the output for all three <code>get</code> commands. While working with multiple clusters, this becomes really useful and we have to keep switching contexts. This is how we work in production environments.</p>
<p>For our little minikube environment also, when we created multiple profiles with the <code>-p</code> option (if you forget what it is, refer to the Initial setup section), minikube automatically does this context switching! </p>
<p>Here is that last line from the <code>minikube start</code> command output.</p>
<pre><code class="language-sh">üèÑ  Done! kubectl is now configured to use &quot;k8s&quot; cluster and &quot;default&quot; namespace by default 
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config get-users
NAME
k8s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config get-clusters
NAME
k8s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config get-contexts
CURRENT   NAME   CLUSTER   AUTHINFO   NAMESPACE
*         k8s    k8s       k8s        default
</code></pre>
<p>We can see that the current user is <code>k8s</code>, how can we change this to some other user, like <code>pradeep</code> user?</p>
<p>For that we need the signed certificate for that user first. We can retrieve the signed/issued certificate using this.
Pay attention to the Status section, there is the certificate.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get csr/pradeep -o yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&quot;apiVersion&quot;:&quot;certificates.k8s.io/v1&quot;,&quot;kind&quot;:&quot;CertificateSigningRequest&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;pradeep&quot;},&quot;spec&quot;:{&quot;expirationSeconds&quot;:86400,&quot;request&quot;:&quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=&quot;,&quot;signerName&quot;:&quot;kubernetes.io/kube-apiserver-client&quot;,&quot;usages&quot;:[&quot;client auth&quot;]}}
  creationTimestamp: &quot;2022-02-17T05:47:16Z&quot;
  name: pradeep
  resourceVersion: &quot;63056&quot;
  uid: 585f7670-134c-4062-a649-442054314407
spec:
  expirationSeconds: 86400
  groups:
  - system:masters
  - system:authenticated
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
  username: minikube-user
status:
  certificate: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMrRENDQWVDZ0F3SUJBZ0lSQUlHNEdtUk5tdmczZENXZHhFcTBIUEV3RFFZSktvWklodmNOQVFFTEJRQXcKRlRFVE1CRUdBMVVFQXhNS2JXbHVhV3QxWW1WRFFUQWVGdzB5TWpBeU1UY3dOVFF6TWpCYUZ3MHlNakF5TVRndwpOVFF6TWpCYU1CSXhFREFPQmdOVkJBTVRCM0J5WVdSbFpYQXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRREZ1S3BLRFBoUWhDNlpJOWlDbXpiM01YVE9KWU5mOWRzbG45T2VNenRWcEd3d1BaQWMKeExTa0tDWmhsV0JMaXYraGppTFN6cldETFlENWF0cTUwV05Lc3JGSkROOVBTZVJnMDUxNHBkcFhoRWlYUDVKMgpSZFJUOEIxS3NwUmJjZmZSYVpMTVBXK1d0MFZYZE1zOC9rYS9WeGFXRWhvTW1mUHFnL1Y2Ly9EbkdlT1pqZ2M0CllZeWNlRnhLREIvOFlZeC9rRHJHWFliK2dwZnJlMkZZZmtvTExTNGt3KzJ1clhzYitSRkxVcU5tS2dXWnN3NlgKdDBiaXpWVjhIN3A3ajcwa3pNS0d1Z2dzY1V3Vi9wR0x0ajVIRUtqMHN0N0gwODNKWkpuakpyaVZBNzMyQzlhdQpxZ1pNT0pJUkt5MWxGQ3FYaTZ4cU52NklERXFxekZLcmR5UEhBZ01CQUFHalJqQkVNQk1HQTFVZEpRUU1NQW9HCkNDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUNNQUF3SHdZRFZSMGpCQmd3Rm9BVWkvb3hwNHQyM3NiMVBjQy8KSlFVZEJYaTVna0F3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUp3R1dVR1IzRVd3bnc2cVdoUVJRTHY1aUFuOQpESmVhb3o1M25VVmhuV0tzRWdETXhzcTgvcHRVK0NJalgvR3NXS2hSaE1nU3Q0ZHR1cEswMHROVnB3cGRQTExQCm5kWHRzZmhmQmRPV1hNWDVBSmNXNTQ2Sk02eVhkQ1VXbnBmSE5LTTJ5bnRqb0FPSXp4azNhUGcxeDNuckdCZVkKQzZ6UmhRdWRYTitwdFFNK0lRa0pYVzF4TWhSeVRoNHU2bXI3c2RTTGdQZ0dsaXZGNGE2Ukx3YWttOWdoNWJtOApIUkxYZWpuTVo0TFVUOExYWmgyR0tRenRrR1QwMkgxanNDMTZpYnJpbmNFSFpxUXVvd0Z3TUMxSUdhS1l0d1BkCnMwVFQxcFZBR3N1VzRlYUpPT3B2MmtpcVVITkExLzBrb1IwYnpscFhObFZIZzVySlY1dFBtWXp4YjRjPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
  conditions:
  - lastTransitionTime: &quot;2022-02-17T05:48:20Z&quot;
    lastUpdateTime: &quot;2022-02-17T05:48:20Z&quot;
    message: This CSR was approved by kubectl certificate approve.
    reason: KubectlApprove
    status: &quot;True&quot;
    type: Approved
</code></pre>
<p>The certificate value is in Base64-encoded format under status.certificate.</p>
<p>Let us export the issued certificate from the CertificateSigningRequest.</p>
<pre><code class="language-sh">pradeep@learnk8s$ echo &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMrRENDQWVDZ0F3SUJBZ0lSQUlHNEdtUk5tdmczZENXZHhFcTBIUEV3RFFZSktvWklodmNOQVFFTEJRQXcKRlRFVE1CRUdBMVVFQXhNS2JXbHVhV3QxWW1WRFFUQWVGdzB5TWpBeU1UY3dOVFF6TWpCYUZ3MHlNakF5TVRndwpOVFF6TWpCYU1CSXhFREFPQmdOVkJBTVRCM0J5WVdSbFpYQXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRREZ1S3BLRFBoUWhDNlpJOWlDbXpiM01YVE9KWU5mOWRzbG45T2VNenRWcEd3d1BaQWMKeExTa0tDWmhsV0JMaXYraGppTFN6cldETFlENWF0cTUwV05Lc3JGSkROOVBTZVJnMDUxNHBkcFhoRWlYUDVKMgpSZFJUOEIxS3NwUmJjZmZSYVpMTVBXK1d0MFZYZE1zOC9rYS9WeGFXRWhvTW1mUHFnL1Y2Ly9EbkdlT1pqZ2M0CllZeWNlRnhLREIvOFlZeC9rRHJHWFliK2dwZnJlMkZZZmtvTExTNGt3KzJ1clhzYitSRkxVcU5tS2dXWnN3NlgKdDBiaXpWVjhIN3A3ajcwa3pNS0d1Z2dzY1V3Vi9wR0x0ajVIRUtqMHN0N0gwODNKWkpuakpyaVZBNzMyQzlhdQpxZ1pNT0pJUkt5MWxGQ3FYaTZ4cU52NklERXFxekZLcmR5UEhBZ01CQUFHalJqQkVNQk1HQTFVZEpRUU1NQW9HCkNDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUNNQUF3SHdZRFZSMGpCQmd3Rm9BVWkvb3hwNHQyM3NiMVBjQy8KSlFVZEJYaTVna0F3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUp3R1dVR1IzRVd3bnc2cVdoUVJRTHY1aUFuOQpESmVhb3o1M25VVmhuV0tzRWdETXhzcTgvcHRVK0NJalgvR3NXS2hSaE1nU3Q0ZHR1cEswMHROVnB3cGRQTExQCm5kWHRzZmhmQmRPV1hNWDVBSmNXNTQ2Sk02eVhkQ1VXbnBmSE5LTTJ5bnRqb0FPSXp4azNhUGcxeDNuckdCZVkKQzZ6UmhRdWRYTitwdFFNK0lRa0pYVzF4TWhSeVRoNHU2bXI3c2RTTGdQZ0dsaXZGNGE2Ukx3YWttOWdoNWJtOApIUkxYZWpuTVo0TFVUOExYWmgyR0tRenRrR1QwMkgxanNDMTZpYnJpbmNFSFpxUXVvd0Z3TUMxSUdhS1l0d1BkCnMwVFQxcFZBR3N1VzRlYUpPT3B2MmtpcVVITkExLzBrb1IwYnpscFhObFZIZzVySlY1dFBtWXp4YjRjPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==&quot; | base64 -d &gt; pradeep.crt
</code></pre>
<p>Just to confirm, let us view the certificate.</p>
<pre><code class="language-sh">pradeep@learnk8s$ cat pradeep.crt
-----BEGIN CERTIFICATE-----
MIIC+DCCAeCgAwIBAgIRAIG4GmRNmvg3dCWdxEq0HPEwDQYJKoZIhvcNAQELBQAw
FTETMBEGA1UEAxMKbWluaWt1YmVDQTAeFw0yMjAyMTcwNTQzMjBaFw0yMjAyMTgw
NTQzMjBaMBIxEDAOBgNVBAMTB3ByYWRlZXAwggEiMA0GCSqGSIb3DQEBAQUAA4IB
DwAwggEKAoIBAQDFuKpKDPhQhC6ZI9iCmzb3MXTOJYNf9dsln9OeMztVpGwwPZAc
xLSkKCZhlWBLiv+hjiLSzrWDLYD5atq50WNKsrFJDN9PSeRg0514pdpXhEiXP5J2
RdRT8B1KspRbcffRaZLMPW+Wt0VXdMs8/ka/VxaWEhoMmfPqg/V6//DnGeOZjgc4
YYyceFxKDB/8YYx/kDrGXYb+gpfre2FYfkoLLS4kw+2urXsb+RFLUqNmKgWZsw6X
t0bizVV8H7p7j70kzMKGuggscUwV/pGLtj5HEKj0st7H083JZJnjJriVA732C9au
qgZMOJIRKy1lFCqXi6xqNv6IDEqqzFKrdyPHAgMBAAGjRjBEMBMGA1UdJQQMMAoG
CCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUi/oxp4t23sb1PcC/
JQUdBXi5gkAwDQYJKoZIhvcNAQELBQADggEBAJwGWUGR3EWwnw6qWhQRQLv5iAn9
DJeaoz53nUVhnWKsEgDMxsq8/ptU+CIjX/GsWKhRhMgSt4dtupK00tNVpwpdPLLP
ndXtsfhfBdOWXMX5AJcW546JM6yXdCUWnpfHNKM2yntjoAOIzxk3aPg1x3nrGBeY
C6zRhQudXN+ptQM+IQkJXW1xMhRyTh4u6mr7sdSLgPgGlivF4a6RLwakm9gh5bm8
HRLXejnMZ4LUT8LXZh2GKQztkGT02H1jsC16ibrincEHZqQuowFwMC1IGaKYtwPd
s0TT1pVAGsuW4eaJOOpv2kiqUHNA1/0koR0bzlpXNlVHg5rJV5tPmYzxb4c=
-----END CERTIFICATE-----
</code></pre>
<h3 id="roles-and-rolebindings">Roles and RoleBindings</h3>
<p>With the certificate created it is time to define the Role and RoleBinding for this user <code>pradeep</code> to access Kubernetes cluster resources.</p>
<p>First let us create a role called <code>developer</code> who can create, get, list and update all pods.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create role developer --verb=create --verb=get --verb=list --verb=update --verb=delete --resource=pods
role.rbac.authorization.k8s.io/developer created
</code></pre>
<p>Next, we have to bind this role to the user, by creating a role binding and giving it a name, in this case <code>developer-pradeep</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create rolebinding developer-pradeep --role=developer --user=pradeep
rolebinding.rbac.authorization.k8s.io/developer-pradeep created
</code></pre>
<p>To verify the role:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get roles
NAME        CREATED AT
developer   2022-02-17T06:25:08Z
</code></pre>
<p>Rolebinding:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get rolebindings
NAME                ROLE             AGE
developer-pradeep   Role/developer   26s
</code></pre>
<p>The last step is to add this user <code>pradeep</code> into the kubeconfig file.</p>
<p>For this, first, you need to add new credentials:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config set-credentials pradeep --client-key=pradeep.key --client-certificate=pradeep.crt --embed-certs=true
User &quot;pradeep&quot; set.
</code></pre>
<p>Then, you need to add the context:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config set-context pradeep --cluster=k8s --user=pradeep
Context &quot;pradeep&quot; created.
</code></pre>
<p>Let us check the available contexts first.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config get-contexts
CURRENT   NAME      CLUSTER   AUTHINFO   NAMESPACE
*         k8s       k8s       k8s        default
          pradeep   k8s       pradeep
</code></pre>
<p>We can see there are two, <code>k8s</code> the old (default) one and the <code>pradeep</code> context which is new. Looking at the first column <code>CURRENT</code>, the <code>*</code> mark is present for the <code>k8s</code> context, indicating that it is the active context.</p>
<p>With this change, let us take another look at the <code>kubectl config</code> file.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /Users/pradeep/.minikube/ca.crt
    extensions:
    - extension:
        last-update: Tue, 15 Feb 2022 12:28:03 IST
        provider: minikube.sigs.k8s.io
        version: v1.25.1
      name: cluster_info
    server: https://192.168.177.29:8443
  name: k8s
contexts:
- context:
    cluster: k8s
    extensions:
    - extension:
        last-update: Tue, 15 Feb 2022 12:28:03 IST
        provider: minikube.sigs.k8s.io
        version: v1.25.1
      name: context_info
    namespace: default
    user: k8s
  name: k8s
- context:
    cluster: k8s
    user: pradeep
  name: pradeep
current-context: pradeep
kind: Config
preferences: {}
users:
- name: k8s
  user:
    client-certificate: /Users/pradeep/.minikube/profiles/k8s/client.crt
    client-key: /Users/pradeep/.minikube/profiles/k8s/client.key
- name: pradeep
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
</code></pre>
<p>We can see that , there is still one cluster, but two contexts and two users.</p>
<p>It is time to test all the work done in the last half an hour by switching the context.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config use-context pradeep
Switched to context &quot;pradeep&quot;.
</code></pre>
<p>Verify it one more way.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config current-context
pradeep
</code></pre>
<p>Now that we are sure of the current-context, let us get the pods as this new user <code>pradeep</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods
NAME                        READY   STATUS             RESTARTS        AGE
kodekloud-8477b7849-blzrs   1/1     Running            0               46h
kodekloud-8477b7849-jkfmr   1/1     Running            0               46h
kodekloud-8477b7849-m65m8   1/1     Running            0               46h
kodekloud-8477b7849-rgbmn   1/1     Running            0               46h
kodekloud-change-color      1/1     Running            0               43h
kodekloud-cm                1/1     Running            0               39h
kodekloud-env-color         1/1     Running            0               40h
kodekloud-env-color-2       1/1     Running            0               40h
kodekloud-secret            1/1     Running            0               29h
multi-containers            1/2     NotReady           0               28h
pod-with-init-container     1/1     Running            14 (12m ago)    28h
</code></pre>
<p>Yay! :clap: We could get the pods as user <code>pradeep</code>.</p>
<p>Let us try to get the nodes this time.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get nodes
Error from server (Forbidden): nodes is forbidden: User &quot;pradeep&quot; cannot list resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope
</code></pre>
<p>Try accessing deployments.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployments
Error from server (Forbidden): deployments.apps is forbidden: User &quot;pradeep&quot; cannot list resource &quot;deployments&quot; in API group &quot;apps&quot; in the namespace &quot;default&quot;
</code></pre>
<p>What happened? It is Forbidden. But, this is expected, right?!  We only gave permission for Pods. Isn't it?</p>
<p>We can't verify that (if we don't remember what we have given for this user/role) from the current context. </p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe roles developer
Error from server (Forbidden): roles.rbac.authorization.k8s.io &quot;developer&quot; is forbidden: User &quot;pradeep&quot; cannot get resource &quot;roles&quot; in API group &quot;rbac.authorization.k8s.io&quot; in the namespace &quot;default&quot;
</code></pre>
<p>So, we have to switch the context back to <code>k8s</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config use-context k8s
Switched to context &quot;k8s&quot;.
</code></pre>
<p>Describe the <code>developer</code> role and check what all Resources this role has access to.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe roles developer
Name:         developer
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [create get list update delete]
</code></pre>
<p>This confirms that, any user who is given this role of <code>developer</code> can only work with <code>pods</code>.</p>
<p>Here is the same in YAML form.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ kubectl get roles developer -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: &quot;2022-02-17T06:25:08Z&quot;
  name: developer
  namespace: default
  resourceVersion: &quot;64839&quot;
  uid: 9eae4036-0858-4c09-8c06-7532d74ca1aa
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - pods
  verbs:
  - create
  - get
  - list
  - update
  - delete

</code></pre>
<p>How can we give access to deployments for this developer? We just need to add additional resource of deployments.</p>
<p>Let us edit the <code>developer</code> role.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl edit role developer
role.rbac.authorization.k8s.io/developer edited
</code></pre>
<p>Describe the role after editing it, just to confirm we have additional access to deployments.
We have modified few Verbs as well (removed some access!)</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe role developer
Name:         developer
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
PolicyRule:
  Resources    Non-Resource URLs  Resource Names  Verbs
  ---------    -----------------  --------------  -----
  deployments  []                 []              [get list]
  pods         []                 []              [get list]
</code></pre>
<p>This is modified YAML version of the <code>developer</code> role.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ kubectl get role developer -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: &quot;2022-02-17T06:25:08Z&quot;
  name: developer
  namespace: default
  resourceVersion: &quot;66960&quot;
  uid: 9eae4036-0858-4c09-8c06-7532d74ca1aa
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - deployments
  - pods
  verbs:
  - get
  - list
</code></pre>
<p>Now switch back to the <code>pradeep</code> context and verify if you can get the nodes.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config use-context pradeep
Switched to context &quot;pradeep&quot;.
</code></pre>
<p>Let us see if <code>pradeep</code> can get deployments.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployments
Error from server (Forbidden): deployments.apps is forbidden: User &quot;pradeep&quot; cannot list resource &quot;deployments&quot; in API group &quot;apps&quot; in the namespace &quot;default&quot;
</code></pre>
<p>Still Forbidden.  It is do with the <code>apiGroups</code> settings. Currently we did not specify anything, it was left blank.</p>
<p>We can edit again, change the <code>apiGroups</code> to any with the <code>*</code>. Though in production environment, we might want to be more specific and only configured limited groups as needed.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config use-context k8s
Switched to context &quot;k8s&quot;.
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl edit role developer
role.rbac.authorization.k8s.io/developer edited
</code></pre>
<p>Look at the <code>apiGroups</code> value of <code>*</code>.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ kubectl get role developer -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: &quot;2022-02-17T06:25:08Z&quot;
  name: developer
  namespace: default
  resourceVersion: &quot;67224&quot;
  uid: 9eae4036-0858-4c09-8c06-7532d74ca1aa
rules:
- apiGroups:
  - '*'
  resources:
  - deployments
  - pods
  verbs:
  - get
  - list
</code></pre>
<p>To test if this is working or not for <code>pradeep</code> user, there is another away, without switching context. We can make use of the <code>--as</code> option (impersonation).</p>
<p>Before modifying the <code>apiGroup</code> setting, it was like this, for the same test.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployment --as pradeep
Error from server (Forbidden): deployments.apps is forbidden: User &quot;pradeep&quot; cannot list resource &quot;deployments&quot; in API group &quot;apps&quot; in the namespace &quot;default&quot;
</code></pre>
<p>Now that we modified, </p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployment --as pradeep
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kodekloud   4/4     4            4           2d
</code></pre>
<p>Though this test confirms that <code>pradeep</code> can get the deployments, let us switch context and verify one more time.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config use-context pradeep
Switched to context &quot;pradeep&quot;.
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get deployments
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kodekloud   4/4     4            4           2d
</code></pre>
<p>:tada: Congratulations!</p>
<h3 id="cluster-roles-and-cluster-role-bindings">Cluster Roles and Cluster Role Bindings</h3>
<p>If we notice, user <code>pradeep</code> still can't get the nodes information.  It is becuase, he still does not have access to resources at the cluster scope.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get nodes
Error from server (Forbidden): nodes is forbidden: User &quot;pradeep&quot; cannot list resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope
</code></pre>
<p>To get this access, we need to create a Cluste role and Cluste role binding.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config use-context k8s
Switched to context &quot;k8s&quot;.
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create clusterrole pradeep-cluster --verb=get,list,watch, --resource=nodes
Warning: '' is not a standard resource verb
clusterrole.rbac.authorization.k8s.io/pradeep-cluster created
</code></pre>
<p>Get all available clusterroles. There are many but we can see our newly created clusterrole <code>pradeep-cluster</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get clusterrole
NAME                                                                   CREATED AT
admin                                                                  2022-02-15T06:57:58Z
cluster-admin                                                          2022-02-15T06:57:58Z
edit                                                                   2022-02-15T06:57:58Z
kindnet                                                                2022-02-15T06:58:03Z
kubeadm:get-nodes                                                      2022-02-15T06:58:01Z
pradeep-cluster                                                        2022-02-17T07:33:43Z
system:aggregate-to-admin                                              2022-02-15T06:57:58Z
system:aggregate-to-edit                                               2022-02-15T06:57:58Z
system:aggregate-to-view                                               2022-02-15T06:57:58Z
system:auth-delegator                                                  2022-02-15T06:57:58Z
system:basic-user                                                      2022-02-15T06:57:58Z
system:certificates.k8s.io:certificatesigningrequests:nodeclient       2022-02-15T06:57:58Z
system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   2022-02-15T06:57:58Z
system:certificates.k8s.io:kube-apiserver-client-approver              2022-02-15T06:57:58Z
system:certificates.k8s.io:kube-apiserver-client-kubelet-approver      2022-02-15T06:57:58Z
system:certificates.k8s.io:kubelet-serving-approver                    2022-02-15T06:57:58Z
system:certificates.k8s.io:legacy-unknown-approver                     2022-02-15T06:57:58Z
system:controller:attachdetach-controller                              2022-02-15T06:57:58Z
system:controller:certificate-controller                               2022-02-15T06:57:58Z
system:controller:clusterrole-aggregation-controller                   2022-02-15T06:57:58Z
system:controller:cronjob-controller                                   2022-02-15T06:57:58Z
system:controller:daemon-set-controller                                2022-02-15T06:57:58Z
system:controller:deployment-controller                                2022-02-15T06:57:58Z
system:controller:disruption-controller                                2022-02-15T06:57:58Z
system:controller:endpoint-controller                                  2022-02-15T06:57:58Z
system:controller:endpointslice-controller                             2022-02-15T06:57:58Z
system:controller:endpointslicemirroring-controller                    2022-02-15T06:57:58Z
system:controller:ephemeral-volume-controller                          2022-02-15T06:57:58Z
system:controller:expand-controller                                    2022-02-15T06:57:58Z
system:controller:generic-garbage-collector                            2022-02-15T06:57:58Z
system:controller:horizontal-pod-autoscaler                            2022-02-15T06:57:58Z
system:controller:job-controller                                       2022-02-15T06:57:58Z
system:controller:namespace-controller                                 2022-02-15T06:57:58Z
system:controller:node-controller                                      2022-02-15T06:57:58Z
system:controller:persistent-volume-binder                             2022-02-15T06:57:58Z
system:controller:pod-garbage-collector                                2022-02-15T06:57:58Z
system:controller:pv-protection-controller                             2022-02-15T06:57:58Z
system:controller:pvc-protection-controller                            2022-02-15T06:57:58Z
system:controller:replicaset-controller                                2022-02-15T06:57:58Z
system:controller:replication-controller                               2022-02-15T06:57:58Z
system:controller:resourcequota-controller                             2022-02-15T06:57:58Z
system:controller:root-ca-cert-publisher                               2022-02-15T06:57:58Z
system:controller:route-controller                                     2022-02-15T06:57:58Z
system:controller:service-account-controller                           2022-02-15T06:57:58Z
system:controller:service-controller                                   2022-02-15T06:57:58Z
system:controller:statefulset-controller                               2022-02-15T06:57:58Z
system:controller:ttl-after-finished-controller                        2022-02-15T06:57:58Z
system:controller:ttl-controller                                       2022-02-15T06:57:58Z
system:coredns                                                         2022-02-15T06:58:01Z
system:discovery                                                       2022-02-15T06:57:58Z
system:heapster                                                        2022-02-15T06:57:58Z
system:kube-aggregator                                                 2022-02-15T06:57:58Z
system:kube-controller-manager                                         2022-02-15T06:57:58Z
system:kube-dns                                                        2022-02-15T06:57:58Z
system:kube-scheduler                                                  2022-02-15T06:57:58Z
system:kubelet-api-admin                                               2022-02-15T06:57:58Z
system:monitoring                                                      2022-02-15T06:57:58Z
system:node                                                            2022-02-15T06:57:58Z
system:node-bootstrapper                                               2022-02-15T06:57:58Z
system:node-problem-detector                                           2022-02-15T06:57:58Z
system:node-proxier                                                    2022-02-15T06:57:58Z
system:persistent-volume-provisioner                                   2022-02-15T06:57:58Z
system:public-info-viewer                                              2022-02-15T06:57:58Z
system:service-account-issuer-discovery                                2022-02-15T06:57:58Z
system:volume-scheduler                                                2022-02-15T06:57:58Z
view                                                                   2022-02-15T06:57:58Z
</code></pre>
<p>Create a clusterrolebinding called <code>pradeep-cluster-binding</code> and bind the user <code>pradeep</code> and clusterrole <code>pradeep-cluster</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create clusterrolebinding pradeep-cluster-binding --clusterrole=pradeep-cluster --user=pradeep
clusterrolebinding.rbac.authorization.k8s.io/pradeep-cluster-binding created
</code></pre>
<p>Get all available clusterrolebindings.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get clusterrolebindings.rbac.authorization.k8s.io
NAME                                                   ROLE                                                                               AGE
cluster-admin                                          ClusterRole/cluster-admin                                                          2d
kindnet                                                ClusterRole/kindnet                                                                2d
kubeadm:get-nodes                                      ClusterRole/kubeadm:get-nodes                                                      2d
kubeadm:kubelet-bootstrap                              ClusterRole/system:node-bootstrapper                                               2d
kubeadm:node-autoapprove-bootstrap                     ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient       2d
kubeadm:node-autoapprove-certificate-rotation          ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   2d
kubeadm:node-proxier                                   ClusterRole/system:node-proxier                                                    2d
minikube-rbac                                          ClusterRole/cluster-admin                                                          2d
pradeep-cluster-binding                                ClusterRole/pradeep-cluster                                                        5s
storage-provisioner                                    ClusterRole/system:persistent-volume-provisioner                                   2d
system:basic-user                                      ClusterRole/system:basic-user                                                      2d
system:controller:attachdetach-controller              ClusterRole/system:controller:attachdetach-controller                              2d
system:controller:certificate-controller               ClusterRole/system:controller:certificate-controller                               2d
system:controller:clusterrole-aggregation-controller   ClusterRole/system:controller:clusterrole-aggregation-controller                   2d
system:controller:cronjob-controller                   ClusterRole/system:controller:cronjob-controller                                   2d
system:controller:daemon-set-controller                ClusterRole/system:controller:daemon-set-controller                                2d
system:controller:deployment-controller                ClusterRole/system:controller:deployment-controller                                2d
system:controller:disruption-controller                ClusterRole/system:controller:disruption-controller                                2d
system:controller:endpoint-controller                  ClusterRole/system:controller:endpoint-controller                                  2d
system:controller:endpointslice-controller             ClusterRole/system:controller:endpointslice-controller                             2d
system:controller:endpointslicemirroring-controller    ClusterRole/system:controller:endpointslicemirroring-controller                    2d
system:controller:ephemeral-volume-controller          ClusterRole/system:controller:ephemeral-volume-controller                          2d
system:controller:expand-controller                    ClusterRole/system:controller:expand-controller                                    2d
system:controller:generic-garbage-collector            ClusterRole/system:controller:generic-garbage-collector                            2d
system:controller:horizontal-pod-autoscaler            ClusterRole/system:controller:horizontal-pod-autoscaler                            2d
system:controller:job-controller                       ClusterRole/system:controller:job-controller                                       2d
system:controller:namespace-controller                 ClusterRole/system:controller:namespace-controller                                 2d
system:controller:node-controller                      ClusterRole/system:controller:node-controller                                      2d
system:controller:persistent-volume-binder             ClusterRole/system:controller:persistent-volume-binder                             2d
system:controller:pod-garbage-collector                ClusterRole/system:controller:pod-garbage-collector                                2d
system:controller:pv-protection-controller             ClusterRole/system:controller:pv-protection-controller                             2d
system:controller:pvc-protection-controller            ClusterRole/system:controller:pvc-protection-controller                            2d
system:controller:replicaset-controller                ClusterRole/system:controller:replicaset-controller                                2d
system:controller:replication-controller               ClusterRole/system:controller:replication-controller                               2d
system:controller:resourcequota-controller             ClusterRole/system:controller:resourcequota-controller                             2d
system:controller:root-ca-cert-publisher               ClusterRole/system:controller:root-ca-cert-publisher                               2d
system:controller:route-controller                     ClusterRole/system:controller:route-controller                                     2d
system:controller:service-account-controller           ClusterRole/system:controller:service-account-controller                           2d
system:controller:service-controller                   ClusterRole/system:controller:service-controller                                   2d
system:controller:statefulset-controller               ClusterRole/system:controller:statefulset-controller                               2d
system:controller:ttl-after-finished-controller        ClusterRole/system:controller:ttl-after-finished-controller                        2d
system:controller:ttl-controller                       ClusterRole/system:controller:ttl-controller                                       2d
system:coredns                                         ClusterRole/system:coredns                                                         2d
system:discovery                                       ClusterRole/system:discovery                                                       2d
system:kube-controller-manager                         ClusterRole/system:kube-controller-manager                                         2d
system:kube-dns                                        ClusterRole/system:kube-dns                                                        2d
system:kube-scheduler                                  ClusterRole/system:kube-scheduler                                                  2d
system:monitoring                                      ClusterRole/system:monitoring                                                      2d
system:node                                            ClusterRole/system:node                                                            2d
system:node-proxier                                    ClusterRole/system:node-proxier                                                    2d
system:public-info-viewer                              ClusterRole/system:public-info-viewer                                              2d
system:service-account-issuer-discovery                ClusterRole/system:service-account-issuer-discovery                                2d
system:volume-scheduler                                ClusterRole/system:volume-scheduler                                                2d
</code></pre>
<p>Again, there are many pre-defined but our newly created clusterrolebinding <code>pradeep-cluster-binding</code> is shown.</p>
<p>With that confirmation, let us verify if user <code>pradeep</code> can get the nodes without context-switching.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get nodes --as pradeep
NAME      STATUS   ROLES                  AGE   VERSION
k8s       Ready    control-plane,master   2d    v1.23.1
k8s-m02   Ready    &lt;none&gt;                 2d    v1.23.1
</code></pre>
<p>Before switching context, let us describe these clusterrole and clusterrolebindings.</p>
<p>ClusterRole:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe clusterrole pradeep-cluster
Name:         pradeep-cluster
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  nodes      []                 []              [get list watch ]
</code></pre>
<p>ClusterRoleBinding:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe clusterrolebindings pradeep-cluster-binding
Name:         pradeep-cluster-binding
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  pradeep-cluster
Subjects:
  Kind  Name     Namespace
  ----  ----     ---------
  User  pradeep
</code></pre>
<p>Switch context now.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config use-context pradeep
Switched to context &quot;pradeep&quot;.
</code></pre>
<p>Finally, verify user <code>pradeep</code> can get the nodes.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get nodes
NAME      STATUS   ROLES                  AGE   VERSION
k8s       Ready    control-plane,master   2d    v1.23.1
k8s-m02   Ready    &lt;none&gt;                 2d    v1.23.1
</code></pre>
<p>Now, we have accomplished our simple goal w.r.t authorization in Kubernetes.</p>
<h3 id="kubectl-auth">Kubectl Auth</h3>
<p>To Inspect authorization, we can make use of <code>kubectl auth</code> command. It will show an :+1: or :no_entry_sign:.
Here is the related help and examples.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl auth can-i -h
Check whether an action is allowed.

 VERB is a logical Kubernetes API verb like 'get', 'list', 'watch', 'delete', etc. TYPE is a Kubernetes resource.
Shortcuts and groups will be resolved. NONRESOURCEURL is a partial URL that starts with &quot;/&quot;. NAME is the name of a
particular Kubernetes resource. This command pairs nicely with impersonation. See --as global flag.

Examples:
  # Check to see if I can create pods in any namespace
  kubectl auth can-i create pods --all-namespaces

  # Check to see if I can list deployments in my current namespace
  kubectl auth can-i list deployments.apps

  # Check to see if I can do everything in my current namespace (&quot;*&quot; means all)
  kubectl auth can-i '*' '*'

  # Check to see if I can get the job named &quot;bar&quot; in namespace &quot;foo&quot;
  kubectl auth can-i list jobs.batch/bar -n foo

  # Check to see if I can read pod logs
  kubectl auth can-i get pods --subresource=log

  # Check to see if I can access the URL /logs/
  kubectl auth can-i get /logs/

  # List all allowed actions in namespace &quot;foo&quot;
  kubectl auth can-i --list --namespace=foo

Options:
  -A, --all-namespaces=false: If true, check the specified action in all namespaces.
      --list=false: If true, prints all allowed actions.
      --no-headers=false: If true, prints allowed actions without headers
  -q, --quiet=false: If true, suppress output and just return the exit code.
      --subresource='': SubResource such as pod/log or deployment/scale

Usage:
  kubectl auth can-i VERB [TYPE | TYPE/NAME | NONRESOURCEURL] [options]

Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).
</code></pre>
<p>For example, from the <code>pradeep</code> context,</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config get-contexts
CURRENT   NAME      CLUSTER   AUTHINFO   NAMESPACE
          k8s       k8s       k8s        default
*         pradeep   k8s       pradeep
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl auth can-i '*' '*'
no
</code></pre>
<p>From the <code>k8s</code> context,</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl config get-contexts
CURRENT   NAME      CLUSTER   AUTHINFO   NAMESPACE
*         k8s       k8s       k8s        default
          pradeep   k8s       pradeep
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl auth can-i '*' '*'
yes
</code></pre>
<h3 id="security-context">Security Context</h3>
<p>To understand security context, first let us try to create a Pod without any security context and  check few things like, <code>whoami</code>, <code>id</code>, and <code>ps</code> command outputs to verify the current user inside the Pod.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat no-security-context-demo.yaml
apiVersion: v1
kind: Pod
metadata:
  name: no-security-context-demo
spec:

  containers:
  - name: no-security-context-demo
    image: busybox
    command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f no-security-context-demo.yaml
pod/no-security-context-demo created
</code></pre>
<p>Login to the Pod, and launch the <code>shell</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it no-security-context-demo -- sh
/ # whoami
root
/ # id
uid=0(root) gid=0(root) groups=10(wheel)
/ # ps
PID   USER     TIME  COMMAND
    1 root      0:00 sleep 1h
    9 root      0:00 sh
   18 root      0:00 ps
/ # exit 0
</code></pre>
<p>From all three command outputs, it is clear that it is the <code>root</code> user with id <code>0</code>.</p>
<p>Now, we can make use of a security context to define privilege and access control settings for a Pod or Container. For example, based on user ID (UID) and group ID (GID).</p>
<p>Let us modify the Pod defintion to include <code>securityContext</code> and create another Pod.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat security-context-demo.yaml
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 1000

  containers:
  - name: security-context-demo
    image: busybox
    command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f security-context-demo.yaml
pod/security-context-demo created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods | grep security
no-security-context-demo    1/1     Running            0                 9m24s
security-context-demo       1/1     Running            0                 31s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it security-context-demo -- sh
/ $ whoami
whoami: unknown uid 1000
/ $ id
uid=1000 gid=0(root)
/ $ ps
PID   USER     TIME  COMMAND
    1 1000      0:00 sleep 1h
    7 1000      0:00 sh
   16 1000      0:00 ps
/ $ cat /etc/passwd
root:x:0:0:root:/root:/bin/sh
daemon:x:1:1:daemon:/usr/sbin:/bin/false
bin:x:2:2:bin:/bin:/bin/false
sys:x:3:3:sys:/dev:/bin/false
sync:x:4:100:sync:/bin:/bin/sync
mail:x:8:8:mail:/var/spool/mail:/bin/false
www-data:x:33:33:www-data:/var/www:/bin/false
operator:x:37:37:Operator:/var:/bin/false
nobody:x:65534:65534:nobody:/home:/bin/false
/ $
</code></pre>
<p>We can see the difference. Now, inside the Pod, its all the user with ID <code>1000</code> as defined in our security context.</p>
<p>As we are still inside the <code>security-context-demo</code> pod, let us check few things like <code>date</code> and try to modify the <code>date</code> value.</p>
<pre><code class="language-sh">/ $ date
Sat Feb 19 01:06:08 UTC 2022
/ $ date -s 202206020937
date: can't set date: Operation not permitted
Thu Jun  2 09:37:00 UTC 2022
/ $ date
Sat Feb 19 01:08:11 UTC 2022
/ $
</code></pre>
<p>It says that <code>date: can't set date: Operation not permitted</code> and date did not change.</p>
<p>Let us create a new Pod definition to run the container as <code>root</code> user with <code>SYS_TIME</code> capability.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat security-context-cap-demo.yaml
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo-cap
spec:
  containers:
  - name: security-context-demo-cap
    image: busybox
    command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]
    securityContext:
      runAsUser: 0
      capabilities:
        add: [&quot;SYS_TIME&quot;]
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f security-context-cap-demo.yaml
pod/security-context-demo-cap created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it security-context-demo-cap -- sh
/ # id
uid=0(root) gid=0(root) groups=10(wheel)
/ # whoami
root
/ # ps
PID   USER     TIME  COMMAND
    1 root      0:00 sleep 1h
    9 root      0:00 sh
   18 root      0:00 ps
/ # date -s 202206020937
Thu Jun  2 09:37:00 UTC 2022
/ # date
Sat Feb 19 13:29:05 UTC 2022
</code></pre>
<p>This time, it is the root user and setting date worked fine (partially!). Important difference to observe is that there is no <code>Operation not permitted</code> warning. It seems to took the new date, but issuing a date again, still shows the current date. I haven't explored this further yet. </p>
<h2 id="storage">Storage</h2>
<h3 id="volumes">Volumes</h3>
<h4 id="emptydir">emptyDir</h4>
<p>There are many volume types. 
An <code>emptyDir</code> volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. 
When a Pod is removed from a node for any reason, the data in the <code>emptyDir</code> is deleted permanently.</p>
<p>Here is an example for creating a volume of type <code>emptyDir</code> and use it in a Pod.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ more pod-with-emptydir-volume.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-emptydir-volume
spec:
  volumes:
  - name: logs-volume
    emptyDir: {}
  containers:
  - image: nginx
    name: pod-with-emptydir-volume
    volumeMounts:
    - mountPath: /var/logs
      name: logs-volume
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-with-emptydir-volume.yaml
pod/pod-with-emptydir-volume created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods | grep volume
pod-with-emptydir-volume    1/1     Running            0               9s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it pod-with-emptydir-volume -- /bin/sh
# ls
bin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
# cd /var/logs
# ls
# pwd
/var/logs
# touch testing-volumes.txt
# ls
testing-volumes.txt
# exit 0
</code></pre>
<h4 id="hostpath">HostPath</h4>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-with-hostpath-volume.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-hostpath-volume
spec:
  containers:
  - image: nginx
    name: pod-with-hostpath-volume
    volumeMounts:
    - mountPath: /usr/share/nginx/html
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data-for-pod-with-hostpath-volume
      # this field is optional
      type: Directory
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ls
$ sudo mkdir /data-for-pod-with-hostpath-volume
$ sudo vi /data-for-pod-with-hostpath-volume/index.html
$ sudo cat /data-for-pod-with-hostpath-volume/index.html
testing Pod with hostPath volume mount!
$ exit
logout
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-with-hostpath-volume.yaml
pod/pod-with-hostpath-volume created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods | grep volume
pod-with-emptydir-volume    1/1     Running             0                 13m
pod-with-hostpath-volume    0/1     ContainerCreating   0                 53s
</code></pre>
<p>Even after 50+ seconds, the <code>pod-with-hostpath-volume</code> seems to be not yet Running.
Let us describe it to understand the reason. Pay attention to Mounts section under Containers and Volumes section. Finally look at the events.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods pod-with-hostpath-volume
Name:         pod-with-hostpath-volume
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Sun, 20 Feb 2022 18:43:42 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Pending
IP:
IPs:          &lt;none&gt;
Containers:
  pod-with-hostpath-volume:
    Container ID:
    Image:          nginx
    Image ID:
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /usr/share/nginx/html from test-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f7lsg (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  test-volume:
    Type:          HostPath (bare host directory volume)
    Path:          /data-for-pod-with-hostpath-volume
    HostPathType:  Directory
  kube-api-access-f7lsg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                From               Message
  ----     ------       ----               ----               -------
  Normal   Scheduled    61s                default-scheduler  Successfully assigned default/pod-with-hostpath-volume to k8s-m02
  Warning  FailedMount  28s (x7 over 60s)  kubelet            MountVolume.SetUp failed for volume &quot;test-volume&quot; : hostPath type check failed: /data-for-pod-with-hostpath-volume is not a directory
</code></pre>
<p>From the events, we can see that the default-scheduler tried to create this pod on the node <code>k8s-m02</code> but MountVolume.SetUp failed for volume <code>test-volume</code> : hostPath type check failed: <code>/data-for-pod-with-hostpath-volume</code> is not a directory.</p>
<p>This is expected, right. A moment ago, we created a directory with this name on the <code>k8s</code> node.</p>
<p>To fix this, we can manually schedule this pod on the <code>k8s</code> node with the <code>nodeName</code> spec.
We did something like this already. Didn't we?</p>
<p>First, let us delete the pod which is still in creatingContainer stage.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods | grep volume
pod-with-emptydir-volume    1/1     Running             0                21m
pod-with-hostpath-volume    0/1     ContainerCreating   0                9m32s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete pods pod-with-hostpath-volume
pod &quot;pod-with-hostpath-volume&quot; deleted
</code></pre>
<p>Modify the YAML definition file like this. Only thing different (from the previous definition) here  is the <code>nodeName: k8s</code> line.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-with-hostpath-volume.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-hostpath-volume
spec:
  nodeName: k8s
  containers:
  - image: nginx
    name: pod-with-hostpath-volume
    volumeMounts:
    - mountPath: /usr/share/nginx/html
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data-for-pod-with-hostpath-volume
      # this field is optional
      type: Directory
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods | grep volume
pod-with-emptydir-volume    1/1     Running            0               26m
pod-with-hostpath-volume    1/1     Running            0               42s
</code></pre>
<p>This time it is running fine.
If we describe this Pod,</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods pod-with-hostpath-volume
Name:         pod-with-hostpath-volume
Namespace:    default
Priority:     0
Node:         k8s/192.168.177.29
Start Time:   Sun, 20 Feb 2022 18:56:51 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.0.18
IPs:
  IP:  10.244.0.18
Containers:
  pod-with-hostpath-volume:
    Container ID:   docker://f1e79dd787c5327766bbdde7b35cab8314de09e1c406fe274ee3f7c364cc5845
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Sun, 20 Feb 2022 18:57:25 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /usr/share/nginx/html from test-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7fv4m (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  test-volume:
    Type:          HostPath (bare host directory volume)
    Path:          /data-for-pod-with-hostpath-volume
    HostPathType:  Directory
  kube-api-access-7fv4m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulling  50s   kubelet  Pulling image &quot;nginx&quot;
  Normal  Pulled   19s   kubelet  Successfully pulled image &quot;nginx&quot; in 30.708250752s
  Normal  Created  19s   kubelet  Created container pod-with-hostpath-volume
  Normal  Started  18s   kubelet  Started container pod-with-hostpath-volume
</code></pre>
<p>Get the IP assigned to the Pod, either from the previous command or with the <code>-o wide</code> option.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide | grep volume
pod-with-emptydir-volume    1/1     Running            0                38m     10.244.1.39   k8s-m02   &lt;none&gt;           &lt;none&gt;
pod-with-hostpath-volume    1/1     Running            0                13m     10.244.0.18   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Let us connect to this Pod on the IP address assigned <code>10.244.0.18</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ curl 10.244.0.18
testing Pod with hostPath volume mount!
</code></pre>
<p>The test is working. The <code>nginx</code> container in the <code>pod-with-hostpath-volume</code> is readigng the data from the host directory.</p>
<h4 id="config-maps">Config Maps</h4>
<p>A ConfigMap provides a way to inject configuration data into pods. The data stored in a ConfigMap can be referenced in a volume of type configMap and then consumed by containerized applications running in a pod.</p>
<p>https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/</p>
<p>Let us try this example as it is in our Minikube setup.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat redis-pod-configmap.yaml
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis:5.0.4
    command:
      - redis-server
      - &quot;/redis-master/redis.conf&quot;
    env:
    - name: MASTER
      value: &quot;true&quot;
    ports:
    - containerPort: 6379
    resources:
      limits:
        cpu: &quot;0.1&quot;
    volumeMounts:
    - mountPath: /redis-master-data
      name: data
    - mountPath: /redis-master
      name: config
  volumes:
    - name: data
      emptyDir: {}
    - name: config
      configMap:
        name: example-redis-config
        items:
        - key: redis-config
          path: redis.conf

</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ cat &lt;&lt;EOF &gt;./example-redis-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-redis-config
data:
  redis-config: &quot;&quot;
EOF
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl apply -f example-redis-config.yaml
configmap/example-redis-config created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl apply -f redis-pod-configmap.yaml
pod/redis created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pod/redis configmap/example-redis-config -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
pod/redis   1/1     Running   0          53s   10.244.1.40   k8s-m02   &lt;none&gt;           &lt;none&gt;

NAME                             DATA   AGE
configmap/example-redis-config   1      64s
</code></pre>
<p>Describe the configmap.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe configmap/example-redis-config
Name:         example-redis-config
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
redis-config:
----


BinaryData
====

Events:  &lt;none&gt;
</code></pre>
<p>You should see an empty <code>redis-config</code> key.</p>
<p>Use <code>kubectl exec</code> to enter the pod and run the <code>redis-cli</code> tool to check the current configuration:</p>
<p>Check <code>maxmemory</code> and <code>maxmemory-policy</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it redis -- redis-cli
127.0.0.1:6379&gt; CONFIG GET maxmemory
1) &quot;maxmemory&quot;
2) &quot;0&quot;
127.0.0.1:6379&gt; CONFIG GET maxmemory-policy
1) &quot;maxmemory-policy&quot;
2) &quot;noeviction&quot;
127.0.0.1:6379&gt;
</code></pre>
<p>Now let's add some configuration values to the <code>example-redis-config</code> ConfigMap:</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat example-redis-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-redis-config
data:
  redis-config: |
    maxmemory 2mb
    maxmemory-policy allkeys-lru
</code></pre>
<p>Apply the updated configmap.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl apply -f example-redis-config.yaml
configmap/example-redis-config configured
</code></pre>
<p>Confirm configmap is updated.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe configmap/example-redis-config
Name:         example-redis-config
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
redis-config:
----
maxmemory 2mb
maxmemory-policy allkeys-lru


BinaryData
====

Events:  &lt;none&gt;
</code></pre>
<p>Check the Redis Pod again using <code>redis-cli</code> via <code>kubectl exec</code> to see if the configuration was applied:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it redis -- redis-cli
127.0.0.1:6379&gt; CONFIG GET maxmemory
1) &quot;maxmemory&quot;
2) &quot;0&quot;
127.0.0.1:6379&gt; CONFIG GET maxmemory-policy
1) &quot;maxmemory-policy&quot;
2) &quot;noeviction&quot;
127.0.0.1:6379&gt; exit
</code></pre>
<p>Are the changes reflecting now? Nope! Not yet. </p>
<p>The configuration values have not changed because the Pod needs to be restarted to grab updated values from associated ConfigMaps. Let's delete and recreate the Pod:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete pod redis
pod &quot;redis&quot; deleted
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl apply -f redis-pod-configmap.yaml
pod/redis created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods | grep redis
redis                       1/1     Running            0                 10s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it redis -- redis-cli
127.0.0.1:6379&gt; CONFIG GET maxmemory
1) &quot;maxmemory&quot;
2) &quot;2097152&quot;
127.0.0.1:6379&gt; CONFIG GET maxmemory-policy
1) &quot;maxmemory-policy&quot;
2) &quot;allkeys-lru&quot;
127.0.0.1:6379&gt; exit
</code></pre>
<p>This time we can seen updated (from ConfigMap) values inside the <code>redis</code> container.</p>
<h4 id="secrets_1">Secrets</h4>
<p>We are re-visiting secrets again, but this time not as an Envrionment variable but as a volume Mount.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create secret generic my-secret --from-literal=user=pradeep --from-literal=password=topsecret
secret/my-secret created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe secrets my-secret
Name:         my-secret
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Type:  Opaque

Data
====
password:  9 bytes
user:      7 bytes
</code></pre>
<p>Now, let us consume this secret as a volume in a Pod.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-with-secret-volume.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-secret-volume
spec:
  containers:
  - name: pod-with-secret-volume
    image: redis
    volumeMounts:
    - name: foo
      mountPath: &quot;/etc/foo&quot;
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: my-secret
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-with-secret-volume.yaml
pod/pod-with-secret-volume created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods | grep volume
pod-with-emptydir-volume    1/1     Running            0                 81m
pod-with-hostpath-volume    1/1     Running            0                 55m
pod-with-secret-volume      1/1     Running            0                 15s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pod pod-with-secret-volume
Name:         pod-with-secret-volume
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Sun, 20 Feb 2022 19:52:10 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.42
IPs:
  IP:  10.244.1.42
Containers:
  pod-with-secret-volume:
    Container ID:   docker://a21403023c1bb4d5348f1aa1eee282072394e3c6785a8f0ef1203f67355f357b
    Image:          redis
    Image ID:       docker-pullable://redis@sha256:0d9c9aed1eb385336db0bc9b976b6b49774aee3d2b9c2788a0d0d9e239986cb3
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Sun, 20 Feb 2022 19:52:24 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /etc/foo from foo (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wlmjs (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  foo:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  my-secret
    Optional:    false
  kube-api-access-wlmjs:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  93s   default-scheduler  Successfully assigned default/pod-with-secret-volume to k8s-m02
  Normal  Pulling    91s   kubelet            Pulling image &quot;redis&quot;
  Normal  Pulled     80s   kubelet            Successfully pulled image &quot;redis&quot; in 11.292924683s
  Normal  Created    80s   kubelet            Created container pod-with-secret-volume
  Normal  Started    79s   kubelet            Started container pod-with-secret-volume
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it pod-with-secret-volume -- sh
# ls /etc/foo
password  user
# ls -l /etc/foo
total 0
lrwxrwxrwx 1 root root 15 Feb 20 14:22 password -&gt; ..data/password
lrwxrwxrwx 1 root root 11 Feb 20 14:22 user -&gt; ..data/user
#
# cat /etc/foo/user
pradeep#
# cat /etc/foo/password
topsecret#
# exit
</code></pre>
<p>One problem here is that the password is in plain text.
We can create another YAML defitinition with a slight modification.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-with-secret-volume-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-secret-volume-2
spec:
  containers:
  - name: pod-with-secret-volume-2
    image: redis
    volumeMounts:
    - name: foo
      mountPath: &quot;/etc/foo&quot;
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: my-secret
      items:
        - key: user
          path: my-group/my-username
          mode: 0777
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-with-secret-volume-2.yaml
pod/pod-with-secret-volume-2 created
</code></pre>
<pre><code class="language-sh">kubectl describe pod pod-with-secret-volume-2
Name:         pod-with-secret-volume-2
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Sun, 20 Feb 2022 20:10:40 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.43
IPs:
  IP:  10.244.1.43
Containers:
  pod-with-secret-volume-2:
    Container ID:   docker://56198fa123177a7c913973c16e686d25df1eeafa0787eef39fbf5306ac7e0b1d
    Image:          redis
    Image ID:       docker-pullable://redis@sha256:0d9c9aed1eb385336db0bc9b976b6b49774aee3d2b9c2788a0d0d9e239986cb3
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Sun, 20 Feb 2022 20:10:47 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /etc/foo from foo (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fwthr (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  foo:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  my-secret
    Optional:    false
  kube-api-access-fwthr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  38s   default-scheduler  Successfully assigned default/pod-with-secret-volume-2 to k8s-m02
  Normal  Pulling    36s   kubelet            Pulling image &quot;redis&quot;
  Normal  Pulled     32s   kubelet            Successfully pulled image &quot;redis&quot; in 4.744969468s
  Normal  Created    32s   kubelet            Created container pod-with-secret-volume-2
  Normal  Started    31s   kubelet            Started container pod-with-secret-volume-2
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it pod-with-secret-volume-2 -- sh
# ls /etc/foo
my-group
# ls -l /etc/foo
total 0
lrwxrwxrwx 1 root root 15 Feb 20 14:40 my-group -&gt; ..data/my-group
# cat /etc/foo/my-group/my-username
pradeep# at /etc/foo/my-group/my-password
sh: 9: at: not found
</code></pre>
<p>Only username is exposed while the password is not projected becuase the key is not exposed.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod-with-secret-volume-3.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-secret-volume-3
spec:
  containers:
  - name: pod-with-secret-volume-3
    image: redis
    volumeMounts:
    - name: foo
      mountPath: &quot;/etc/foo&quot;
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: my-secret
      items:
        - key: user
          mode: 0777
          path: my-group/my-username
        - key: password
          mode: 0777
          path: my-group/my-pass
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod-with-secret-volume-3.yaml
pod/pod-with-secret-volume-3 created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods | grep volume
pod-with-emptydir-volume    1/1     Running            0               110m
pod-with-hostpath-volume    1/1     Running            0               84m
pod-with-secret-volume      1/1     Running            0               29m
pod-with-secret-volume-2    1/1     Running            0               3m40s
pod-with-secret-volume-3    1/1     Running            0               16s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it pod-with-secret-volume-3 sh
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
# ls -l /etc/foo
total 0
lrwxrwxrwx 1 root root 15 Feb 20 14:51 my-group -&gt; ..data/my-group
# cat /etc/foo/my-group/my-username
pradeep#
# cat /etc/foo/my-group/my-pass
topsecret#
# exit
</code></pre>
<h3 id="persistentvolume">PersistentVolume</h3>
<p>Create a directory called <code>/mnt/data</code> on the <code>k8s</code> node. Within that directory, create <code>index.html</code> file with some text.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ sudo mkdir /mnt/data
$ sudo sh -c &quot;echo 'Hello from Kubernetes storage' &gt; /mnt/data/indext.html&quot;
$ cat /mnt/data/indext.html
Hello from Kubernetes storage
$ exit
logout
</code></pre>
<p>Now create a <code>PersistentVolume</code> named <code>my-pv-volume</code> of 1Gi using the <code>hostPath</code> pointing to the newly created directory. There are three types of <code>accessModes</code>, for this example use, <code>RWO</code>.</p>
<table>
<thead>
<tr>
<th>RWO</th>
<th>ReadWriteOnce</th>
</tr>
</thead>
<tbody>
<tr>
<td>RWX</td>
<td>ReadWriteMany</td>
</tr>
<tr>
<td>ROX</td>
<td>ReadyOnlyMany</td>
</tr>
</tbody>
</table>
<pre><code class="language-yaml">pradeep@learnk8s$ cat persistent-volume-demo.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/mnt/data&quot;
</code></pre>
<p>Create the PV using this YAML file.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f persistent-volume-demo.yaml
persistentvolume/my-pv-volume created
</code></pre>
<p>Verify the available PVs.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pv
NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
my-pv-volume   1Gi        RWO            Retain           Available           manual                  4s
</code></pre>
<p>Describe the PersistentVolume for addtional details.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pv
Name:            my-pv-volume
Labels:          type=local
Annotations:     &lt;none&gt;
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    manual
Status:          Available
Claim:
Reclaim Policy:  Retain
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        1Gi
Node Affinity:   &lt;none&gt;
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /mnt/data
    HostPathType:
Events:            &lt;none&gt;
</code></pre>
<h3 id="persistentvolumeclaim">PersistentVolumeClaim</h3>
<pre><code class="language-yaml">pradeep@learnk8s$ cat persistent-volume-claim-demo.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 300Mi
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f persistent-volume-claim-demo.yaml
persistentvolumeclaim/my-pv-claim created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pvc
NAME          STATUS   VOLUME         CAPACITY   ACCESS MODES   STORAGECLASS   AGE
my-pv-claim   Bound    my-pv-volume   1Gi        RWO            manual         40s
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pvc
Name:          my-pv-claim
Namespace:     default
StorageClass:  manual
Status:        Bound
Volume:        my-pv-volume
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      1Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       &lt;none&gt;
Events:        &lt;none&gt;
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pv
NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
my-pv-volume   1Gi        RWO            Retain           Bound    default/my-pv-claim   manual                  12m
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pv
Name:            my-pv-volume
Labels:          type=local
Annotations:     pv.kubernetes.io/bound-by-controller: yes
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    manual
Status:          Bound
Claim:           default/my-pv-claim
Reclaim Policy:  Retain
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        1Gi
Node Affinity:   &lt;none&gt;
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /mnt/data
    HostPathType:
Events:            &lt;none&gt;
</code></pre>
<p>The next step is to create a Pod that uses this PersistentVolumeClaim as a volume.</p>
<p>Here is the configuration file for the Pod:</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat my-pv-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pv-pod
spec:
  volumes:
    - name: my-pv-storage
      persistentVolumeClaim:
        claimName: my-pv-claim
  containers:
    - name: my-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: &quot;http-server&quot;
      volumeMounts:
        - mountPath: &quot;/usr/share/nginx/html&quot;
          name: my-pv-storage
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f my-pv-pod.yaml
pod/my-pv-pod created
</code></pre>
<p>The scheduler has placed this Pod on <code>k8s-m02</code> node.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods my-pv-pod -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP           NODE      NOMINATED NODE   READINESS GATES
my-pv-pod   1/1     Running   0          10s   10.244.1.2   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Let us describe the pod and look at the <code>Volumes</code> section.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods my-pv-pod
Name:         my-pv-pod
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Sun, 27 Feb 2022 22:35:45 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.2
IPs:
  IP:  10.244.1.2
Containers:
  my-pv-container:
    Container ID:   docker://9dc92c5c1037ad4d534fe6440275f62a68f521e3bd22fe47b57c06a8b3414e24
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sun, 27 Feb 2022 22:35:49 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /usr/share/nginx/html from my-pv-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xk6kh (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  my-pv-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  my-pv-claim
    ReadOnly:   false
  kube-api-access-xk6kh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  19s   default-scheduler  Successfully assigned default/my-pv-pod to k8s-m02
  Normal  Pulling    18s   kubelet            Pulling image &quot;nginx&quot;
  Normal  Pulled     15s   kubelet            Successfully pulled image &quot;nginx&quot; in 2.858388422s
  Normal  Created    15s   kubelet            Created container my-pv-container
  Normal  Started    15s   kubelet            Started container my-pv-container
</code></pre>
<p>It looks like no issues, but let us verify the <code>nginx</code> application by logging into the pod and issuing the <code>curl localhost</code> command.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it my-pv-pod -- /bin/sh
# curl localhost
&lt;html&gt;
&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.21.6&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
# exit
</code></pre>
<p>Hmm! This seems to be not working. It is forbidden. But this is expected right, we did not have the volume (hostPath) on this <code>k8s-m02</code> node. We created it on <code>k8s</code> node only.</p>
<p>To solve this, we can add a <code>nodeName</code> spec to the Pod definition.
Delete this pod and after adding the <code>nodeName</code> recreate the Pod.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete pod my-pv-pod
pod &quot;my-pv-pod&quot; deleted
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ vi my-pv-pod.yaml
</code></pre>
<pre><code class="language-yaml">pradeep@learnk8s$ cat my-pv-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pv-pod
spec:
  nodeName: k8s
  volumes:
    - name: my-pv-storage
      persistentVolumeClaim:
        claimName: my-pv-claim
  containers:
    - name: my-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: &quot;http-server&quot;
      volumeMounts:
        - mountPath: &quot;/usr/share/nginx/html&quot;
          name: my-pv-storage

</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f my-pv-pod.yaml
pod/my-pv-pod created
</code></pre>
<p>:warning: There seems to be some issue with this. Even after scheduling this pod on the node <code>k8s</code>, nginx container is reporting Forbidden. There are couple of issues already reported, but could not find any resolution to this problem yet.</p>
<p>https://github.com/kubernetes/website/issues/9523</p>
<h3 id="storageclass">StorageClass</h3>
<p>In the previous example, we used a storageClass named <code>manual</code>, but that does not seem to be present. </p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get sc
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  12d
</code></pre>
<p>There is a storage class by name standard, and is the default storage class.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe sc
Name:            standard
IsDefaultClass:  Yes
Annotations:     kubectl.kubernetes.io/last-applied-configuration={&quot;apiVersion&quot;:&quot;storage.k8s.io/v1&quot;,&quot;kind&quot;:&quot;StorageClass&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;},&quot;labels&quot;:{&quot;addonmanager.kubernetes.io/mode&quot;:&quot;EnsureExists&quot;},&quot;name&quot;:&quot;standard&quot;},&quot;provisioner&quot;:&quot;k8s.io/minikube-hostpath&quot;}
,storageclass.kubernetes.io/is-default-class=true
Provisioner:           k8s.io/minikube-hostpath
Parameters:            &lt;none&gt;
AllowVolumeExpansion:  &lt;unset&gt;
MountOptions:          &lt;none&gt;
ReclaimPolicy:         Delete
VolumeBindingMode:     Immediate
Events:                &lt;none&gt;
</code></pre>
<p>:memo: Changing the storage class name to <code>standard</code> also did not help solve the Forbidden issue with the <code>nginx</code>.</p>
<p>Strangely, inside the Pod, we can access the <code>index.html</code> file with <code>cat</code> command, but <code>curl</code> is not working.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it my-pv-pod -- /bin/bash
root@my-pv-pod:/# cat /usr/share/nginx/html/indext.html
Hello from Kubernetes storage
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl explain sc
KIND:     StorageClass
VERSION:  storage.k8s.io/v1

DESCRIPTION:
     StorageClass describes the parameters for a class of storage for which
     PersistentVolumes can be dynamically provisioned.

     StorageClasses are non-namespaced; the name of the storage class according
     to etcd is in ObjectMeta.Name.

FIELDS:
   allowVolumeExpansion &lt;boolean&gt;
     AllowVolumeExpansion shows whether the storage class allow volume expand

   allowedTopologies    &lt;[]Object&gt;
     Restrict the node topologies where volumes can be dynamically provisioned.
     Each volume plugin defines its own supported topology specifications. An
     empty TopologySelectorTerm list means there is no topology restriction.
     This field is only honored by servers that enable the VolumeScheduling
     feature.

   apiVersion   &lt;string&gt;
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind &lt;string&gt;
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata &lt;Object&gt;
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   mountOptions &lt;[]string&gt;
     Dynamically provisioned PersistentVolumes of this storage class are created
     with these mountOptions, e.g. [&quot;ro&quot;, &quot;soft&quot;]. Not validated - mount of the
     PVs will simply fail if one is invalid.

   parameters   &lt;map[string]string&gt;
     Parameters holds the parameters for the provisioner that should create
     volumes of this storage class.

   provisioner  &lt;string&gt; -required-
     Provisioner indicates the type of the provisioner.

   reclaimPolicy    &lt;string&gt;
     Dynamically provisioned PersistentVolumes of this storage class are created
     with this reclaimPolicy. Defaults to Delete.

   volumeBindingMode    &lt;string&gt;
     VolumeBindingMode indicates how PersistentVolumeClaims should be
     provisioned and bound. When unset, VolumeBindingImmediate is used. This
     field is only honored by servers that enable the VolumeScheduling feature.
</code></pre>
<h3 id="static-binding">Static Binding</h3>
<p>Here is the PersistentVolume definition. If we omit the storageClassName, PVC is net getting bound. As you see, there is a default storageClass named <code>standard</code> and PVC is using it by default.</p>
<pre><code class="language-sh">pradeep@learnk8s$ cat pv.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv
spec:
  storageClassName: standard
  capacity:
    storage: 512m
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /data/config
</code></pre>
<p>Create the PV from the YAML file.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pv.yaml
persistentvolume/pv created
</code></pre>
<p>Verify the available PVs. The newly created PV should be in <code>Available</code> state. Currently, there are no Claims for this volume.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv     512m       RWX            Retain           Available           standard                8s
</code></pre>
<p>Describe the PV for addtional details.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pv pv
Name:            pv
Labels:          &lt;none&gt;
Annotations:     &lt;none&gt;
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Available
Claim:
Reclaim Policy:  Retain
Access Modes:    RWX
VolumeMode:      Filesystem
Capacity:        512m
Node Affinity:   &lt;none&gt;
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /data/config
    HostPathType:
Events:            &lt;none&gt;
</code></pre>
<p>Now, create a PVC with same <code>accessModes: ReadWriteMany</code>.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 256m
</code></pre>
<p>Create PVC from the YAML file.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pvc.yaml
persistentvolumeclaim/pvc created
</code></pre>
<p>Verify the newly created PVC. It should be in <code>Bound</code> state. Note the <code>STORAGECLASS</code> column. It is using the default one named <code>standard</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pvc
NAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc    Bound    pv       512m       RWX            standard       4s
</code></pre>
<p>Describe the PVC for addtional details.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pvc
Name:          pvc
Namespace:     default
StorageClass:  standard
Status:        Bound
Volume:        pv
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      512m
Access Modes:  RWX
VolumeMode:    Filesystem
Used By:       &lt;none&gt;
Events:        &lt;none&gt;
</code></pre>
<p>Now, create a Pod that uses the newly defined PVC.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - image: nginx
    name: app
    volumeMounts:
    - mountPath: &quot;/data/app/config&quot;
      name: configpvc
  volumes:
  - name: configpvc
    persistentVolumeClaim:
      claimName: pvc
  restartPolicy: Never
</code></pre>
<p>Create the Pod from the YAML file.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod.yaml
pod/app created
</code></pre>
<p>Use the <code>-o wide</code> option to check the IP and NODE details.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods app -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP           NODE      NOMINATED NODE   READINESS GATES
app    1/1     Running   0          18s   10.244.1.5   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Describe the Pod and look for Volumes section.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods app
Name:         app
Namespace:    default
Priority:     0
Node:         k8s-m02/192.168.177.30
Start Time:   Mon, 28 Feb 2022 09:25:46 +0530
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.244.1.5
IPs:
  IP:  10.244.1.5
Containers:
  app:
    Container ID:   docker://09871149e09a1da02a86a9e38ace82082def59aa38dffbf739e14f7694f0f79e
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Mon, 28 Feb 2022 09:25:50 +0530
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /data/app/config from configpvc (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vfx8b (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  configpvc:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pvc
    ReadOnly:   false
  kube-api-access-vfx8b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  63s   default-scheduler  Successfully assigned default/app to k8s-m02
  Normal  Pulling    62s   kubelet            Pulling image &quot;nginx&quot;
  Normal  Pulled     59s   kubelet            Successfully pulled image &quot;nginx&quot; in 3.044922832s
  Normal  Created    59s   kubelet            Created container app
  Normal  Started    59s   kubelet            Started container app
</code></pre>
<p>Login to the Container and create a file in the <code>/data/app/config</code> folder, which is the container MountPath.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it app -- /bin/sh
# cd /data/app/config
# ls -l
total 0
# touch hello.txt
# echo &quot;Testing PV, PVC, and SC in K8S!&quot; &gt; hello.txt
# exit
</code></pre>
<p>Login to the minikube node and check if you are able to view the <code>hello.txt</code> file in the host.
On the control plane, there is no such file in the <code>/data/config</code> folder.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ls /data/config/
$ ls -la /data/config/
total 8
drwxr-xr-x 2 root root 4096 Feb 28 03:33 .
drwxr-xr-x 3 root root 4096 Feb 28 03:33 ..
$ exit
logout
</code></pre>
<p>On the other node, <code>k8s-m02</code>,  there is the <code>hello.txt</code> file.  This is because, the <code>app</code> pod got created in this node.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ls /data/config
hello.txt
$ ls -la /data/config/
total 12
drwxr-xr-x 2 root root 4096 Feb 28 03:58 .
drwxr-xr-x 3 root root 4096 Feb 28 03:55 ..
-rw-r--r-- 1 root root   32 Feb 28 03:58 hello.txt
$ cat /data/config/hello.txt
Testing PV, PVC, and SC in K8S!
$ exit
logout
</code></pre>
<p>We can also view the contents of the <code>hello.txt</code> file.</p>
<p>Verify the PV, PVC, and SC details.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pv,pvc,sc
NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE
persistentvolume/pv   512m       RWX            Retain           Bound    default/pvc   standard                9m35s

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/pvc   Bound    pv       512m       RWX            standard       8m17s

NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
storageclass.storage.k8s.io/standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  12d
</code></pre>
<p>Describe the PV one more time, after the Pod creation, to see if there is any change.
One change that we notice is that the Claim is showing <code>default/pvc</code> which was empty earlier. Also, Staus changed to <code>Bound</code> from <code>Available</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pv
Name:            pv
Labels:          &lt;none&gt;
Annotations:     pv.kubernetes.io/bound-by-controller: yes
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Bound
Claim:           default/pvc
Reclaim Policy:  Retain
Access Modes:    RWX
VolumeMode:      Filesystem
Capacity:        512m
Node Affinity:   &lt;none&gt;
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /data/config
    HostPathType:
Events:            &lt;none&gt;
</code></pre>
<p>Similarly describe the PVC and look for the changes. The  <code>Used By</code> value has changed to <code>app</code> now, showing that it is used by this Pod.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pvc
Name:          pvc
Namespace:     default
StorageClass:  standard
Status:        Bound
Volume:        pv
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      512m
Access Modes:  RWX
VolumeMode:    Filesystem
Used By:       app
Events:        &lt;none&gt;
</code></pre>
<p>Also, look at the StorageClass description. Verify that, <code>IsDefaultClass:  Yes</code> .</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe sc
Name:            standard
IsDefaultClass:  Yes
Annotations:     kubectl.kubernetes.io/last-applied-configuration={&quot;apiVersion&quot;:&quot;storage.k8s.io/v1&quot;,&quot;kind&quot;:&quot;StorageClass&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;},&quot;labels&quot;:{&quot;addonmanager.kubernetes.io/mode&quot;:&quot;EnsureExists&quot;},&quot;name&quot;:&quot;standard&quot;},&quot;provisioner&quot;:&quot;k8s.io/minikube-hostpath&quot;}
,storageclass.kubernetes.io/is-default-class=true
Provisioner:           k8s.io/minikube-hostpath
Parameters:            &lt;none&gt;
AllowVolumeExpansion:  &lt;unset&gt;
MountOptions:          &lt;none&gt;
ReclaimPolicy:         Delete
VolumeBindingMode:     Immediate
Events:                &lt;none&gt;
</code></pre>
<p>Now delete the pod.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete pod app
pod &quot;app&quot; deleted
</code></pre>
<p>After deleting the pod, check if the data is persistent or not. You should still be able to view the contents of the <code>hello.txt</code> file on the <code>k8s-m02</code> node, even after deleting the Pod, confirming that the storage is persistent now.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ cd /data/config/
$ cat hello.txt
Testing PV, PVC, and SC in K8S!
$ exit
logout
</code></pre>
<h3 id="dynamic-binding">Dynamic Binding</h3>
<p>Define a new storage class named <code>pradeep-sc-demo</code> in the <code>kube-system</code> namespace  using a sample YAML file.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat sc.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  namespace: kube-system
  name: pradeep-sc-demo
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: &quot;false&quot;
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
provisioner: k8s.io/minikube-hostpath
</code></pre>
<p>Create the storage class from this YAML file  and verify that it was created.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f sc.yaml
storageclass.storage.k8s.io/pradeep-sc-demo created
</code></pre>
<p>Now you should see two storageclasses, both with same PROVISIONER details. Also, not the <code>default</code> value next to the <code>standard</code> class, which is not present for the newly defined storage class.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get storageclass
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
pradeep-sc-demo      k8s.io/minikube-hostpath   Delete          Immediate           false                  47s
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  12d
</code></pre>
<p>Let us modify the PVC definition file to specify this <code>pradeep-sc-demo</code> StorageClass, instead of the <code>standard</code>.
First, let us delete the existing PVC.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete pvc pvc
persistentvolumeclaim &quot;pvc&quot; deleted
</code></pre>
<p>Modify the PVC definition, to include the new storage class.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc
spec:
  storageClassName: pradeep-sc-demo
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 256m
</code></pre>
<p>Create the PVC from the modified definition file.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pvc.yaml
persistentvolumeclaim/pvc created
</code></pre>
<p>Verify the Status of the new PVC.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pvc
NAME   STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
pvc    Pending                                      pradeep-sc-demo   64s
</code></pre>
<p>It is still showing <code>Pending</code>, to find out why let us describe it.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pvc
Name:          pvc
Namespace:     default
StorageClass:  pradeep-sc-demo
Status:        Pending
Volume:
Labels:        &lt;none&gt;
Annotations:   volume.beta.kubernetes.io/storage-provisioner: k8s.io/minikube-hostpath
               volume.kubernetes.io/storage-provisioner: k8s.io/minikube-hostpath
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:
Access Modes:
VolumeMode:    Filesystem
Used By:       &lt;none&gt;
Events:
  Type    Reason                Age               From                         Message
  ----    ------                ----              ----                         -------
  Normal  ExternalProvisioning  2s (x8 over 76s)  persistentvolume-controller  waiting for a volume to be created, either by external provisioner &quot;k8s.io/minikube-hostpath&quot; or manually created by system administrator
</code></pre>
<p>We can see that the PVC is waiting for a matching volume to be created.</p>
<p>Our existing PV is using the <code>standard</code> storageclass which is not matching with this PVC definition. So let us delete the existing PV and modify it to specify the new SC.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete pv pv
persistentvolume &quot;pv&quot; deleted
</code></pre>
<p>Here is the modified PV definition.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pv.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv
spec:
  storageClassName: pradeep-sc-demo
  capacity:
    storage: 512m
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /data/config
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pv,pvc
NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS      REASON   AGE
persistentvolume/pv   512m       RWX            Retain           Bound    default/pvc   pradeep-sc-demo            15s

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
persistentvolumeclaim/pvc   Bound    pv       512m       RWX            pradeep-sc-demo   6m29s
</code></pre>
<p>Now that the storageclass is matching, both the PV and PVC are in <code>Bound</code> State.</p>
<p>Create the <code>app</code> pod again using the same manifest file that we used earlier.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - image: nginx
    name: app
    volumeMounts:
    - mountPath: &quot;/data/app/config&quot;
      name: configpvc
  volumes:
  - name: configpvc
    persistentVolumeClaim:
      claimName: pvc
  restartPolicy: Never
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f pod.yaml
pod/app created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods app -o wide
NAME   READY   STATUS    RESTARTS   AGE     IP           NODE      NOMINATED NODE   READINESS GATES
app    1/1     Running   0          2m19s   10.244.1.6   k8s-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Shell into the Pod and create a file in the mounted directory.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec app -it -- /bin/sh
# cd /data/app/config
# ls -l
total 4
-rw-r--r-- 1 root root 32 Feb 28 03:58 hello.txt
# touch HiAgain.txt
# echo &quot;Hello again from K8s PV,PVC, and SC!&quot; &gt; HiAgain.txt
# cat HiAgain.txt
Hello again from K8s PV,PVC, and SC!
# exit
</code></pre>
<p>Becuase of persistent nature, we still see the old <code>hello.txt</code> file in this new container. 
We have created another file called <code>HiAgain.txt</code> with some sample text.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pv,pvc,sc
NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS      REASON   AGE
persistentvolume/pv   512m       RWX            Retain           Bound    default/pvc   pradeep-sc-demo            6m43s

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
persistentvolumeclaim/pvc   Bound    pv       512m       RWX            pradeep-sc-demo   12m

NAME                                             PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
storageclass.storage.k8s.io/pradeep-sc-demo      k8s.io/minikube-hostpath   Delete          Immediate           false                  18m
storageclass.storage.k8s.io/standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  12d
</code></pre>
<p>Let us delete the Pod again and verify the files.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete pod app
pod &quot;app&quot; deleted
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ cd /data/config/
$ ls
HiAgain.txt  hello.txt
$ ls -la
total 16
drwxr-xr-x 2 root root 4096 Feb 28 04:50 .
drwxr-xr-x 3 root root 4096 Feb 28 03:55 ..
-rw-r--r-- 1 root root   37 Feb 28 04:50 HiAgain.txt
-rw-r--r-- 1 root root   32 Feb 28 03:58 hello.txt
$ cat HiAgain.txt
Hello again from K8s PV,PVC, and SC!
$ cat hello.txt
Testing PV, PVC, and SC in K8S!
$ exit
logout
</code></pre>
<p>This confirms that, the storage is persistent again (even after Pod deletion).</p>
<h2 id="networking">Networking</h2>
<h3 id="kindnet">KindNet</h3>
<p>Welcome to Kubernetes Networking.</p>
<p>Let us setup a fresh minikube cluster with default settings and with 3 nodes in it. </p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube start --nodes=3
üòÑ  minikube v1.25.2 on Darwin 12.2.1
‚ú®  Automatically selected the hyperkit driver
üíæ  Downloading driver docker-machine-driver-hyperkit:
    &gt; docker-machine-driver-hyper...: 65 B / 65 B [----------] 100.00% ? p/s 0s
    &gt; docker-machine-driver-hyper...: 8.35 MiB / 8.35 MiB  100.00% 9.87 MiB p/s
üîë  The 'hyperkit' driver requires elevated permissions. The following commands will be executed:

    $ sudo chown root:wheel /Users/pradeep/.minikube/bin/docker-machine-driver-hyperkit
    $ sudo chmod u+s /Users/pradeep/.minikube/bin/docker-machine-driver-hyperkit


Password:
üíø  Downloading VM boot image ...
    &gt; minikube-v1.25.2.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s
    &gt; minikube-v1.25.2.iso: 237.06 MiB / 237.06 MiB [] 100.00% 6.12 MiB p/s 39s
üëç  Starting control plane node minikube in cluster minikube
üíæ  Downloading Kubernetes v1.23.3 preload ...
    &gt; preloaded-images-k8s-v17-v1...: 505.68 MiB / 505.68 MiB  100.00% 9.37 MiB
üî•  Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    ‚ñ™ kubelet.housekeeping-interval=5m
    ‚ñ™ kubelet.cni-conf-dir=/etc/cni/net.mk
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîó  Configuring CNI (Container Networking Interface) ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass

üëç  Starting worker node minikube-m02 in cluster minikube
üî•  Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=172.16.30.3
üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    ‚ñ™ env NO_PROXY=172.16.30.3
üîé  Verifying Kubernetes components...

üëç  Starting worker node minikube-m03 in cluster minikube
üî•  Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=172.16.30.3,172.16.30.4
üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    ‚ñ™ env NO_PROXY=172.16.30.3
    ‚ñ™ env NO_PROXY=172.16.30.3,172.16.30.4
üîé  Verifying Kubernetes components...
üèÑ  Done! kubectl is now configured to use &quot;minikube&quot; cluster and &quot;default&quot; namespace by default
pradeep@learnk8s$
</code></pre>
<p>Our cluster nodes are assinged the IPs: <code>172.16.30.3</code>, <code>172.16.30.4</code>, and <code>172.16.30.5</code> respectively.</p>
<p>Also, note that the CNI config directory <code>kubelet.cni-conf-dir=/etc/cni/net.mk</code> and minikube is configuring CNI (Container Networking Interface) during the start.</p>
<pre><code class="language-shell">pradeep@learnk8s$ kubectl get nodes -o wide
NAME           STATUS   ROLES                  AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube       Ready    control-plane,master   6m54s   v1.23.3   172.16.30.3   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
minikube-m02   Ready    &lt;none&gt;                 4m43s   v1.23.3   172.16.30.4   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
minikube-m03   Ready    &lt;none&gt;                 74s     v1.23.3   172.16.30.5   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
pradeep@learnk8s$
</code></pre>
<p>Let us take a look at all the pods in this newly deployed cluster. Currently, all the pods are in the <code>kube-system</code> namespace. The pods that are of interest to us in this post are the ones starting with the name <code>kindnet</code>.</p>
<p>So what is Kindnet? Kindnet is a simple CNI plugin for Kubernetes with IPv4 and IPv6 support that provides the Cluster Networking.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-64897985d-llrjh            1/1     Running   0          7m25s
kube-system   etcd-minikube                      1/1     Running   0          7m40s
kube-system   kindnet-4mrvw                      1/1     Running   0          2m2s
kube-system   kindnet-cpv4s                      1/1     Running   0          5m31s
kube-system   kindnet-xqjlm                      1/1     Running   0          7m26s
kube-system   kube-apiserver-minikube            1/1     Running   0          7m37s
kube-system   kube-controller-manager-minikube   1/1     Running   0          7m37s
kube-system   kube-proxy-b97w8                   1/1     Running   0          7m26s
kube-system   kube-proxy-gtlw7                   1/1     Running   0          2m2s
kube-system   kube-proxy-rts4b                   1/1     Running   0          5m31s
kube-system   kube-scheduler-minikube            1/1     Running   0          7m37s
kube-system   storage-provisioner                1/1     Running   1          7m35s
</code></pre>
<p>There are three kindnet pods, and if we check the IP address of these pods, all of them are having the same IP address as that of their node.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -A -o wide | grep kindnet
kube-system   kindnet-4mrvw                      1/1     Running   0          6m17s   172.16.30.5   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-cpv4s                      1/1     Running   0          9m46s   172.16.30.4   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-xqjlm                      1/1     Running   0          11m     172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>Let us login to the first node, and check its routing table and all the interfaces currently configured on this node.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether d6:df:bb:d6:c7:bc brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.3/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 85395sec preferred_lft 85395sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:df:e0:80:59 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: veth2b322de6@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 8e:09:87:56:8c:50 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.0.1/32 brd 10.244.0.1 scope global veth2b322de6
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.3 metric 1024
10.244.0.2 dev veth2b322de6 scope host
10.244.1.0/24 via 172.16.30.4 dev eth0
10.244.2.0/24 via 172.16.30.5 dev eth0
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.3
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.3 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
</code></pre>
<p>This <code>minikube</code> node has the IP address of <code>172.16.30.3/24</code> on the <code>eth0</code> interface and  there is a static route to <code>10.244.1.0/24</code> with next-hop as the <code>minikube-m02</code> node with IP address of <code>172.16.30.4</code> and similarly another static route to <code>10.244.2.0/24</code> with next-hop as <code>172.16.30.5</code> which is the IP address of the the <code>minikube-m03</code> node.</p>
<p>There is another virtual interface (<code>veth</code>) called <code>veth2b322de6@if4</code> with an IP address of <code>10.244.0.1/32</code>. So what are these subnets <code>10.244.X.0/24</code> used for?</p>
<p>If we look at the <code>kindnet</code> CNI configuration file (located at the path given in the <code>minikube start</code> output shown above), on this <code>minikube</code> node,  we can see that there is a subnet <code>10.244.0.0/24</code> range defined.</p>
<pre><code class="language-json">$ cat /etc/cni/net.mk/10-kindnet.conflist
{
    &quot;cniVersion&quot;: &quot;0.3.1&quot;,
    &quot;name&quot;: &quot;kindnet&quot;,
    &quot;plugins&quot;: [
    {
        &quot;type&quot;: &quot;ptp&quot;,
        &quot;ipMasq&quot;: false,
        &quot;ipam&quot;: {
            &quot;type&quot;: &quot;host-local&quot;,
            &quot;dataDir&quot;: &quot;/run/cni-ipam-state&quot;,
            &quot;routes&quot;: [


                { &quot;dst&quot;: &quot;0.0.0.0/0&quot; }
            ],
            &quot;ranges&quot;: [


                [ { &quot;subnet&quot;: &quot;10.244.0.0/24&quot; } ]
            ]
        }
        ,
        &quot;mtu&quot;: 1500

    },
    {
        &quot;type&quot;: &quot;portmap&quot;,
        &quot;capabilities&quot;: {
            &quot;portMappings&quot;: true
        }
    }
    ]
}
</code></pre>
<p>Similarly, let us verify the same details in other two nodes of the cluster.</p>
<p>First, login to the <code>minikube-m02</code> node.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m02
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 2e:6c:52:09:b4:bb brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.4/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 84780sec preferred_lft 84780sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:b9:25:87:9f brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.4 metric 1024
10.244.0.0/24 via 172.16.30.3 dev eth0
10.244.2.0/24 via 172.16.30.5 dev eth0
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.4
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.4 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$ cat /etc/cni/net.mk/10-kindnet.conflist

{
    &quot;cniVersion&quot;: &quot;0.3.1&quot;,
    &quot;name&quot;: &quot;kindnet&quot;,
    &quot;plugins&quot;: [
    {
        &quot;type&quot;: &quot;ptp&quot;,
        &quot;ipMasq&quot;: false,
        &quot;ipam&quot;: {
            &quot;type&quot;: &quot;host-local&quot;,
            &quot;dataDir&quot;: &quot;/run/cni-ipam-state&quot;,
            &quot;routes&quot;: [


                { &quot;dst&quot;: &quot;0.0.0.0/0&quot; }
            ],
            &quot;ranges&quot;: [


                [ { &quot;subnet&quot;: &quot;10.244.1.0/24&quot; } ]
            ]
        }
        ,
        &quot;mtu&quot;: 1500

    },
    {
        &quot;type&quot;: &quot;portmap&quot;,
        &quot;capabilities&quot;: {
            &quot;portMappings&quot;: true
        }
    }
    ]
}
$
</code></pre>
<p>We can see a similar setup. Local subnet range defined in the CNI (kindnet) configuration file is the <code>10.244.1.0/24</code> subnet and two static routes (<code>10.244.0.0/24</code> via <code>172.16.30.3</code> and another static route <code>10.244.2.0/24 via 172.16.30.5</code>).</p>
<p>Finally, let us check the same in the <code>minikube-m03</code> node.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m03
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 84738sec preferred_lft 84738sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.5 metric 1024
10.244.0.0/24 via 172.16.30.3 dev eth0
10.244.1.0/24 via 172.16.30.4 dev eth0
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.5
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.5 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$ cat /etc/cni/net.mk/10-kindnet.conflist

{
    &quot;cniVersion&quot;: &quot;0.3.1&quot;,
    &quot;name&quot;: &quot;kindnet&quot;,
    &quot;plugins&quot;: [
    {
        &quot;type&quot;: &quot;ptp&quot;,
        &quot;ipMasq&quot;: false,
        &quot;ipam&quot;: {
            &quot;type&quot;: &quot;host-local&quot;,
            &quot;dataDir&quot;: &quot;/run/cni-ipam-state&quot;,
            &quot;routes&quot;: [


                { &quot;dst&quot;: &quot;0.0.0.0/0&quot; }
            ],
            &quot;ranges&quot;: [


                [ { &quot;subnet&quot;: &quot;10.244.2.0/24&quot; } ]
            ]
        }
        ,
        &quot;mtu&quot;: 1500

    },
    {
        &quot;type&quot;: &quot;portmap&quot;,
        &quot;capabilities&quot;: {
            &quot;portMappings&quot;: true
        }
    }
    ]
}
$ exit
logout
</code></pre>
<p>Here also, very similar setup. <code>10.244.2.0/24</code> is the local subnet and remote subnets are cofnigured via static routes (<code>10.244.0.0/24 via 172.16.30.3</code> and <code>10.244.1.0/24 via 172.16.30.4</code>).</p>
<p>One thing to note is that, in both of the worker nodes, there isn't any <code>veth</code> interface present yet, unlike the <code>controleplane</code> node.</p>
<p>To understand, why let us get the IP addresses of all the running pods.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -A -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
kube-system   coredns-64897985d-llrjh            1/1     Running   0          32m   10.244.0.2    minikube       &lt;none&gt;           &lt;none&gt;
kube-system   etcd-minikube                      1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-4mrvw                      1/1     Running   0          27m   172.16.30.5   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-cpv4s                      1/1     Running   0          31m   172.16.30.4   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   kindnet-xqjlm                      1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-minikube            1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-minikube   1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-b97w8                   1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-gtlw7                   1/1     Running   0          27m   172.16.30.5   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-rts4b                   1/1     Running   0          31m   172.16.30.4   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-minikube            1/1     Running   0          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   storage-provisioner                1/1     Running   1          33m   172.16.30.3   minikube       &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>There is one Pod named <code>coredns-64897985d-llrjh</code> in the <code>kube-system</code> namespace with an IP address of <code>10.244.0.2</code> which is in the same subnet range defined in the CNI config on the <code>minikube</code> node.</p>
<p>Let us create our first pod (we can use any image, it does not matter). For test purposes, let us create a new pod using the <code>busybox</code> image.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run busybox --image=busybox
pod/busybox created
</code></pre>
<p>Verify the IP address assigned and the node on which it is running.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME      READY   STATUS      RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
busybox   0/1     Completed   0          12s   10.244.2.2   minikube-m03   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>We can see that the scheduler has assigned the <code>minikube-m03</code> node for this new pod and this <code>busybox</code> pod obtained its IP address <code>10.244.2.2</code> which is from the CNI assigned subnet (<code>10.244.2.0/24</code>) for this node.</p>
<p>Now, let us go back to the <code>minikube-m03</code> node and check the list of interfaces and see if there is anything new!</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m03
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 83749sec preferred_lft 83749sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: veth07b7de9e@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 6a:9a:f7:af:ac:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.2.1/32 brd 10.244.2.1 scope global veth07b7de9e
       valid_lft forever preferred_lft forever
$
</code></pre>
<p>Compared to the initial setup, there is one new interface (#5, with the name <code>veth07b7de9e@if4</code>). This is very similar to the <code>controlplane</code> node now. </p>
<p>This <code>veth</code> interface has the first IP address (<code>10.244.2.1/32</code>) from the CNI assigned subnet.</p>
<p>I tried to login to this container and check few things from inside the container, but before I do that the container crashed, so I have deleted it.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -it busybox -- /bin/bash
error: unable to upgrade connection: container not found (&quot;busybox&quot;)
pradeep@learnk8s$ kubectl get pods
NAME      READY   STATUS             RESTARTS      AGE
busybox   0/1     CrashLoopBackOff   6 (63s ago)   7m3s
pradeep@learnk8s$ kubectl delete pod busybox
pod &quot;busybox&quot; deleted
</code></pre>
<p>Create another containter, this time using another image, <code>nginx</code> for test purposes.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run nginx --image=nginx
pod/nginx created
</code></pre>
<p>Verify the IP address of this new pod and the node on which it is running.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          25s   10.244.2.3   minikube-m03   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>Ah!, this pod also got assigned to the same node, <code>minikube-m03</code> and look at the IP address, the next IP address in the same range, <code>10.244.2.3</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m03
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 83069sec preferred_lft 83069sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
6: veth37e46d8f@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 7e:96:f9:98:5f:4a brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.2.1/32 brd 10.244.2.1 scope global veth37e46d8f
       valid_lft forever preferred_lft forever
$
</code></pre>
<p>From the routing table, we can see that the new Pod IP is routed via the <code>veth</code> interface <code>10.244.2.3 dev veth37e46d8f</code>.</p>
<pre><code class="language-sh">$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.5 metric 1024
10.244.0.0/24 via 172.16.30.3 dev eth0
10.244.1.0/24 via 172.16.30.4 dev eth0
10.244.2.3 dev veth37e46d8f scope host
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.5
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.5 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$
</code></pre>
<p>Let us manually schedule a Pod on the <code>minikube-m02</code> node as well and observe.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat my-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-manual
spec:
  nodeName: minikube-m02
  containers:
  - image: nginx
    name: nginx
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f my-pod.yaml
pod/nginx-manual created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
nginx-manual   1/1     Running   0          73s   10.244.1.2   minikube-m02   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>We can see that, this pod has been given an IP (<code>10.244.1.2</code>) from the 10.244.1.0/24 subnet allocated to <code>minikube-m02</code> node.</p>
<p>Let us login to this node and verify the list of currently running pods with the <code>docker ps</code> command.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m02
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS     NAMES
d6397d143118   nginx                  &quot;/docker-entrypoint.‚Ä¶&quot;   3 minutes ago   Up 3 minutes             k8s_nginx_nginx-manual_default_950195d8-bc1d-420f-aede-fcd4a273f0e0_0
d2fb47206b71   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 4 minutes ago   Up 4 minutes             k8s_POD_nginx-manual_default_950195d8-bc1d-420f-aede-fcd4a273f0e0_0
ea4e0c180710   6de166512aa2           &quot;/bin/kindnetd&quot;          9 minutes ago   Up 9 minutes             k8s_kindnet-cni_kindnet-cpv4s_kube-system_5d3d3977-c5ff-4bea-917a-e0db52896da2_1
6ae7df286995   9b7cc9982109           &quot;/usr/local/bin/kube‚Ä¶&quot;   9 minutes ago   Up 9 minutes             k8s_kube-proxy_kube-proxy-rts4b_kube-system_9773fb68-a469-4cb2-827b-546076677b3b_1
d77c3e7c06e4   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 9 minutes ago   Up 9 minutes             k8s_POD_kindnet-cpv4s_kube-system_5d3d3977-c5ff-4bea-917a-e0db52896da2_1
5e18805db658   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 9 minutes ago   Up 9 minutes             k8s_POD_kube-proxy-rts4b_kube-system_9773fb68-a469-4cb2-827b-546076677b3b_1
$
</code></pre>
<p>Obtain the container ID for the <code>nginx-manual</code> pod. In this case ,it is <code>d6397d143118</code>.</p>
<p>Use the <code>docker inspect</code> command to get the PID of this container.</p>
<pre><code class="language-sh">$ docker inspect d2fb47206b71 | grep Pid
            &quot;Pid&quot;: 4200,
            &quot;PidMode&quot;: &quot;&quot;,
            &quot;PidsLimit&quot;: null,
$ 
</code></pre>
<p>Now, take the <code>Pid</code> and use the <code>nsenter</code> command to issue the <code>ip a</code> command from inside this Pod namespace. This requires root privileges, so use <code>sudo</code> command.</p>
<pre><code class="language-sh">$ sudo nsenter -t 4200 -n ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: eth0@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 52:a1:40:3b:54:18 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.1.2/24 brd 10.244.1.255 scope global eth0
       valid_lft forever preferred_lft forever
$
</code></pre>
<p>We can see that the Pod IP (<code>10.244.1.2</code>) is present on the interface <code>`#4</code> named <code>eth0@if5</code>.</p>
<p>Look at all the interfaces from this <code>minikube-m02</code> node.</p>
<pre><code class="language-sh">$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 2e:6c:52:09:b4:bb brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.4/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 85770sec preferred_lft 85770sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:41:3c:16:fe brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: vethfa21a27c@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether b2:26:8c:88:bf:d3 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.1.1/32 brd 10.244.1.1 scope global vethfa21a27c
       valid_lft forever preferred_lft forever
$
</code></pre>
<p>From the interface naming, we can see the link between the Pod namespace and the host.  The <code>4: eth0@if5:</code> interface with the IP address <code>10.244.1.2/24</code> is linked to the <code>5: vethfa21a27c@if4:</code> interface with the IP address <code>10.244.1.1/32</code>.</p>
<p>The name followed by the <code>@</code> symbol tells us the interface number. For example, inside the Pod namespace, it is shown as <code>@if5</code> for the interface number <code>4:</code>, this corresponds to interface number <code>5:</code> on the host, which is nothing but the <code>5: vethfa21a27c@if4:</code>. From this <code>5: vethfa21a27c@if4:</code> on the host, <code>@if4</code> corresponds to the interface number <code>4</code> on the Pod, which is the <code>eth0</code> interface indicated by <code>4: eth0@if5:</code>.</p>
<p>Now you can see the full picture of the internal connectivity. </p>
<p>Just to confirm, this Pod hosted on <code>minikube-m02</code> can communicate with a Pod with IP address 10.244.0.2 hosted on the <code>minikube</code> node.
This can be confirmed by first looking at the Pod routing table, using the same <code>nsenter</code> command. There is a <code>default</code> route pointing to the gateway <code>10.244.1.1</code>.</p>
<pre><code class="language-sh">$ sudo nsenter -t 4200 -n ip route
default via 10.244.1.1 dev eth0
10.244.1.0/24 via 10.244.1.1 dev eth0 src 10.244.1.2
10.244.1.1 dev eth0 scope link src 10.244.1.2
$
</code></pre>
<p>Ping to another Pod in the same cluster, but on another node.</p>
<pre><code class="language-sh">$ sudo nsenter -t 4200 -n ping 10.244.0.2 -c 3
PING 10.244.0.2 (10.244.0.2): 56 data bytes
64 bytes from 10.244.0.2: seq=0 ttl=62 time=1.313 ms
64 bytes from 10.244.0.2: seq=1 ttl=62 time=1.980 ms
64 bytes from 10.244.0.2: seq=2 ttl=62 time=1.139 ms

--- 10.244.0.2 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 1.139/1.477/1.980 ms
$
</code></pre>
<p>We can also see that using the same <code>default</code> route, this Pod can communicate with the outside cluster as well.</p>
<pre><code class="language-sh">$ sudo nsenter -t 4200 -n ping 8.8.8.8
PING 8.8.8.8 (8.8.8.8): 56 data bytes
64 bytes from 8.8.8.8: seq=0 ttl=115 time=12.515 ms
64 bytes from 8.8.8.8: seq=1 ttl=115 time=12.238 ms
64 bytes from 8.8.8.8: seq=2 ttl=115 time=11.216 ms
64 bytes from 8.8.8.8: seq=3 ttl=115 time=11.303 ms
^C
--- 8.8.8.8 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max = 11.216/11.818/12.515 ms
$
</code></pre>
<p>The following diagram summarizes our discussion so far.</p>
<p><img alt="" src="/assets/images/k8s-minikube-kindnet.png" /></p>
<p>This concludes our initial verification of the Kubernetes networking on the minikube with the default CNI (kindnet) which uses the simple <code>static</code> routes to facilitate communication across nodes. We will look at other CNIs later in other posts.</p>
<h3 id="calico">Calico</h3>
<p>Kubernetes Networking with Calico CNI</p>
<p>Let us build another minikube cluster, this time with the Calico CNI. Minikube supports many CNIs.</p>
<p>From the <code>minikube start -h</code> output, we can see the supported CNI plug-ins list.</p>
<pre><code class="language-sh"> --cni='': CNI plug-in to use. Valid options: auto, bridge, calico, cilium, flannel, kindnet, or path to a CNI
manifest (default: auto)
</code></pre>
<p>Start the 3-node minikube cluster with <code>calico</code> CNI plugin.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube start --nodes=3 --cni=calico
üòÑ  minikube v1.25.2 on Darwin 12.2.1
‚ú®  Automatically selected the hyperkit driver
üëç  Starting control plane node minikube in cluster minikube
üî•  Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    ‚ñ™ kubelet.housekeeping-interval=5m
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîó  Configuring Calico (Container Networking Interface) ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: default-storageclass, storage-provisioner

üëç  Starting worker node minikube-m02 in cluster minikube
üî•  Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=172.16.30.6
üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    ‚ñ™ env NO_PROXY=172.16.30.6
üîé  Verifying Kubernetes components...

üëç  Starting worker node minikube-m03 in cluster minikube
üî•  Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=172.16.30.6,172.16.30.7
üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    ‚ñ™ env NO_PROXY=172.16.30.6
    ‚ñ™ env NO_PROXY=172.16.30.6,172.16.30.7
üîé  Verifying Kubernetes components...
üèÑ  Done! kubectl is now configured to use &quot;minikube&quot; cluster and &quot;default&quot; namespace by default
pradeep@learnk8s$
</code></pre>
<p>Verify the cluster node IPs: <code>172.16.30.6</code>, <code>172.16.30.7</code>, and <code>172.16.30.8</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get nodes -o wide
NAME           STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME
minikube       Ready    control-plane,master   22m   v1.23.3   172.16.30.6   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
minikube-m02   Ready    &lt;none&gt;                 20m   v1.23.3   172.16.30.7   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
minikube-m03   Ready    &lt;none&gt;                 17m   v1.23.3   172.16.30.8   &lt;none&gt;        Buildroot 2021.02.4   4.19.202         docker://20.10.12
pradeep@learnk8s$
</code></pre>
<p>Verify the list of Pods in all namespaces.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -A -o wide
NAMESPACE     NAME                                       READY   STATUS    RESTARTS        AGE     IP            NODE           NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-8594699699-dztlm   1/1     Running   0               6m50s   10.88.0.3     minikube       &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-gqvw6                          1/1     Running   1 (2m54s ago)   4m38s   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-qdbcf                          1/1     Running   0               6m50s   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-sw74l                          1/1     Running   0               114s    172.16.30.8   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-64897985d-58btq                    1/1     Running   0               6m50s   10.88.0.2     minikube       &lt;none&gt;           &lt;none&gt;
kube-system   etcd-minikube                              1/1     Running   0               7m1s    172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-minikube                    1/1     Running   0               7m1s    172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-minikube           1/1     Running   0               7m1s    172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-7k4lb                           1/1     Running   0               114s    172.16.30.8   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-gm2dh                           1/1     Running   0               4m38s   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-hvkqd                           1/1     Running   0               6m50s   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-minikube                    1/1     Running   0               7m1s    172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-system   storage-provisioner                        1/1     Running   1 (2m26s ago)   6m47s   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>There are two Pods using the 10.88.0.X subnet (<code>calico-kube-controllers</code> and <code>coredns</code> Pods ).</p>
<p>Login to the <code>controlplane</code> node and check the CNI directory.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ cat /etc/cni/net.d/
.keep                      10-calico.conflist         87-podman-bridge.conflist  calico-kubeconfig
$ cat /etc/cni/net.d/10-calico.conflist
{
  &quot;name&quot;: &quot;k8s-pod-network&quot;,
  &quot;cniVersion&quot;: &quot;0.3.1&quot;,
  &quot;plugins&quot;: [
    {
      &quot;type&quot;: &quot;calico&quot;,
      &quot;log_level&quot;: &quot;info&quot;,
      &quot;log_file_path&quot;: &quot;/var/log/calico/cni/cni.log&quot;,
      &quot;datastore_type&quot;: &quot;kubernetes&quot;,
      &quot;nodename&quot;: &quot;minikube&quot;,
      &quot;mtu&quot;: 0,
      &quot;ipam&quot;: {
          &quot;type&quot;: &quot;calico-ipam&quot;
      },
      &quot;policy&quot;: {
          &quot;type&quot;: &quot;k8s&quot;
      },
      &quot;kubernetes&quot;: {
          &quot;kubeconfig&quot;: &quot;/etc/cni/net.d/calico-kubeconfig&quot;
      }
    },
    {
      &quot;type&quot;: &quot;portmap&quot;,
      &quot;snat&quot;: true,
      &quot;capabilities&quot;: {&quot;portMappings&quot;: true}
    },
    {
      &quot;type&quot;: &quot;bandwidth&quot;,
      &quot;capabilities&quot;: {&quot;bandwidth&quot;: true}
    }
  ]
}$
</code></pre>
<p>Apart from the Calico , there is a config file for Podman as well. Let us check that as well.</p>
<pre><code class="language-sh">$ cat /etc/cni/net.d/87-podman-bridge.conflist
{
  &quot;cniVersion&quot;: &quot;0.4.0&quot;,
  &quot;name&quot;: &quot;podman&quot;,
  &quot;plugins&quot;: [
    {
      &quot;type&quot;: &quot;bridge&quot;,
      &quot;bridge&quot;: &quot;cni-podman0&quot;,
      &quot;isGateway&quot;: true,
      &quot;ipMasq&quot;: true,
      &quot;hairpinMode&quot;: true,
      &quot;ipam&quot;: {
        &quot;type&quot;: &quot;host-local&quot;,
        &quot;routes&quot;: [{ &quot;dst&quot;: &quot;0.0.0.0/0&quot; }],
        &quot;ranges&quot;: [
          [
            {
              &quot;subnet&quot;: &quot;10.88.0.0/16&quot;,
              &quot;gateway&quot;: &quot;10.88.0.1&quot;
            }
          ]
        ]
      }
    },
    {
      &quot;type&quot;: &quot;portmap&quot;,
      &quot;capabilities&quot;: {
        &quot;portMappings&quot;: true
      }
    },
    {
      &quot;type&quot;: &quot;firewall&quot;
    },
    {
      &quot;type&quot;: &quot;tuning&quot;
    }
  ]
}
$
</code></pre>
<p>From this, it is clear that while starting the minikube cluster, those two Pods got the IP from the Podman CNI, which seems to compete with the Calico CNI.</p>
<p>This issue is reported here: https://github.com/kubernetes/kubernetes/issues/107687 </p>
<p>Let us create a new Pod in the default namespace and check which IP will be obtained.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl run nginx --image=nginx
pod/nginx created
</code></pre>
<p>Verify the IP of the new Pod and the node on which it got hosted.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          56s   10.244.205.193   minikube-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>The new nginx pod got the IP address of <code>10.244.205.193</code> and is running on the <code>minikube-m02</code> node.</p>
<p>Let us log in to this <code>minikube-m02</code> node and verify the routing table and list of interfaces.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m02
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 22:a7:1a:d9:be:42 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.7/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 84337sec preferred_lft 84337sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:d3:aa:60:ec brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.205.192/32 scope global tunl0
       valid_lft forever preferred_lft forever
8: calic440f455693@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.7 metric 1024
10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink
10.244.151.0/26 via 172.16.30.8 dev tunl0 proto bird onlink
blackhole 10.244.205.192/26 proto bird
10.244.205.193 dev calic440f455693 scope link
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.7
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.7 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$
</code></pre>
<p>There are two special interfaces, <code>5: tunl0@NONE:</code> and <code>8: calic440f455693@if5:</code>. We can see that it is an <code>ipip</code> tunnel and tunnel interface (<code>tunl0</code>) has the IP address of <code>10.244.205.192/32</code>.  </p>
<p>From the routing table, there are two /26 routes pointing to the <code>tunl0</code> interface. The <code>10.244.120.64/26 via 172.16.30.6</code> and <code>10.244.151.0/26 via 172.16.30.8</code> entries in the routing table, shows  the <code>Calico</code> assigned subnets on the other two nodes, and the routes for those remote subnets via the <code>IPIP</code> tunnel interface.</p>
<p>From within the <code>minikube-m02</code> node, issue the <code>docker ps</code> command the retrieve the container ID of the <code>nginx</code> container.</p>
<pre><code class="language-sh">$ docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS     NAMES
0d1f5d390956   nginx                  &quot;/docker-entrypoint.‚Ä¶&quot;   28 minutes ago   Up 28 minutes             k8s_nginx_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0
6b67d9586b86   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 28 minutes ago   Up 28 minutes             k8s_POD_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0
6f65ec9ee321   5ef66b403f4f           &quot;start_runit&quot;            39 minutes ago   Up 39 minutes             k8s_calico-node_calico-node-gqvw6_kube-system_6a835837-d36e-4366-aefe-cedc09d2f148_1
3d0be715b116   9b7cc9982109           &quot;/usr/local/bin/kube‚Ä¶&quot;   41 minutes ago   Up 41 minutes             k8s_kube-proxy_kube-proxy-gm2dh_kube-system_770d601e-7b93-42fa-8019-5976ae95684e_0
9c077d1f8374   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 41 minutes ago   Up 41 minutes             k8s_POD_calico-node-gqvw6_kube-system_6a835837-d36e-4366-aefe-cedc09d2f148_0
4b2360ad79a9   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 41 minutes ago   Up 41 minutes             k8s_POD_kube-proxy-gm2dh_kube-system_770d601e-7b93-42fa-8019-5976ae95684e_0
$
</code></pre>
<p>Get the <code>Pid</code> of the <code>nginx</code> container with the <code>docker inspect</code> command.</p>
<pre><code class="language-sh">$ docker inspect 0d1f5d390956 | grep Pid
            &quot;Pid&quot;: 11380,
            &quot;PidMode&quot;: &quot;&quot;,
            &quot;PidsLimit&quot;: null,
$
</code></pre>
<p>With the <code>nsenter</code> command, verify the list of interfaces inside the <code>nginx</code> container.</p>
<pre><code class="language-sh">$ sudo nsenter -t 11380 -n ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
3: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
5: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default
    link/ether e6:ea:af:be:ec:53 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.205.193/32 brd 10.244.205.193 scope global eth0
       valid_lft forever preferred_lft forever
$
</code></pre>
<p>We can see that the Pod IP is configured on the <code>5: eth0@if8:</code> interface. From the <code>@if8</code>, we can see the link to the <code>8: calic440f455693@if5:</code> interface on the host (like the <code>veth</code> interfaces in the case of Kindnet CNI). </p>
<p>Look at the routing table of the <code>nginx</code> pod.</p>
<pre><code class="language-sh">$ sudo nsenter -t 11380 -n ip route
default via 169.254.1.1 dev eth0
169.254.1.1 dev eth0 scope link
$
</code></pre>
<p>Exit the <code>minikube-m02</code> node and use the <code>kubectl describe nodes</code> command and filter for the PodCIDRs.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe nodes | grep -e Name -e PodCIDR
Name:               minikube
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
Name:               minikube-m02
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
  Namespace                   Name                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
Name:               minikube-m03
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
  Namespace                   Name                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
pradeep@learnk8s$
</code></pre>
<p>Currently, there is only one pod (<code>nginx</code>) in the cluster. Similar to what we have verified on the <code>minikube-m02</code> node, do the same on the other two nodes as well.</p>
<p>From the <code>minikube</code> node:</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 9e:55:51:7f:e1:98 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.6/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 82845sec preferred_lft 82845sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:92:37:51:98 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: cni-podman0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 36:85:e8:76:d2:1c brd ff:ff:ff:ff:ff:ff
    inet 10.88.0.1/16 brd 10.88.255.255 scope global cni-podman0
       valid_lft forever preferred_lft forever
6: veth5a561323@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cni-podman0 state UP group default
    link/ether f6:67:49:87:51:36 brd ff:ff:ff:ff:ff:ff link-netnsid 0
7: vethb6ed82ba@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cni-podman0 state UP group default
    link/ether 22:b5:fe:8a:37:59 brd ff:ff:ff:ff:ff:ff link-netnsid 1
8: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.120.64/32 scope global tunl0
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.6 metric 1024
10.88.0.0/16 dev cni-podman0 proto kernel scope link src 10.88.0.1
blackhole 10.244.120.64/26 proto bird
10.244.151.0/26 via 172.16.30.8 dev tunl0 proto bird onlink
10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.6
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.6 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$ exit
logout
pradeep@learnk8s$
</code></pre>
<p>On the <code>controlplane</code> node, there are extra interfaces coming from the <code>Podman CNI</code>.</p>
<p>From the <code>minikube-m03</code> node:</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m03
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether de:b8:1d:5e:d9:c0 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.8/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 83040sec preferred_lft 83040sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:66:0b:71:73 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.151.0/32 scope global tunl0
       valid_lft forever preferred_lft forever
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.8 metric 1024
10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink
blackhole 10.244.151.0/26 proto bird
10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.8
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.8 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$
</code></pre>
<p>On other nodes also, we do see similar <code>tunl0</code> interface and routes for Calico subnets (/26s) via the <code>IPIP</code> tunnel.
For the local subnet, there is a <code>blackhole</code> route: for example on the <code>minikube-m03</code> node <code>blackhole 10.244.151.0/26 proto bird</code>, and on <code>minikube-m02</code> node <code>blackhole 10.244.205.192/26 proto bird</code>, on the <code>minikube</code> node <code>blackhole 10.244.120.64/26 proto bird</code>.</p>
<p>Let us manually schedule a Pod on the <code>minikube-m03</code> node.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat my-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-manual
spec:
  nodeName: minikube-m03
  containers:
  - image: nginx
    name: nginx
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f my-pod.yaml
pod/nginx-manual created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx          1/1     Running   0          53m   10.244.205.193   minikube-m02   &lt;none&gt;           &lt;none&gt;
nginx-manual   1/1     Running   0          39s   10.244.151.1     minikube-m03   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>As expected, the <code>nginx-manual</code> pod got an IP from the <code>10.244.151.0/26</code> subnet.</p>
<p>Log back to the <code>minikube-m03</code> node and check the routing table and list of interfaces.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m03
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether de:b8:1d:5e:d9:c0 brd ff:ff:ff:ff:ff:ff
    inet 172.16.30.8/24 brd 172.16.30.255 scope global dynamic eth0
       valid_lft 82414sec preferred_lft 82414sec
3: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:66:0b:71:73 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.151.0/32 scope global tunl0
       valid_lft forever preferred_lft forever
8: califba6dd09590@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0
$ ip route
default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.8 metric 1024
10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink
blackhole 10.244.151.0/26 proto bird
10.244.151.1 dev califba6dd09590 scope link
10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink
172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.8
172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.8 metric 1024
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
$
</code></pre>
<p>Compared to the previous output (when there were no pods on this node), there is a new interface <code>8: califba6dd09590@if5:</code> and new route entry for the Pod IP <code>10.244.151.1 dev califba6dd09590 scope link</code>.</p>
<p>Get the container ID and Pid of this <code>nginx-manual</code> container.</p>
<pre><code class="language-sh">$ docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED             STATUS             PORTS     NAMES
244b50fad9d4   nginx                  &quot;/docker-entrypoint.‚Ä¶&quot;   5 minutes ago       Up 5 minutes                 k8s_nginx_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0
e7117e519923   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 5 minutes ago       Up 5 minutes                 k8s_POD_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0
c4c993bc1f2e   calico/node            &quot;start_runit&quot;            About an hour ago   Up About an hour             k8s_calico-node_calico-node-sw74l_kube-system_0e30bbad-8370-4907-ab20-ce81450ad13c_0
a3482f106e3a   9b7cc9982109           &quot;/usr/local/bin/kube‚Ä¶&quot;   About an hour ago   Up About an hour             k8s_kube-proxy_kube-proxy-7k4lb_kube-system_f284e305-e71b-40b0-a715-796d0733bc03_0
25a914d158da   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 About an hour ago   Up About an hour             k8s_POD_calico-node-sw74l_kube-system_0e30bbad-8370-4907-ab20-ce81450ad13c_0
abce35d2a0ae   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 About an hour ago   Up About an hour             k8s_POD_kube-proxy-7k4lb_kube-system_f284e305-e71b-40b0-a715-796d0733bc03_0
$
</code></pre>
<pre><code class="language-sh">$ docker inspect 244b50fad9d4 | grep Pid
            &quot;Pid&quot;: 42125,
            &quot;PidMode&quot;: &quot;&quot;,
            &quot;PidsLimit&quot;: null,
$
</code></pre>
<p>Use the <code>nsenter</code> command and verify the Pod interfaces and route table.</p>
<pre><code class="language-sh">$ nsenter -t 42125 -n ip a
nsenter: cannot open /proc/42125/ns/net: Permission denied
$ sudo nsenter -t 42125 -n ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
3: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
5: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default
    link/ether 9a:93:7b:77:8f:c7 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.151.1/32 brd 10.244.151.1 scope global eth0
       valid_lft forever preferred_lft forever
$
</code></pre>
<p>The <code>eth0</code> interface (<code>5: eth0@if8</code>) of the Pod has the IP address <code>10.244.151.1/32</code> and is mapped to the  new <code>cali</code> interface <code>8: califba6dd09590@if5:</code> on the host.</p>
<pre><code class="language-sh">$ sudo nsenter -t 42125 -n ip route
default via 169.254.1.1 dev eth0
169.254.1.1 dev eth0 scope link
$
</code></pre>
<p>Verify communication between the two Pods. <code>10.244.205.193</code> is the IP address of the <code>nginx</code> pod running on the other node <code>minikube-m02</code>.</p>
<pre><code class="language-sh">$ sudo nsenter -t 42125 -n ping 10.244.205.193
PING 10.244.205.193 (10.244.205.193): 56 data bytes
64 bytes from 10.244.205.193: seq=0 ttl=62 time=17.321 ms
64 bytes from 10.244.205.193: seq=1 ttl=62 time=0.963 ms
64 bytes from 10.244.205.193: seq=2 ttl=62 time=0.891 ms
64 bytes from 10.244.205.193: seq=3 ttl=62 time=1.529 ms
^C
--- 10.244.205.193 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max = 0.891/5.176/17.321 ms
$
</code></pre>
<p>Verify communication to the outside cluster hosts, like any internet host ( for example 8.8.8.8)</p>
<pre><code class="language-sh">$ sudo nsenter -t 42125 -n ping 8.8.8.8
PING 8.8.8.8 (8.8.8.8): 56 data bytes
64 bytes from 8.8.8.8: seq=0 ttl=115 time=20.260 ms
64 bytes from 8.8.8.8: seq=1 ttl=115 time=15.457 ms
^C
--- 8.8.8.8 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 15.457/17.858/20.260 ms
$
</code></pre>
<p>Let us install <code>calicoctl</code> as a Pod itself.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl apply -f https://projectcalico.docs.tigera.io/manifests/calicoctl.yaml
serviceaccount/calicoctl created
pod/calicoctl created
clusterrole.rbac.authorization.k8s.io/calicoctl created
clusterrolebinding.rbac.authorization.k8s.io/calicoctl created
pradeep@learnk8s$
</code></pre>
<p>Verify that the <code>calicoctl</code> pod is running.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -n kube-system -o wide
NAME                                       READY   STATUS    RESTARTS      AGE   IP            NODE           NOMINATED NODE   READINESS GATES
calico-kube-controllers-8594699699-dztlm   1/1     Running   0             82m   10.88.0.3     minikube       &lt;none&gt;           &lt;none&gt;
calico-node-gqvw6                          1/1     Running   1 (78m ago)   80m   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
calico-node-qdbcf                          1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
calico-node-sw74l                          1/1     Running   0             77m   172.16.30.8   minikube-m03   &lt;none&gt;           &lt;none&gt;
calicoctl                                  1/1     Running   0             18s   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
coredns-64897985d-58btq                    1/1     Running   0             82m   10.88.0.2     minikube       &lt;none&gt;           &lt;none&gt;
etcd-minikube                              1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-apiserver-minikube                    1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-controller-manager-minikube           1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-proxy-7k4lb                           1/1     Running   0             77m   172.16.30.8   minikube-m03   &lt;none&gt;           &lt;none&gt;
kube-proxy-gm2dh                           1/1     Running   0             80m   172.16.30.7   minikube-m02   &lt;none&gt;           &lt;none&gt;
kube-proxy-hvkqd                           1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
kube-scheduler-minikube                    1/1     Running   0             82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
storage-provisioner                        1/1     Running   1 (77m ago)   82m   172.16.30.6   minikube       &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>Verify the <code>calicoctl</code></p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get nodes -o wide
Failed to get resources: Version mismatch.
Client Version:   v3.22.1
Cluster Version:  v3.20.0
Use --allow-version-mismatch to override.

command terminated with exit code 1
pradeep@learnk8s$
</code></pre>
<p>Repeat it with <code>--allow-version-mismatch</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get nodes -o wide --allow-version-mismatch
NAME           ASN       IPV4             IPV6
minikube       (64512)   172.16.30.6/24
minikube-m02   (64512)   172.16.30.7/24
minikube-m03   (64512)   172.16.30.8/24

pradeep@learnk8s$
</code></pre>
<p>We can confirm that the <code>calicoctl</code> is working and we can see some BGP Autonomous systems shown as well. Currently all three nodes are part of the same ASN (<code>64512</code>).</p>
<p>Create an alias for the <code>calicoctl</code> </p>
<pre><code class="language-sh">pradeep@learnk8s$ alias calicoctl=&quot;kubectl exec -i -n kube-system calicoctl -- /calicoctl --allow-version-mismatch&quot;
</code></pre>
<p>By default, calicoctl will attempt to read from the Kubernetes API using the default kubeconfig located at $(HOME)/.kube/config.</p>
<p>If the default kubeconfig does not exist, or you would like to specify alternative API access information, you can do so using the following configuration options.</p>
<pre><code class="language-sh">pradeep@learnk8s$ DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes
NAME
minikube
minikube-m02
minikube-m03

pradeep@learnk8s$
</code></pre>
<p>Verify all API-Resources installed by <code>Calico</code> CNI plugin.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl api-resources | grep calico
bgpconfigurations                              crd.projectcalico.org/v1               false        BGPConfiguration
bgppeers                                       crd.projectcalico.org/v1               false        BGPPeer
blockaffinities                                crd.projectcalico.org/v1               false        BlockAffinity
clusterinformations                            crd.projectcalico.org/v1               false        ClusterInformation
felixconfigurations                            crd.projectcalico.org/v1               false        FelixConfiguration
globalnetworkpolicies                          crd.projectcalico.org/v1               false        GlobalNetworkPolicy
globalnetworksets                              crd.projectcalico.org/v1               false        GlobalNetworkSet
hostendpoints                                  crd.projectcalico.org/v1               false        HostEndpoint
ipamblocks                                     crd.projectcalico.org/v1               false        IPAMBlock
ipamconfigs                                    crd.projectcalico.org/v1               false        IPAMConfig
ipamhandles                                    crd.projectcalico.org/v1               false        IPAMHandle
ippools                                        crd.projectcalico.org/v1               false        IPPool
kubecontrollersconfigurations                  crd.projectcalico.org/v1               false        KubeControllersConfiguration
networkpolicies                                crd.projectcalico.org/v1               true         NetworkPolicy
networksets                                    crd.projectcalico.org/v1               true         NetworkSet
pradeep@learnk8s$
</code></pre>
<p>Verify some of these resources with <code>kubectl get</code> and <code>kubectl describe</code> commands.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get bgppeers -A
No resources found
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get ippools
NAME                  AGE
default-ipv4-ippool   97m
pradeep@learnk8s$ kubectl describe ippools
Name:         default-ipv4-ippool
Namespace:
Labels:       &lt;none&gt;
Annotations:  projectcalico.org/metadata: {&quot;uid&quot;:&quot;cf94cf43-8887-4528-a934-ca498f0422e1&quot;,&quot;creationTimestamp&quot;:&quot;2022-03-19T18:20:36Z&quot;}
API Version:  crd.projectcalico.org/v1
Kind:         IPPool
Metadata:
  Creation Timestamp:  2022-03-19T18:20:36Z
  Generation:          1
  Managed Fields:
    API Version:  crd.projectcalico.org/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:projectcalico.org/metadata:
      f:spec:
        .:
        f:blockSize:
        f:cidr:
        f:ipipMode:
        f:natOutgoing:
        f:nodeSelector:
        f:vxlanMode:
    Manager:         Go-http-client
    Operation:       Update
    Time:            2022-03-19T18:20:36Z
  Resource Version:  659
  UID:               8d9e076a-dae6-4bbd-8f44-920da83472ba
Spec:
  Block Size:     26
  Cidr:           10.244.0.0/16
  Ipip Mode:      Always
  Nat Outgoing:   true
  Node Selector:  all()
  Vxlan Mode:     Never
Events:           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get ipamblocks
NAME                AGE
10-244-120-64-26    98m
10-244-151-0-26     93m
10-244-205-192-26   95m
pradeep@learnk8s$
</code></pre>
<p>We have seen these IPAM blocks already, when we verified the routing tables on each node.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get networkpolicies
No resources found in default namespace.
</code></pre>
<p>Let us create a network policy now.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat network-policy.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
pradeep@learnk8s$
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f network-policy.yaml
networkpolicy.networking.k8s.io/default-deny-ingress created
</code></pre>
<p>Verify that the network policy is created.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get netpol
NAME                   POD-SELECTOR   AGE
default-deny-ingress   &lt;none&gt;         2s
</code></pre>
<p>Describe it for more details.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe netpol
Name:         default-deny-ingress
Namespace:    default
Created on:   2022-03-20 01:45:29 +0530 IST
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    &lt;none&gt; (Selected pods are isolated for ingress connectivity)
  Not affecting egress traffic
  Policy Types: Ingress
pradeep@learnk8s$ cat network-policy.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
pradeep@learnk8s$
</code></pre>
<p>If you recall, we have two pods in our cluster at the moment.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME           READY   STATUS    RESTARTS   AGE    IP               NODE           NOMINATED NODE   READINESS GATES
nginx          1/1     Running   0          105m   10.244.205.193   minikube-m02   &lt;none&gt;           &lt;none&gt;
nginx-manual   1/1     Running   0          52m    10.244.151.1     minikube-m03   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Now that the network policy is applied, let us check the connectivity between these two pods again.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m02
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ docker ps | grep nginx
0d1f5d390956   nginx                  &quot;/docker-entrypoint.‚Ä¶&quot;   2 hours ago      Up 2 hours                k8s_nginx_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0
6b67d9586b86   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 2 hours ago      Up 2 hours                k8s_POD_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0
$ docker inspect 0d1f5d390956 | grep Pid
            &quot;Pid&quot;: 11380,
            &quot;PidMode&quot;: &quot;&quot;,
            &quot;PidsLimit&quot;: null,
$ sudo nsenter -t 11380 -n ping 10.244.151.1
PING 10.244.151.1 (10.244.151.1): 56 data bytes
^C
--- 10.244.151.1 ping statistics ---
3 packets transmitted, 0 packets received, 100% packet loss
$
</code></pre>
<p>From <code>nginx</code> pod, we are not able to ping to <code>nginx-manual</code> pod.</p>
<p>Similarly, verify the other way, ping from <code>nginx-manual</code> to <code>nginx</code> Pod (which was working earlier, that we tested already!).</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m03
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ docker ps | grep nginx
244b50fad9d4   nginx                  &quot;/docker-entrypoint.‚Ä¶&quot;   55 minutes ago   Up 55 minutes             k8s_nginx_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0
e7117e519923   k8s.gcr.io/pause:3.6   &quot;/pause&quot;                 55 minutes ago   Up 55 minutes             k8s_POD_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0
$ docker inspect 244b50fad9d4 | grep Pid
            &quot;Pid&quot;: 42125,
            &quot;PidMode&quot;: &quot;&quot;,
            &quot;PidsLimit&quot;: null,
$ sudo nsenter -t 42125 -n ping 10.244.205.193
PING 10.244.205.193 (10.244.205.193): 56 data bytes
^C
--- 10.244.205.193 ping statistics ---
2 packets transmitted, 0 packets received, 100% packet loss
$ exit 0
logout
pradeep@learnk8s$
</code></pre>
<p>With network policy applied, the communication is blocked.</p>
<p>Let us create another network policy, this time to allow all ingress.</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat allow-ingress.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress

</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f allow-ingress.yaml
networkpolicy.networking.k8s.io/allow-all-ingress created
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get netpol
NAME                   POD-SELECTOR   AGE
allow-all-ingress      &lt;none&gt;         5s
default-deny-ingress   &lt;none&gt;         11m
</code></pre>
<p>Describe the new policy.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe netpol allow-all-ingress
Name:         allow-all-ingress
Namespace:    default
Created on:   2022-03-20 01:56:32 +0530 IST
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From: &lt;any&gt; (traffic not restricted by source)
  Not affecting egress traffic
  Policy Types: Ingress
pradeep@learnk8s$
</code></pre>
<p>Now that we have allowed the ingress traffic to all pods/all ports.</p>
<p>Verify again. Now that we know the Pid of the containers, those steps are not needed.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m02
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ sudo nsenter -t 11380 -n ping 10.244.151.1
PING 10.244.151.1 (10.244.151.1): 56 data bytes
64 bytes from 10.244.151.1: seq=0 ttl=62 time=3.834 ms
64 bytes from 10.244.151.1: seq=1 ttl=62 time=1.409 ms
64 bytes from 10.244.151.1: seq=2 ttl=62 time=0.723 ms
^C
--- 10.244.151.1 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.723/1.988/3.834 ms
$ exit 0
logout
</code></pre>
<p>Yay! Ping is working again.</p>
<p>Similarly, verify in the other direction.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube ssh -n minikube-m03
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$ sudo nsenter -t 42125 -n ping 10.244.205.193
PING 10.244.205.193 (10.244.205.193): 56 data bytes
64 bytes from 10.244.205.193: seq=0 ttl=62 time=1.132 ms
64 bytes from 10.244.205.193: seq=1 ttl=62 time=1.672 ms
64 bytes from 10.244.205.193: seq=2 ttl=62 time=1.695 ms
^C
--- 10.244.205.193 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 1.132/1.499/1.695 ms
$ exit 0
logout
</code></pre>
<p>This completes our initial discussion on the Calico CNI. We just scratched the surface of it, there are many more features, which will be discussed in other posts.</p>
<h3 id="kubernetes-ingress-controller">Kubernetes Ingress Controller</h3>
<h5 id="enable-ingress-addon">Enable Ingress Addon</h5>
<pre><code class="language-sh">pradeep@learnk8s$ minikube addons list
|-----------------------------|----------|--------------|--------------------------------|
|         ADDON NAME          | PROFILE  |    STATUS    |           MAINTAINER           |
|-----------------------------|----------|--------------|--------------------------------|
| ambassador                  | minikube | disabled     | third-party (ambassador)       |
| auto-pause                  | minikube | disabled     | google                         |
| csi-hostpath-driver         | minikube | disabled     | kubernetes                     |
| dashboard                   | minikube | disabled     | kubernetes                     |
| default-storageclass        | minikube | enabled ‚úÖ   | kubernetes                     |
| efk                         | minikube | disabled     | third-party (elastic)          |
| freshpod                    | minikube | disabled     | google                         |
| gcp-auth                    | minikube | disabled     | google                         |
| gvisor                      | minikube | disabled     | google                         |
| helm-tiller                 | minikube | disabled     | third-party (helm)             |
| ingress                     | minikube | disabled     | unknown (third-party)          |
| ingress-dns                 | minikube | disabled     | google                         |
| istio                       | minikube | disabled     | third-party (istio)            |
| istio-provisioner           | minikube | disabled     | third-party (istio)            |
| kong                        | minikube | disabled     | third-party (Kong HQ)          |
| kubevirt                    | minikube | disabled     | third-party (kubevirt)         |
| logviewer                   | minikube | disabled     | unknown (third-party)          |
| metallb                     | minikube | disabled     | third-party (metallb)          |
| metrics-server              | minikube | disabled     | kubernetes                     |
| nvidia-driver-installer     | minikube | disabled     | google                         |
| nvidia-gpu-device-plugin    | minikube | disabled     | third-party (nvidia)           |
| olm                         | minikube | disabled     | third-party (operator          |
|                             |          |              | framework)                     |
| pod-security-policy         | minikube | disabled     | unknown (third-party)          |
| portainer                   | minikube | disabled     | portainer.io                   |
| registry                    | minikube | disabled     | google                         |
| registry-aliases            | minikube | disabled     | unknown (third-party)          |
| registry-creds              | minikube | disabled     | third-party (upmc enterprises) |
| storage-provisioner         | minikube | enabled ‚úÖ   | google                         |
| storage-provisioner-gluster | minikube | disabled     | unknown (third-party)          |
| volumesnapshots             | minikube | disabled     | kubernetes                     |
|-----------------------------|----------|--------------|--------------------------------|
pradeep@learnk8s$
</code></pre>
<p>Enable the <code>ingress</code> addon which is currently disabled in this environment.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube addons enable ingress
    ‚ñ™ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
    ‚ñ™ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
    ‚ñ™ Using image k8s.gcr.io/ingress-nginx/controller:v1.1.1
üîé  Verifying ingress addon...
üåü  The 'ingress' addon is enabled
pradeep@learnk8s$
</code></pre>
<p>Verify the addons list again, and confirm that the <code>ingress</code> is enabled.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube addons list
|-----------------------------|----------|--------------|--------------------------------|
|         ADDON NAME          | PROFILE  |    STATUS    |           MAINTAINER           |
|-----------------------------|----------|--------------|--------------------------------|
| ambassador                  | minikube | disabled     | third-party (ambassador)       |
| auto-pause                  | minikube | disabled     | google                         |
| csi-hostpath-driver         | minikube | disabled     | kubernetes                     |
| dashboard                   | minikube | disabled     | kubernetes                     |
| default-storageclass        | minikube | enabled ‚úÖ   | kubernetes                     |
| efk                         | minikube | disabled     | third-party (elastic)          |
| freshpod                    | minikube | disabled     | google                         |
| gcp-auth                    | minikube | disabled     | google                         |
| gvisor                      | minikube | disabled     | google                         |
| helm-tiller                 | minikube | disabled     | third-party (helm)             |
| ingress                     | minikube | enabled ‚úÖ   | unknown (third-party)          |
| ingress-dns                 | minikube | disabled     | google                         |
| istio                       | minikube | disabled     | third-party (istio)            |
| istio-provisioner           | minikube | disabled     | third-party (istio)            |
| kong                        | minikube | disabled     | third-party (Kong HQ)          |
| kubevirt                    | minikube | disabled     | third-party (kubevirt)         |
| logviewer                   | minikube | disabled     | unknown (third-party)          |
| metallb                     | minikube | disabled     | third-party (metallb)          |
| metrics-server              | minikube | disabled     | kubernetes                     |
| nvidia-driver-installer     | minikube | disabled     | google                         |
| nvidia-gpu-device-plugin    | minikube | disabled     | third-party (nvidia)           |
| olm                         | minikube | disabled     | third-party (operator          |
|                             |          |              | framework)                     |
| pod-security-policy         | minikube | disabled     | unknown (third-party)          |
| portainer                   | minikube | disabled     | portainer.io                   |
| registry                    | minikube | disabled     | google                         |
| registry-aliases            | minikube | disabled     | unknown (third-party)          |
| registry-creds              | minikube | disabled     | third-party (upmc enterprises) |
| storage-provisioner         | minikube | enabled ‚úÖ   | google                         |
| storage-provisioner-gluster | minikube | disabled     | unknown (third-party)          |
| volumesnapshots             | minikube | disabled     | kubernetes                     |
|-----------------------------|----------|--------------|--------------------------------|
pradeep@learnk8s$
</code></pre>
<p>Verify all resources that got created as part of this addon installation.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -n ingress-nginx
NAME                                       READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-cvwv7       0/1     Completed   0          65s
ingress-nginx-admission-patch-zsrq4        0/1     Completed   1          65s
ingress-nginx-controller-cc8496874-6zr4h   1/1     Running     0          65s
pradeep@learnk8s$ kubectl get all -n ingress-nginx
NAME                                           READY   STATUS      RESTARTS   AGE
pod/ingress-nginx-admission-create-cvwv7       0/1     Completed   0          83s
pod/ingress-nginx-admission-patch-zsrq4        0/1     Completed   1          83s
pod/ingress-nginx-controller-cc8496874-6zr4h   1/1     Running     0          83s

NAME                                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/ingress-nginx-controller             NodePort    10.101.85.4      &lt;none&gt;        80:30419/TCP,443:30336/TCP   83s
service/ingress-nginx-controller-admission   ClusterIP   10.104.163.136   &lt;none&gt;        443/TCP                      83s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ingress-nginx-controller   1/1     1            1           83s

NAME                                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/ingress-nginx-controller-cc8496874   1         1         1       83s

NAME                                       COMPLETIONS   DURATION   AGE
job.batch/ingress-nginx-admission-create   1/1           10s        83s
job.batch/ingress-nginx-admission-patch    1/1           11s        83s
pradeep@learnk8s$ 
</code></pre>
<h4 id="deploy-a-hello-world-app">Deploy a Hello, World App!</h4>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0
deployment.apps/web created
pradeep@learnk8s$
</code></pre>
<p>Expose the deployment as  a NodePort Service on port number 8080.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl expose deployment web --type=NodePort -
-port=8080
service/web exposed
pradeep@learnk8s$
</code></pre>
<p>Verify that the service is created properly.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get service web
NAME   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
web    NodePort   10.105.47.136   &lt;none&gt;        8080:31270/TCP   11s
pradeep@learnk8s$
</code></pre>
<p>Get the URL for this newly created service.</p>
<pre><code class="language-sh">pradeep@learnk8s$ minikube service web --url
http://172.16.30.6:31270
pradeep@learnk8s$
</code></pre>
<p>Access the URL</p>
<pre><code class="language-sh">pradeep@learnk8s$ curl http://172.16.30.6:31270
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
pradeep@learnk8s$
</code></pre>
<h4 id="create-an-ingress">Create an Ingress</h4>
<pre><code class="language-yaml">pradeep@learnk8s$ cat example-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: hello-world.info
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web
                port:
                  number: 8080
pradeep@learnk8s$
</code></pre>
<p>Create the Ingress using the above definition.</p>
<pre><code class="language-sh">pradeep@learnk8s$kubectl apply -f example-ingress.yaml
ingress.networking.k8s.io/example-ingress created
pradeep@learnk8s$
</code></pre>
<p>Verify that the Ingress is created.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get ingress
NAME              CLASS   HOSTS              ADDRESS       PORTS   AGE
example-ingress   nginx   hello-world.info   172.16.30.6   80      87s
pradeep@learnk8s$
</code></pre>
<p>Add the <code>172.16.30.6    hello-world.info</code> to the bottom of the /etc/hosts file on your computer (you will need administrator access):</p>
<pre><code class="language-sh">pradeep@learnk8s$ cat /etc/hosts
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1   localhost
255.255.255.255 broadcasthost
::1             localhost
# Added by Docker Desktop
# To allow the same kube context to work on the host and the container:
127.0.0.1 kubernetes.docker.internal
# End of section
172.16.30.6 hello-world.info
pradeep@learnk8s$
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ curl hello-world.info
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
pradeep@learnk8s$
</code></pre>
<h4 id="create-a-second-deployment">Create a second Deployment</h4>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create deployment web2 --image=gcr.io/google-samples/hello-app:2.0
deployment.apps/web2 created
pradeep@learnk8s$
</code></pre>
<p>Expose the deployment.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl expose deployment web2 --port=8080 --type=NodePort
service/web2 exposed
pradeep@learnk8s$
</code></pre>
<p>Edit the existing <code>example-ingress.yaml</code> manifest, and add the following lines at the end:</p>
<pre><code class="language-yaml">pradeep@learnk8s$ cat example-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: hello-world.info
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web
                port:
                  number: 8080
          - path: /v2
            pathType: Prefix
            backend:
              service:
                name: web2
                port:
                  number: 8080
pradeep@learnk8s$
</code></pre>
<p>Apply the changes.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl apply -f example-ingress.yaml
ingress.networking.k8s.io/example-ingress configured
pradeep@learnk8s$
</code></pre>
<h4 id="test-your-ingress">Test your Ingress</h4>
<pre><code class="language-sh">pradeep@learnk8s$ curl hello-world.info
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
pradeep@learnk8s$
</code></pre>
<p>Access the 2nd version of the Hello World app.</p>
<pre><code class="language-sh">pradeep@learnk8s$ curl hello-world.info/v2
Hello, world!
Version: 2.0.0
Hostname: web2-5858b4c7c5-krt4l
pradeep@learnk8s$
</code></pre>
<pre><code>pradeep@learnk8s$ kubectl get ingress
NAME              CLASS   HOSTS              ADDRESS       PORTS   AGE
example-ingress   nginx   hello-world.info   172.16.30.6   80      11m
pradeep@learnk8s$
</code></pre>
<p>Let us describe the ingress to see how it looks like.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe ingress
Name:             example-ingress
Namespace:        default
Address:          172.16.30.6
Default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)
Rules:
  Host              Path  Backends
  ----              ----  --------
  hello-world.info
                    /     web:8080 (10.244.205.195:8080)
                    /v2   web2:8080 (10.244.151.3:8080)
Annotations:        nginx.ingress.kubernetes.io/rewrite-target: /$1
Events:
  Type    Reason  Age                From                      Message
  ----    ------  ----               ----                      -------
  Normal  Sync    12m (x3 over 20m)  nginx-ingress-controller  Scheduled for sync
pradeep@learnk8s$
</code></pre>
<p>We can see one host (<code>hello-world.info</code>) and two paths (<code>/</code> and <code>/v2</code>), two backends (<code>web:8080</code> and <code>web2:8080</code>). These two services point to the respective Pods.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
web-746c8679d4-nfkcw    1/1     Running   0          75m   10.244.205.195   minikube-m02   &lt;none&gt;           &lt;none&gt;
web2-5858b4c7c5-krt4l   1/1     Running   0          66m   10.244.151.3     minikube-m03   &lt;none&gt;           &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>What happens to the requests sent to a path different from both of these (<code>/</code> and <code>/v2</code>)?
Those requests are handled by the default backend.</p>
<p>From the above description, we can see there is a default backend <code>default-http-backend:80</code> which is not defined yet, so there is an error.
Let us test few undefined paths like (<code>/v3</code>, <code>/v5</code> etc)</p>
<pre><code class="language-sh">pradeep@learnk8s$ curl hello-world.info/v3
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
pradeep@learnk8s$ curl hello-world.info/v5
Hello, world!
Version: 1.0.0
Hostname: web-746c8679d4-nfkcw
</code></pre>
<p>It looks like all these undefined paths are handled by the <code>web:8080</code> backend only. According to Kubernetes documentation, If <code>defaultBackend</code> is not set, the handling of requests that do not match any of the rules will be up to the ingress controller (consult the documentation for your ingress controller to find out how it handles this case).</p>
<p>If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.</p>
<p>Let us edit the ingress, to add a defaultBackend, in this case , change it to <code>web2</code>.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl edit ingress example-ingress
ingress.networking.k8s.io/example-ingress edited
</code></pre>
<p>Describe it again, to see the changes.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe ingress
Name:             example-ingress
Namespace:        default
Address:          172.16.30.6
Default backend:  web2:8080 (10.244.151.3:8080)
Rules:
  Host              Path  Backends
  ----              ----  --------
  hello-world.info
                    /     web:8080 (10.244.205.195:8080)
                    /v2   web2:8080 (10.244.151.3:8080)
Annotations:        nginx.ingress.kubernetes.io/rewrite-target: /$1
Events:
  Type    Reason  Age                From                      Message
  ----    ------  ----               ----                      -------
  Normal  Sync    11s (x4 over 78m)  nginx-ingress-controller  Scheduled for sync
pradeep@learnk8s$
</code></pre>
<p>This line <code>Default backend:  web2:8080 (10.244.151.3:8080)</code> confirms our change.
List out all the endpoints.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get ep
NAME         ENDPOINTS             AGE
kubernetes   172.16.30.6:8443      47h
web          10.244.205.195:8080   87m
web2         10.244.151.3:8080     78m
pradeep@learnk8s$
</code></pre>
<p>Let us look at the ingress controller logs.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl -n ingress-nginx logs ingress-nginx-controller-cc8496874-6zr4h
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.1.1
  Build:         a17181e43ec85534a6fea968d95d019c5a4bc8cf
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.19.9

-------------------------------------------------------------------------------

W0321 15:42:29.028539       9 client_config.go:615] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0321 15:42:29.028983       9 main.go:223] &quot;Creating API client&quot; host=&quot;https://10.96.0.1:443&quot;
I0321 15:42:29.057278       9 main.go:267] &quot;Running in Kubernetes cluster&quot; major=&quot;1&quot; minor=&quot;23&quot; git=&quot;v1.23.3&quot; state=&quot;clean&quot; commit=&quot;816c97ab8cff8a1c72eccca1026f7820e93e0d25&quot; platform=&quot;linux/amd64&quot;
I0321 15:42:29.317187       9 main.go:104] &quot;SSL fake certificate created&quot; file=&quot;/etc/ingress-controller/ssl/default-fake-certificate.pem&quot;
I0321 15:42:29.374201       9 ssl.go:531] &quot;loading tls certificate&quot; path=&quot;/usr/local/certificates/cert&quot; key=&quot;/usr/local/certificates/key&quot;
I0321 15:42:29.419869       9 nginx.go:255] &quot;Starting NGINX Ingress controller&quot;
I0321 15:42:29.437178       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;ConfigMap&quot;, Namespace:&quot;ingress-nginx&quot;, Name:&quot;ingress-nginx-controller&quot;, UID:&quot;d14f8541-76a0-4357-b9ad-38c7ce8f6586&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;12223&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0321 15:42:29.442938       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;ConfigMap&quot;, Namespace:&quot;ingress-nginx&quot;, Name:&quot;tcp-services&quot;, UID:&quot;9cffa0f1-6754-4bfd-bf78-6cae4febc393&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;12224&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0321 15:42:29.443183       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;ConfigMap&quot;, Namespace:&quot;ingress-nginx&quot;, Name:&quot;udp-services&quot;, UID:&quot;3ac9b32a-17b9-4824-bc0b-51a609648e9c&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;12225&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0321 15:42:30.622870       9 nginx.go:297] &quot;Starting NGINX process&quot;
I0321 15:42:30.623750       9 leaderelection.go:248] attempting to acquire leader lease ingress-nginx/ingress-controller-leader...
I0321 15:42:30.624864       9 controller.go:155] &quot;Configuration changes detected, backend reload required&quot;
I0321 15:42:30.623967       9 nginx.go:317] &quot;Starting validation webhook&quot; address=&quot;:8443&quot; certPath=&quot;/usr/local/certificates/cert&quot; keyPath=&quot;/usr/local/certificates/key&quot;
I0321 15:42:30.666454       9 leaderelection.go:258] successfully acquired lease ingress-nginx/ingress-controller-leader
I0321 15:42:30.667675       9 status.go:84] &quot;New leader elected&quot; identity=&quot;ingress-nginx-controller-cc8496874-6zr4h&quot;
I0321 15:42:30.700891       9 status.go:214] &quot;POD is not ready&quot; pod=&quot;ingress-nginx/ingress-nginx-controller-cc8496874-6zr4h&quot; node=&quot;minikube&quot;
I0321 15:42:30.884747       9 controller.go:172] &quot;Backend successfully reloaded&quot;
I0321 15:42:30.885032       9 controller.go:183] &quot;Initial sync, sleeping for 1 second&quot;
I0321 15:42:30.885722       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;Pod&quot;, Namespace:&quot;ingress-nginx&quot;, Name:&quot;ingress-nginx-controller-cc8496874-6zr4h&quot;, UID:&quot;de6796ef-94a6-499c-b9b5-418c39cc62c0&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;12337&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0321 15:56:40.753346       9 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.412s renderingIngressLength:1 renderingIngressTime:0.009s admissionTime:18.0kBs testedConfigurationSize:0.421}
I0321 15:56:40.755944       9 main.go:101] &quot;successfully validated configuration, accepting&quot; ingress=&quot;default/example-ingress&quot;
I0321 15:56:40.794396       9 store.go:424] &quot;Found valid IngressClass&quot; ingress=&quot;default/example-ingress&quot; ingressclass=&quot;nginx&quot;
I0321 15:56:40.796827       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;Ingress&quot;, Namespace:&quot;default&quot;, Name:&quot;example-ingress&quot;, UID:&quot;51471ff6-1982-47da-85ee-a9356165df9f&quot;, APIVersion:&quot;networking.k8s.io/v1&quot;, ResourceVersion:&quot;13112&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0321 15:56:40.802072       9 controller.go:155] &quot;Configuration changes detected, backend reload required&quot;
I0321 15:56:40.979390       9 controller.go:172] &quot;Backend successfully reloaded&quot;
I0321 15:56:40.988286       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;Pod&quot;, Namespace:&quot;ingress-nginx&quot;, Name:&quot;ingress-nginx-controller-cc8496874-6zr4h&quot;, UID:&quot;de6796ef-94a6-499c-b9b5-418c39cc62c0&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;12337&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0321 15:57:28.297662       9 status.go:299] &quot;updating Ingress status&quot; namespace=&quot;default&quot; ingress=&quot;example-ingress&quot; currentValue=[] newValue=[{IP:172.16.30.6 Hostname: Ports:[]}]
I0321 15:57:28.322904       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;Ingress&quot;, Namespace:&quot;default&quot;, Name:&quot;example-ingress&quot;, UID:&quot;51471ff6-1982-47da-85ee-a9356165df9f&quot;, APIVersion:&quot;networking.k8s.io/v1&quot;, ResourceVersion:&quot;13164&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'Sync' Scheduled for sync
172.16.30.1 - - [21/Mar/2022:16:00:09 +0000] &quot;GET / HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 80 0.009 [default-web-8080] [] 10.244.205.195:8080 60 0.009 200 766ea45be48fc8dcb53c425eb3800ab2
I0321 16:04:49.720933       9 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.161s renderingIngressLength:1 renderingIngressTime:0s admissionTime:21.7kBs testedConfigurationSize:0.161}
I0321 16:04:49.721038       9 main.go:101] &quot;successfully validated configuration, accepting&quot; ingress=&quot;default/example-ingress&quot;
I0321 16:04:49.733433       9 controller.go:155] &quot;Configuration changes detected, backend reload required&quot;
I0321 16:04:49.734529       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;Ingress&quot;, Namespace:&quot;default&quot;, Name:&quot;example-ingress&quot;, UID:&quot;51471ff6-1982-47da-85ee-a9356165df9f&quot;, APIVersion:&quot;networking.k8s.io/v1&quot;, ResourceVersion:&quot;13649&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0321 16:04:49.911585       9 controller.go:172] &quot;Backend successfully reloaded&quot;
I0321 16:04:49.914092       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;Pod&quot;, Namespace:&quot;ingress-nginx&quot;, Name:&quot;ingress-nginx-controller-cc8496874-6zr4h&quot;, UID:&quot;de6796ef-94a6-499c-b9b5-418c39cc62c0&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;12337&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
172.16.30.1 - - [21/Mar/2022:16:05:30 +0000] &quot;GET / HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 80 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 c3d48047b7ef7ac70326098b25f1e6c1
172.16.30.1 - - [21/Mar/2022:16:05:33 +0000] &quot;GET /v2 HTTP/1.1&quot; 200 61 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.013 [default-web2-8080] [] 10.244.151.3:8080 61 0.013 200 e90cf46a8385a64ac01c834734768488
172.16.30.1 - - [21/Mar/2022:16:32:44 +0000] &quot;GET /v2 HTTP/1.1&quot; 200 61 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.114 [default-web2-8080] [] 10.244.151.3:8080 61 0.114 200 cb87a3a8abd12099f7db64a2ec83c681
172.16.30.1 - - [21/Mar/2022:17:03:17 +0000] &quot;GET /v3 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.040 [default-web-8080] [] 10.244.205.195:8080 60 0.041 200 f794e2a783715ac73ff7177f60e9dd68
172.16.30.1 - - [21/Mar/2022:17:03:21 +0000] &quot;GET /v5 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.016 [default-web-8080] [] 10.244.205.195:8080 60 0.017 200 095cbb53ca04bdb4359fa7b745e5a809
172.16.30.1 - - [21/Mar/2022:17:03:29 +0000] &quot;GET /v2 HTTP/1.1&quot; 200 61 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web2-8080] [] 10.244.151.3:8080 61 0.002 200 cbc0c654a89c5f98f18a5102ddf61b0e
172.16.30.1 - - [21/Mar/2022:17:03:33 +0000] &quot;GET /1 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 81 0.008 [default-web-8080] [] 10.244.205.195:8080 60 0.008 200 767955b2b4eed7791912b2a843363b28
172.16.30.1 - - [21/Mar/2022:17:05:26 +0000] &quot;GET /1 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 81 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 0ec584df780661bccfa10731bcc70524
172.16.30.1 - - [21/Mar/2022:17:05:29 +0000] &quot;GET / HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 80 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 5ecb91527d77741daf16c17a8d79a462
172.16.30.1 - - [21/Mar/2022:17:05:31 +0000] &quot;GET /as HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.006 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 034643529f3104fef19915593168b200
I0321 17:14:53.079003       9 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.41s renderingIngressLength:1 renderingIngressTime:0.007s admissionTime:21.7kBs testedConfigurationSize:0.417}
I0321 17:14:53.079746       9 main.go:101] &quot;successfully validated configuration, accepting&quot; ingress=&quot;default/example-ingress&quot;
I0321 17:14:53.099157       9 controller.go:155] &quot;Configuration changes detected, backend reload required&quot;
I0321 17:14:53.099689       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;Ingress&quot;, Namespace:&quot;default&quot;, Name:&quot;example-ingress&quot;, UID:&quot;51471ff6-1982-47da-85ee-a9356165df9f&quot;, APIVersion:&quot;networking.k8s.io/v1&quot;, ResourceVersion:&quot;15050&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0321 17:14:53.225350       9 controller.go:172] &quot;Backend successfully reloaded&quot;
I0321 17:14:53.227604       9 event.go:282] Event(v1.ObjectReference{Kind:&quot;Pod&quot;, Namespace:&quot;ingress-nginx&quot;, Name:&quot;ingress-nginx-controller-cc8496874-6zr4h&quot;, UID:&quot;de6796ef-94a6-499c-b9b5-418c39cc62c0&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;12337&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
172.16.30.1 - - [21/Mar/2022:17:16:53 +0000] &quot;GET /v5 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.009 [default-web-8080] [] 10.244.205.195:8080 60 0.009 200 2e8ee25ad012ea3d5ac6fb4646f37d61
172.16.30.1 - - [21/Mar/2022:17:16:57 +0000] &quot;GET /v3 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 70d9d27891c2b97a6bcda5e315fa3691
172.16.30.1 - - [21/Mar/2022:17:17:01 +0000] &quot;GET /v2 HTTP/1.1&quot; 200 61 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.010 [default-web2-8080] [] 10.244.151.3:8080 61 0.009 200 297effa7825bef53d93e6dce4e5d9d62
172.16.30.1 - - [21/Mar/2022:17:17:02 +0000] &quot;GET /v1 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 421c9cb655e07183d9a7d1a89562db97
172.16.30.1 - - [21/Mar/2022:17:17:29 +0000] &quot;GET /v1 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 3bff305b8c1b782d08acf2de386076d9
172.16.30.1 - - [21/Mar/2022:17:17:31 +0000] &quot;GET /v3 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.001 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 e3dca522b7470345325f5dcda8adfc8e
172.16.30.1 - - [21/Mar/2022:17:17:34 +0000] &quot;GET /v5 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 207b1c9f4b52683061bb0807857c32f6
172.16.30.1 - - [21/Mar/2022:17:17:36 +0000] &quot;GET /v6 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 f2d0ae4264c26168246099ee90cc9f27
172.16.30.1 - - [21/Mar/2022:17:17:39 +0000] &quot;GET /vdfs HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 84 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.001 200 b08773ef6c074b4b10fee5626457df47
172.16.30.1 - - [21/Mar/2022:17:17:40 +0000] &quot;GET /vdfs HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 84 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 cd5a39f9d32b212bf97b41c57f7b429b
172.16.30.1 - - [21/Mar/2022:17:18:58 +0000] &quot;GET /vdfs HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 84 0.009 [default-web-8080] [] 10.244.205.195:8080 60 0.008 200 f413dd82a31c72df36114843c5054aad
172.16.30.1 - - [21/Mar/2022:17:18:59 +0000] &quot;GET /vdfs HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 84 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 09a92db2773acffcd23b31c856673a9b
172.16.30.1 - - [21/Mar/2022:17:19:12 +0000] &quot;GET /v1 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.001 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 c21e9af32944af9d9d70b39eaedbe11c
172.16.30.1 - - [21/Mar/2022:17:19:14 +0000] &quot;GET /v HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 81 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 5daf84fe1f8992ff8d9d0cdec4b751c1
172.16.30.1 - - [21/Mar/2022:17:19:17 +0000] &quot;GET /v2 HTTP/1.1&quot; 200 61 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.007 [default-web2-8080] [] 10.244.151.3:8080 61 0.008 200 52df2d3780d65d11e34a545583958a17
172.16.30.1 - - [21/Mar/2022:17:20:32 +0000] &quot;GET /v2 HTTP/1.1&quot; 200 61 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.003 [default-web2-8080] [] 10.244.151.3:8080 61 0.003 200 d05940e016c2b36ce79996c04065b5a8
172.16.30.1 - - [21/Mar/2022:17:20:34 +0000] &quot;GET /v2f HTTP/1.1&quot; 200 61 &quot;-&quot; &quot;curl/7.77.0&quot; 83 0.003 [default-web2-8080] [] 10.244.151.3:8080 61 0.002 200 c59de52e577935709573bf0515e999e2
172.16.30.1 - - [21/Mar/2022:17:20:36 +0000] &quot;GET /v3 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 3354dc569894182a6ee5a5f69892ca66
172.16.30.1 - - [21/Mar/2022:17:20:40 +0000] &quot;GET /v35 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 83 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 75cccfe798f3a6f2ee072c9233c8e3b7
172.16.30.1 - - [21/Mar/2022:17:20:44 +0000] &quot;GET /v2a HTTP/1.1&quot; 200 61 &quot;-&quot; &quot;curl/7.77.0&quot; 83 0.004 [default-web2-8080] [] 10.244.151.3:8080 61 0.004 200 8ae6170f4a95446d0615bfc5d4a16403
172.16.30.1 - - [21/Mar/2022:17:20:48 +0000] &quot;GET /dg HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 c4cea1304f930170d7a7148fc2ac04a7
172.16.30.1 - - [21/Mar/2022:17:21:49 +0000] &quot;GET /dg HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 b860ca473d1b622514dd4ebf194311f2
172.16.30.1 - - [21/Mar/2022:17:21:50 +0000] &quot;GET /dg HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 1f8bf150a942fb5272d01d9a82c6fd77
172.16.30.1 - - [21/Mar/2022:17:21:51 +0000] &quot;GET /dg HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 2a2c3eec3ebe85dd546ea35cf92f1357
172.16.30.1 - - [21/Mar/2022:17:21:52 +0000] &quot;GET /dg HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.005 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 fca07770b107dfb25126f60a1f4dabcc
172.16.30.1 - - [21/Mar/2022:17:21:52 +0000] &quot;GET /dg HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 421ddaffd339aafd3ac912c195035b7e
172.16.30.1 - - [21/Mar/2022:17:22:41 +0000] &quot;GET /dg HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 58a2dc6300d892b640bd26ab007daba8
172.16.30.1 - - [21/Mar/2022:17:22:42 +0000] &quot;GET /dg HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 402a0a5fd8037182ea8671d3d9347ee7
172.16.30.1 - - [21/Mar/2022:17:22:44 +0000] &quot;GET /df HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 4cb6f78cb734373510f98f669cdf253d
172.16.30.1 - - [21/Mar/2022:17:22:46 +0000] &quot;GET /dfdg HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 84 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.001 200 2fd8c843e0fac657bc80e2a6b23dcaf6
172.16.30.1 - - [21/Mar/2022:17:23:52 +0000] &quot;GET /v7 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 5aaabd3963132e95499b5be8b57368e6
172.16.30.1 - - [21/Mar/2022:17:23:54 +0000] &quot;GET /v9 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 452ce52c0ab19f2fdc37152f925887f7
172.16.30.1 - - [21/Mar/2022:17:24:28 +0000] &quot;GET /v9 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 faf0e7f8be1b66b38f6747c7df815525
172.16.30.1 - - [21/Mar/2022:17:24:30 +0000] &quot;GET /v9 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 c4526f5d5a9fab7245b1d77becf24bb7
172.16.30.1 - - [21/Mar/2022:17:24:31 +0000] &quot;GET /v9 HTTP/1.1&quot; 200 60 &quot;-&quot; &quot;curl/7.77.0&quot; 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 47fd2dd3ed81e5fa3762687343ed06c0
pradeep@learnk8s$
</code></pre>
<h3 id="kubernetes-dns">Kubernetes DNS</h3>
<p>When we setup our minikube cluster, it is configured to use the CoreDNS addon or its precursor, kube-dns.</p>
<p>We can verify the same.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS        AGE
calico-kube-controllers-8594699699-dztlm   1/1     Running   2 (120m ago)    47h
calico-node-gqvw6                          1/1     Running   2 (119m ago)    47h
calico-node-qdbcf                          1/1     Running   1 (120m ago)    47h
calico-node-sw74l                          1/1     Running   1 (118m ago)    47h
coredns-64897985d-58btq                    1/1     Running   2 (120m ago)    47h
etcd-minikube                              1/1     Running   1 (120m ago)    47h
kube-apiserver-minikube                    1/1     Running   3 (120m ago)    47h
kube-controller-manager-minikube           1/1     Running   3 (120m ago)    47h
kube-proxy-7k4lb                           1/1     Running   1 (118m ago)    47h
kube-proxy-gm2dh                           1/1     Running   1 (119m ago)    47h
kube-proxy-hvkqd                           1/1     Running   1 (120m ago)    47h
kube-scheduler-minikube                    1/1     Running   2 (120m ago)    47h
storage-provisioner                        1/1     Running   10 (107m ago)   47h
</code></pre>
<p>Describe the coredns pod, to get additional information.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe pods coredns-64897985d-58btq -n kube-system
Name:                 coredns-64897985d-58btq
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 minikube/172.16.30.6
Start Time:           Sat, 19 Mar 2022 23:48:56 +0530
Labels:               k8s-app=kube-dns
                      pod-template-hash=64897985d
Annotations:          &lt;none&gt;
Status:               Running
IP:                   10.88.0.4
IPs:
  IP:           10.88.0.4
Controlled By:  ReplicaSet/coredns-64897985d
Containers:
  coredns:
    Container ID:  docker://c866d7b12f5f086407e04f3fd4e500889f860bfd55790e1e2a4870fad1052330
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      docker-pullable://k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Mon, 21 Mar 2022 21:07:03 +0530
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Sun, 20 Mar 2022 19:25:02 +0530
      Finished:     Mon, 21 Mar 2022 21:06:40 +0530
    Ready:          True
    Restart Count:  2
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  &lt;none&gt;
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rhcln (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-rhcln:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                  From     Message
  ----     ------     ----                 ----     -------
  Warning  Unhealthy  107m                 kubelet  Liveness probe failed: Get &quot;http://10.88.0.4:8080/health&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  107m (x2 over 107m)  kubelet  Readiness probe failed: Get &quot;http://10.88.0.4:8181/ready&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)
pradeep@learnk8s$
</code></pre>
<h4 id="create-a-simple-pod-to-use-as-a-test-environment">Create a simple Pod to use as a test environment</h4>
<pre><code class="language-yaml">pradeep@learnk8s$ cat dnsutils.yaml
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3
    command:
      - sleep
      - &quot;3600&quot;
    imagePullPolicy: IfNotPresent
  restartPolicy: Always

pradeep@learnk8s$
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl create -f dnsutils.yaml
pod/dnsutils created
pradeep@learnk8s$
</code></pre>
<p>Once that Pod is running, you can exec <code>nslookup</code> in that environment.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup kubernetes.default
;; connection timed out; no servers could be reached

command terminated with exit code 1
pradeep@learnk8s$
</code></pre>
<p>As the nslookup command failed, check the following:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          47h
web          NodePort    10.105.47.136   &lt;none&gt;        8080:31270/TCP   111m
web2         NodePort    10.106.73.151   &lt;none&gt;        8080:31691/TCP   102m
</code></pre>
<p><code>kubernetes</code> service is running the <code>default</code> namespace.
Check the <code>kube-dns</code> service in the <code>kube-system</code> namespace.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get svc -n kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   47h
pradeep@learnk8s$
</code></pre>
<p>It is running fine. Let us check what <code>nameserver</code> is being used by this Pod.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -ti dnsutils -- cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
</code></pre>
<p>This confirms that the <code>dnsutils</code> pod is using the correct nameserver.</p>
<p>Are DNS endpoints exposed?
You can verify that DNS endpoints are exposed by using the <code>kubectl get endpoints</code> command.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get ep kube-dns -n kube-system
NAME       ENDPOINTS                                  AGE
kube-dns   10.88.0.4:53,10.88.0.4:53,10.88.0.4:9153   47h
pradeep@learnk8s$
</code></pre>
<p>Hmm, the endpoints are coming from the Podman CNI but not from the Calico CNI that we are using in this cluster. You remember the race condition issue  between Calico and Podman CNIs?!</p>
<p>Let us get the replicaset ID for the coredns service and delete it.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get rs -n kube-system
NAME                                 DESIRED   CURRENT   READY   AGE
calico-kube-controllers-8594699699   1         1         1       47h
coredns-64897985d                    1         1         1       47h
</code></pre>
<p>Deleting the replicaset</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl delete rs coredns-64897985d -n kube-system
replicaset.apps &quot;coredns-64897985d&quot; deleted
pradeep@learnk8s$
</code></pre>
<p>Immediately, the replicaset got re-created. Check the AGE!</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get rs -n kube-system
NAME                                 DESIRED   CURRENT   READY   AGE
calico-kube-controllers-8594699699   1         1         1       47h
coredns-64897985d                    1         1         0       2s
pradeep@learnk8s$
</code></pre>
<p>Let us check the endpoints again.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get ep kube-dns -n kube-system
NAME       ENDPOINTS                                                 AGE
kube-dns   10.244.205.196:53,10.244.205.196:53,10.244.205.196:9153   47h
pradeep@learnk8s$
</code></pre>
<p>Now that the endpoints are correctly pointing to the Calico CNI IP addresses, verify the nslookup again.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -ti dnsutils -- cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
</code></pre>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup kubernetes.default
Server:     10.96.0.10
Address:    10.96.0.10#53

Name:   kubernetes.default.svc.cluster.local
Address: 10.96.0.1

pradeep@learnk8s$
</code></pre>
<p>Just to verify the coredns status.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS        AGE
calico-kube-controllers-8594699699-dztlm   1/1     Running   2 (132m ago)    47h
calico-node-gqvw6                          1/1     Running   2 (131m ago)    47h
calico-node-qdbcf                          1/1     Running   1 (132m ago)    47h
calico-node-sw74l                          1/1     Running   1 (130m ago)    47h
coredns-64897985d-kwp95                    1/1     Running   0               74s
etcd-minikube                              1/1     Running   1 (132m ago)    47h
kube-apiserver-minikube                    1/1     Running   3 (132m ago)    47h
kube-controller-manager-minikube           1/1     Running   3 (132m ago)    47h
kube-proxy-7k4lb                           1/1     Running   1 (130m ago)    47h
kube-proxy-gm2dh                           1/1     Running   1 (131m ago)    47h
kube-proxy-hvkqd                           1/1     Running   1 (132m ago)    47h
kube-scheduler-minikube                    1/1     Running   2 (132m ago)    47h
storage-provisioner                        1/1     Running   10 (118m ago)   47h
pradeep@learnk8s$
</code></pre>
<p>Are DNS queries being received/processed?
You can verify if queries are being received by CoreDNS by adding the <code>log</code> plugin to the CoreDNS configuration (aka Corefile). The CoreDNS Corefile is held in a ConfigMap named <code>coredns</code>. </p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get cm -n kube-system
NAME                                 DATA   AGE
calico-config                        4      47h
coredns                              1      47h
extension-apiserver-authentication   6      47h
kube-proxy                           2      47h
kube-root-ca.crt                     1      47h
kubeadm-config                       1      47h
kubelet-config-1.23                  1      47h
pradeep@learnk8s$
</code></pre>
<p>Describe the config map.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe cm coredns -n kube-system
Name:         coredns
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
Corefile:
----
.:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    hosts {
       192.168.64.1 host.minikube.internal
       fallthrough
    }
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30
    loop
    reload
    loadbalance
}


BinaryData
====

Events:  &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>Edit this configmap and add the <code>log</code> plugin.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl edit cm coredns -n kube-system
configmap/coredns edited
pradeep@learnk8s$
</code></pre>
<p>Describe it again and verify the change</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl describe cm coredns -n kube-system
Name:         coredns
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
Corefile:
----
.:53 {
    log
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    hosts {
       192.168.64.1 host.minikube.internal
       fallthrough
    }
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30
    loop
    reload
    loadbalance
}


BinaryData
====

Events:  &lt;none&gt;
pradeep@learnk8s$
</code></pre>
<p>After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods.</p>
<p>Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95
.:53
[INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] Reloading
[INFO] plugin/health: Going into lameduck mode for 5s
pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup kubernetes.default
Server:     10.96.0.10
Address:    10.96.0.10#53

Name:   kubernetes.default.svc.cluster.local
Address: 10.96.0.1

pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95
.:53
[INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] Reloading
[INFO] plugin/health: Going into lameduck mode for 5s
[INFO] plugin/reload: Running configuration MD5 = 07d7c5ad4525bf2c472eaef020d0184d
[INFO] Reloading complete
[INFO] 127.0.0.1:39552 - 1628 &quot;HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512&quot; NXDOMAIN qr,rd,ra 132 1.058936054s
[INFO] 10.244.151.4:41947 - 42256 &quot;A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512&quot; NXDOMAIN qr,aa,rd 155 0.000469847s
[INFO] 10.244.151.4:55247 - 43436 &quot;A IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NOERROR qr,aa,rd 106 0.000357148s
pradeep@learnk8s$
</code></pre>
<p>Let us make some more queries,</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup web.default
Server:     10.96.0.10
Address:    10.96.0.10#53

Name:   web.default.svc.cluster.local
Address: 10.105.47.136

pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup web2.default
Server:     10.96.0.10
Address:    10.96.0.10#53

Name:   web2.default.svc.cluster.local
Address: 10.106.73.151

pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup web3.default
Server:     10.96.0.10
Address:    10.96.0.10#53

** server can't find web3.default: NXDOMAIN

command terminated with exit code 1
pradeep@learnk8s$
</code></pre>
<p>All the queries worked as expected. We did not have any service named <code>web3</code> in the <code>default</code> namespace, so it failed.</p>
<p>Let us review the coredns logs again.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95
.:53
[INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] Reloading
[INFO] plugin/health: Going into lameduck mode for 5s
[INFO] plugin/reload: Running configuration MD5 = 07d7c5ad4525bf2c472eaef020d0184d
[INFO] Reloading complete
[INFO] 127.0.0.1:39552 - 1628 &quot;HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512&quot; NXDOMAIN qr,rd,ra 132 1.058936054s
[INFO] 10.244.151.4:41947 - 42256 &quot;A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512&quot; NXDOMAIN qr,aa,rd 155 0.000469847s
[INFO] 10.244.151.4:55247 - 43436 &quot;A IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NOERROR qr,aa,rd 106 0.000357148s
[INFO] 10.244.151.4:46146 - 25028 &quot;A IN web.default.default.svc.cluster.local. udp 55 false 512&quot; NXDOMAIN qr,aa,rd 148 0.000302078s
[INFO] 10.244.151.4:43513 - 15523 &quot;A IN web.default.svc.cluster.local. udp 47 false 512&quot; NOERROR qr,aa,rd 92 0.000412597s
[INFO] 10.244.151.4:41650 - 22470 &quot;A IN web2.default.default.svc.cluster.local. udp 56 false 512&quot; NXDOMAIN qr,aa,rd 149 0.000398918s
[INFO] 10.244.151.4:33214 - 54758 &quot;A IN web2.default.svc.cluster.local. udp 48 false 512&quot; NOERROR qr,aa,rd 94 0.00031088s
[INFO] 10.244.151.4:34263 - 58079 &quot;A IN web3.default.default.svc.cluster.local. udp 56 false 512&quot; NXDOMAIN qr,aa,rd 149 0.000433005s
[INFO] 10.244.151.4:52587 - 56008 &quot;A IN web3.default.svc.cluster.local. udp 48 false 512&quot; NXDOMAIN qr,aa,rd 141 0.000420513s
[INFO] 10.244.151.4:49172 - 35844 &quot;A IN web3.default.cluster.local. udp 44 false 512&quot; NXDOMAIN qr,aa,rd 137 0.000302094s
[INFO] 10.244.151.4:44833 - 3203 &quot;A IN web3.default. udp 30 false 512&quot; NXDOMAIN qr,rd,ra 105 0.18944251s
pradeep@learnk8s$
</code></pre>
<p>Are you in the right namespace for the service?
DNS queries that don't specify a namespace are limited to the pod's namespace.</p>
<p>If the namespace of the pod and service differ, the DNS query must include the namespace of the service.</p>
<p>This query is limited to the pod's namespace:</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup ingress-nginx-controller
Server:     10.96.0.10
Address:    10.96.0.10#53

** server can't find ingress-nginx-controller: NXDOMAIN

command terminated with exit code 1
pradeep@learnk8s$
</code></pre>
<p>This failed because, <code>ingress-nginx-controller</code> svc is not present in the <code>default</code> namespace.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95
.:53
[INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] Reloading
[INFO] plugin/health: Going into lameduck mode for 5s
[INFO] plugin/reload: Running configuration MD5 = 07d7c5ad4525bf2c472eaef020d0184d
[INFO] Reloading complete
[INFO] 127.0.0.1:39552 - 1628 &quot;HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512&quot; NXDOMAIN qr,rd,ra 132 1.058936054s
[INFO] 10.244.151.4:41947 - 42256 &quot;A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512&quot; NXDOMAIN qr,aa,rd 155 0.000469847s
[INFO] 10.244.151.4:55247 - 43436 &quot;A IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NOERROR qr,aa,rd 106 0.000357148s
[INFO] 10.244.151.4:46146 - 25028 &quot;A IN web.default.default.svc.cluster.local. udp 55 false 512&quot; NXDOMAIN qr,aa,rd 148 0.000302078s
[INFO] 10.244.151.4:43513 - 15523 &quot;A IN web.default.svc.cluster.local. udp 47 false 512&quot; NOERROR qr,aa,rd 92 0.000412597s
[INFO] 10.244.151.4:41650 - 22470 &quot;A IN web2.default.default.svc.cluster.local. udp 56 false 512&quot; NXDOMAIN qr,aa,rd 149 0.000398918s
[INFO] 10.244.151.4:33214 - 54758 &quot;A IN web2.default.svc.cluster.local. udp 48 false 512&quot; NOERROR qr,aa,rd 94 0.00031088s
[INFO] 10.244.151.4:34263 - 58079 &quot;A IN web3.default.default.svc.cluster.local. udp 56 false 512&quot; NXDOMAIN qr,aa,rd 149 0.000433005s
[INFO] 10.244.151.4:52587 - 56008 &quot;A IN web3.default.svc.cluster.local. udp 48 false 512&quot; NXDOMAIN qr,aa,rd 141 0.000420513s
[INFO] 10.244.151.4:49172 - 35844 &quot;A IN web3.default.cluster.local. udp 44 false 512&quot; NXDOMAIN qr,aa,rd 137 0.000302094s
[INFO] 10.244.151.4:44833 - 3203 &quot;A IN web3.default. udp 30 false 512&quot; NXDOMAIN qr,rd,ra 105 0.18944251s
[INFO] 10.244.151.4:50251 - 50181 &quot;A IN ingress-nginx-controller.default.svc.cluster.local. udp 68 false 512&quot; NXDOMAIN qr,aa,rd 161 0.000381672s
[INFO] 10.244.151.4:52033 - 12007 &quot;A IN ingress-nginx-controller.svc.cluster.local. udp 60 false 512&quot; NXDOMAIN qr,aa,rd 153 0.000844354s
[INFO] 10.244.151.4:50003 - 30556 &quot;A IN ingress-nginx-controller.cluster.local. udp 56 false 512&quot; NXDOMAIN qr,aa,rd 149 0.000453492s
[INFO] 10.244.151.4:41810 - 19431 &quot;A IN ingress-nginx-controller. udp 42 false 512&quot; NXDOMAIN qr,rd,ra 117 0.057777871s
pradeep@learnk8s$
</code></pre>
<p>We can see that the query is done with the <code>default</code> namespace: <code>ingress-nginx-controller.default.svc.cluster.local.</code></p>
<p>Let us list all active services in all namespaces.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl get svc -A
NAMESPACE       NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
default         kubernetes                           ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                      47h
default         web                                  NodePort    10.105.47.136    &lt;none&gt;        8080:31270/TCP               137m
default         web2                                 NodePort    10.106.73.151    &lt;none&gt;        8080:31691/TCP               127m
ingress-nginx   ingress-nginx-controller             NodePort    10.101.85.4      &lt;none&gt;        80:30419/TCP,443:30336/TCP   147m
ingress-nginx   ingress-nginx-controller-admission   ClusterIP   10.104.163.136   &lt;none&gt;        443/TCP                      147m
kube-system     kube-dns                             ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP       47h
pradeep@learnk8s$
</code></pre>
<p>Now that we see <code>ingress-nginx-controller</code> in the <code>ingress-nginx</code> namespace, let us query for the correct name.</p>
<pre><code class="language-sh">pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup ingress-nginx-controller.ingress-nginx
Server:     10.96.0.10
Address:    10.96.0.10#53

Name:   ingress-nginx-controller.ingress-nginx.svc.cluster.local
Address: 10.101.85.4
</code></pre>
<p>We can see the name resolution is successful this time.</p>
<p>:end:</p>
<p>Thank you for reading  my :book:</p>
<p>:warning: <strong>Warning:</strong> Do not push the big red button.</p>
<p>:memo: <strong>Note:</strong> Sunrises are beautiful.</p>
<p>:bulb: <strong>Tip:</strong> Remember to appreciate the little things in life.</p>
<p>:building_construction: :building_construction::building_construction: Learning continues and new topics will be updated here regularly. </p>
<p>Keep learning :rocket:</p>
<p>Best regards,
Pradeep</p>
<p><img alt="kubernetes-logo" src="../kubernetes-logo.png" /></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../about/" class="btn btn-neutral" title="About"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../about/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
