{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome","title":"Welcome"},{"location":"#welcome","text":"","title":"Welcome"},{"location":"Kubernetes/","text":"Kubernetes Introduction Google has created a resource in the form of a comic to understand why Kubernetes and what problems can be addressed by Kubernetes. Here is the link to the same. https://cloud.google.com/kubernetes-engine/kubernetes-comic And here is an extract: Learning Resources While there are many resources out there, personally I have used these resources: Introduction to Kubernetes by Linux Foundation then, the most popular Udemy course on this subject: Certified Kubernetes Administrator (CKA) with Practice Tests by Mumshad Mannambeth , and the official Linux Foundation course LFS258 Also, I got the opportunity to attend some of the O'Reilly Live Classes\u2013 Certified Kubernetes Administrator (CKA) Exam Prep, hosted by Benjamin Muschko , and Certified Kubernetes Administrator (CKA) Crash Course, presented by Sander van Vugt . Of course, the official Kubernetes Documentation Lab Setup In this post, let's explore the world of Kubernetes right from our workstation. All the examples shown here are executed on a machine running macOS. To get started, we need to install three packages: minikube , hyperkit , and kubectl . If you already have some kind of virtualization software like Vmware Fusion or Virtualbox, hyperkit is not mandatory, but I prefer to use hyperkit (which seems to be the case with minikube also). Minikube Download pradeep@learnk8s$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 Install pradeep@learnk8s$ sudo install minikube-darwin-amd64 /usr/local/bin/minikube pradeep@learnk8s$ minikube version minikube version: v1.25.1 commit: 3e64b11ed75e56e4898ea85f96b2e4af0301f43d Hyperkit Download If not, install Brew using, https://brew.sh/ Install if you have Brew package manager, run: pradeep@learnk8s$ brew install hyperkit pradeep@learnk8s$ hyperkit -v hyperkit: 0.20200908 Homepage: https://github.com/docker/hyperkit License: BSD Kubectl The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. Download https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/ Install pradeep@learnk8s$ brew install kubectl pradeep@learnk8s$ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.2\", GitCommit:\"9d142434e3af351a628bffee3939e64c681afa4d\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:27:51Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.1\", GitCommit:\"86ec240af8cbd1b60bcc4c03c20da9b98005b92e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T11:34:54Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"} All in one Minikube Cluster (macOS) pradeep@learnk8s$ minikube start \ud83d\ude04 minikube v1.25.1 on Darwin 11.6.2 \u2728 Automatically selected the hyperkit driver. Other choices: vmware, virtualbox, ssh \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ... \u2757 This VM is having trouble accessing https://k8s.gcr.io \ud83d\udca1 To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/ \ud83d\udc33 Preparing Kubernetes v1.23.1 on Docker 20.10.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \u2757 /usr/local/bin/kubectl is version 1.21.2, which may have incompatibilites with Kubernetes 1.23.1. \u25aa Want kubectl v1.23.1? Try 'minikube kubectl -- get pods -A' \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default Verify pradeep@learnk8s$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured pradeep@learnk8s$ kubectl get pods No resources found in default namespace. pradeep@learnk8s$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-64897985d-nvzjq 1/1 Running 0 5m7s kube-system etcd-minikube 1/1 Running 0 5m7s kube-system kube-apiserver-minikube 1/1 Running 0 5m7s kube-system kube-controller-manager-minikube 1/1 Running 0 5m7s kube-system kube-proxy-2ch8q 1/1 Running 0 5m7s kube-system kube-scheduler-minikube 1/1 Running 0 5m7s kube-system storage-provisioner 1/1 Running 0 5m7s Multi-node Minikube Cluster (macOS) pradeep@learnk8s$ minikube start --nodes 2 -p k8s \ud83d\ude04 [k8s] minikube v1.25.1 on Darwin 11.6.2 \u2728 Automatically selected the hyperkit driver. Other choices: vmware, virtualbox, ssh \ud83d\udc4d Starting control plane node k8s in cluster k8s \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \u2757 This VM is having trouble accessing https://k8s.gcr.io \ud83d\udca1 To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/ \ud83d\udc33 Preparing Kubernetes v1.23.1 on Docker 20.10.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa kubelet.cni-conf-dir=/etc/cni/net.mk \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd17 Configuring CNI (Container Networking Interface) ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: storage-provisioner, default-storageclass \ud83d\udc4d Starting worker node k8s-m02 in cluster k8s \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=192.168.177.17 \u2757 This VM is having trouble accessing https://k8s.gcr.io \ud83d\udca1 To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/ \ud83d\udc33 Preparing Kubernetes v1.23.1 on Docker 20.10.12 ... \u25aa env NO_PROXY=192.168.177.17 \ud83d\udd0e Verifying Kubernetes components... \u2757 /usr/local/bin/kubectl is version 1.21.2, which may have incompatibilites with Kubernetes 1.23.1. \u25aa Want kubectl v1.23.1? Try 'minikube kubectl -- get pods -A' \ud83c\udfc4 Done! kubectl is now configured to use \"k8s\" cluster and \"default\" namespace by default Verify pradeep@learnk8s$ minikube status -p k8s k8s type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured k8s-m02 type: Worker host: Running kubelet: Running Minikube Cluster with a different runtime (containerd) Read this post on Kubernetes Blog https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ pradeep@learnk8s$ minikube start --container-runtime=containerd \ud83d\ude04 minikube v1.25.1 on Darwin 11.6.2 \u2728 Automatically selected the hyperkit driver. Other choices: vmware, virtualbox, ssh \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udcbe Downloading Kubernetes v1.23.1 preload ... > preloaded-images-k8s-v16-v1...: 571.57 MiB / 571.57 MiB 100.00% 3.17 MiB \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ... \u2757 This VM is having trouble accessing https://k8s.gcr.io \ud83d\udca1 To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/ \ud83d\udce6 Preparing Kubernetes v1.23.1 on containerd 1.4.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd17 Configuring bridge CNI (Container Networking Interface) ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: storage-provisioner, default-storageclass \u2757 /usr/local/bin/kubectl is version 1.21.2, which may have incompatibilites with Kubernetes 1.23.1. \u25aa Want kubectl v1.23.1? Try 'minikube kubectl -- get pods -A' \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default Kubernetes Architecture https://kubernetes.io/docs/concepts/overview/components/ Control Plane Etcd\u2014A distributed data store which persists cluster configuration API Server\u2014Central communication hub for all Kubernetes components This is also the endpoint used by Kubernetes clients, such as the kubectl CLI Scheduler\u2014The component responsible for assigning application components to worker nodes Controller Manager\u2014Handles cluster-level operations, such as tracking worker node status, handling node failures, and replicating components For our examples we will use a single master configuration, but the master functions can be replicated and distributed for reliability Worker Nodes Kubelet\u2014Deploys and manages application components assigned to the local node by the scheduler. Kube-proxy\u2014The local networking component on the worker. Handles load-balancing between application components. Container engine \u2013 The container runtime used by the local worker node. For our examples we will use the most common runtime, Docker. Kubernetes API Resources pradeep@learnk8s$ kubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND bindings v1 true Binding componentstatuses cs v1 false ComponentStatus configmaps cm v1 true ConfigMap endpoints ep v1 true Endpoints events ev v1 true Event limitranges limits v1 true LimitRange namespaces ns v1 false Namespace nodes no v1 false Node persistentvolumeclaims pvc v1 true PersistentVolumeClaim persistentvolumes pv v1 false PersistentVolume pods po v1 true Pod podtemplates v1 true PodTemplate replicationcontrollers rc v1 true ReplicationController resourcequotas quota v1 true ResourceQuota secrets v1 true Secret serviceaccounts sa v1 true ServiceAccount services svc v1 true Service mutatingwebhookconfigurations admissionregistration.k8s.io/v1 false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io/v1 false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io/v1 false CustomResourceDefinition apiservices apiregistration.k8s.io/v1 false APIService controllerrevisions apps/v1 true ControllerRevision daemonsets ds apps/v1 true DaemonSet deployments deploy apps/v1 true Deployment replicasets rs apps/v1 true ReplicaSet statefulsets sts apps/v1 true StatefulSet tokenreviews authentication.k8s.io/v1 false TokenReview localsubjectaccessreviews authorization.k8s.io/v1 true LocalSubjectAccessReview selfsubjectaccessreviews authorization.k8s.io/v1 false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io/v1 false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io/v1 false SubjectAccessReview horizontalpodautoscalers hpa autoscaling/v2 true HorizontalPodAutoscaler cronjobs cj batch/v1 true CronJob jobs batch/v1 true Job certificatesigningrequests csr certificates.k8s.io/v1 false CertificateSigningRequest leases coordination.k8s.io/v1 true Lease endpointslices discovery.k8s.io/v1 true EndpointSlice events ev events.k8s.io/v1 true Event flowschemas flowcontrol.apiserver.k8s.io/v1beta2 false FlowSchema prioritylevelconfigurations flowcontrol.apiserver.k8s.io/v1beta2 false PriorityLevelConfiguration ingressclasses networking.k8s.io/v1 false IngressClass ingresses ing networking.k8s.io/v1 true Ingress networkpolicies netpol networking.k8s.io/v1 true NetworkPolicy runtimeclasses node.k8s.io/v1 false RuntimeClass poddisruptionbudgets pdb policy/v1 true PodDisruptionBudget podsecuritypolicies psp policy/v1beta1 false PodSecurityPolicy clusterrolebindings rbac.authorization.k8s.io/v1 false ClusterRoleBinding clusterroles rbac.authorization.k8s.io/v1 false ClusterRole rolebindings rbac.authorization.k8s.io/v1 true RoleBinding roles rbac.authorization.k8s.io/v1 true Role priorityclasses pc scheduling.k8s.io/v1 false PriorityClass csidrivers storage.k8s.io/v1 false CSIDriver csinodes storage.k8s.io/v1 false CSINode csistoragecapacities storage.k8s.io/v1beta1 true CSIStorageCapacity storageclasses sc storage.k8s.io/v1 false StorageClass volumeattachments storage.k8s.io/v1 false VolumeAttachment Kubernetes resources of a standard application As shown here, to deploy a standard application in Kubernetes, we might have to deploy a lot of these API resources. https://github.com/kubernetes/community/blob/master/icons/examples/schemas/std-app.png Reference https://kubernetes.io/docs Pods Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. ReplicaSet A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. Deployments A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. Service An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you don't need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them. Namespace Defaults pradeep@learnk8s$ kubectl get namespaces NAME STATUS AGE default Active 2m7s kube-node-lease Active 2m10s kube-public Active 2m10s kube-system Active 2m10s As we saw at the beginning, when we setup a kubernetes cluster, all control plane and worker node compononets gets installed as pods in the system defined namespace kube-system . Create a new namespace pradeep@learnk8s$ kubectl create namespace prod namespace/prod created pradeep@learnk8s$ kubectl get ns NAME STATUS AGE default Active 5m4s prod Active 33s kube-node-lease Active 5m7s kube-public Active 5m7s kube-system Active 5m7s pradeep@learnk8s$ kubectl run nginx --image=nginx -n prod pod/nginx created pradeep@learnk8s$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 79s Life cycle of a Pod kubectl client sends new pod request Scheduler assigns new pod to worker node API server instructs kubelet to provision new Pod kubelet requests that Docker create container Docker retrieves nginx image Docker creates container Kubelet passes container details to K8s API Pod configs stored in etcd Network configs are created on kube-proxy Create a Deployment pradeep@learnk8s$ kubectl create deployment demo --image=k8s.gcr.io/echoserver:1.4 deployment.apps/demo created pradeep@learnk8s$ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE demo 1/1 1 1 36s pradeep@learnk8s$ kubectl get replicasets NAME DESIRED CURRENT READY AGE demo-75f9c7566f 1 1 1 31s pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-75f9c7566f-nmjw2 1/1 Running 0 26s View Cluster Events pradeep@learnk8s$ kubectl get events LAST SEEN TYPE REASON OBJECT MESSAGE 5m15s Normal Scheduled pod/demo-75f9c7566f-nmjw2 Successfully assigned default/demo-75f9c7566f-nmjw2 to k8s-m02 5m14s Normal Pulling pod/demo-75f9c7566f-nmjw2 Pulling image \"k8s.gcr.io/echoserver:1.4\" 4m57s Normal Pulled pod/demo-75f9c7566f-nmjw2 Successfully pulled image \"k8s.gcr.io/echoserver:1.4\" in 16.898815167s 4m57s Normal Created pod/demo-75f9c7566f-nmjw2 Created container echoserver 4m57s Normal Started pod/demo-75f9c7566f-nmjw2 Started container echoserver 5m15s Normal SuccessfulCreate replicaset/demo-75f9c7566f Created pod: demo-75f9c7566f-nmjw2 5m15s Normal ScalingReplicaSet deployment/demo Scaled up replica set demo-75f9c7566f to 1 14m Normal Starting node/k8s-m02 Starting kubelet. 14m Normal NodeHasSufficientMemory node/k8s-m02 Node k8s-m02 status is now: NodeHasSufficientMemory 14m Normal NodeHasNoDiskPressure node/k8s-m02 Node k8s-m02 status is now: NodeHasNoDiskPressure 14m Normal NodeHasSufficientPID node/k8s-m02 Node k8s-m02 status is now: NodeHasSufficientPID 14m Normal NodeAllocatableEnforced node/k8s-m02 Updated Node Allocatable limit across pods 14m Normal Starting node/k8s-m02 14m Normal RegisteredNode node/k8s-m02 Node k8s-m02 event: Registered Node k8s-m02 in Controller 13m Normal NodeReady node/k8s-m02 Node k8s-m02 status is now: NodeReady 15m Normal NodeHasSufficientMemory node/k8s Node k8s status is now: NodeHasSufficientMemory 15m Normal NodeHasNoDiskPressure node/k8s Node k8s status is now: NodeHasNoDiskPressure 15m Normal NodeHasSufficientPID node/k8s Node k8s status is now: NodeHasSufficientPID 14m Normal Starting node/k8s Starting kubelet. 14m Normal NodeHasSufficientMemory node/k8s Node k8s status is now: NodeHasSufficientMemory 14m Normal NodeHasNoDiskPressure node/k8s Node k8s status is now: NodeHasNoDiskPressure 14m Normal NodeHasSufficientPID node/k8s Node k8s status is now: NodeHasSufficientPID 14m Normal NodeAllocatableEnforced node/k8s Updated Node Allocatable limit across pods 14m Normal RegisteredNode node/k8s Node k8s event: Registered Node k8s in Controller 14m Normal Starting node/k8s 14m Normal NodeReady node/k8s Node k8s status is now: NodeReady Scaling a Deployment pradeep@learnk8s$ kubectl scale deployment demo --replicas=3 deployment.apps/demo scaled pradeep@learnk8s$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE demo 3/3 3 3 11m pradeep@learnk8s$ kubectl get rs NAME DESIRED CURRENT READY AGE demo-75f9c7566f 3 3 3 12m pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-75f9c7566f-fct76 1/1 Running 0 2m2s demo-75f9c7566f-kz9kb 1/1 Running 0 2m3s demo-75f9c7566f-nmjw2 1/1 Running 0 12m pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-75f9c7566f-fct76 1/1 Running 0 2m10s 10.244.1.4 k8s-m02 <none> <none> demo-75f9c7566f-kz9kb 1/1 Running 0 2m11s 10.244.0.3 k8s <none> <none> demo-75f9c7566f-nmjw2 1/1 Running 0 12m 10.244.1.3 k8s-m02 <none> <none> Minikube SSH and Access Pods pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.4:8080 CLIENT VALUES: client_address=192.168.177.27 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.244.1.4:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.244.1.4:8080 user-agent=curl/7.78.0 BODY: -no body in request-$ $ curl 10.244.0.3:8080 CLIENT VALUES: client_address=10.244.0.1 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.244.0.3:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.244.0.3:8080 user-agent=curl/7.78.0 BODY: -no body in request-$ $ curl 10.244.1.3:8080 CLIENT VALUES: client_address=192.168.177.27 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.244.1.3:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.244.1.3:8080 user-agent=curl/7.78.0 BODY: -no body in request-$ Services ClusterIP: Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType. NodePort: Exposes the Service on each Node's IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You'll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort> . LoadBalancer: Exposes the Service externally using a cloud provider's load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. Default Services From the default namespace pradeep@learnk8s$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 33m From all namespaces pradeep@learnk8s$ kubectl get services -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 33m kube-system kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 33m NodePort type service Expose the deployment as a NodePort type Service pradeep@learnk8s$ kubectl expose deployment demo --type=NodePort --port=8080 service/demo exposed pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo NodePort 10.101.87.60 <none> 8080:32298/TCP 4s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 35m Note the port number 32298 assigned to the service Access NodePort type Service pradeep@learnk8s$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s Ready control-plane,master 38m v1.23.1 192.168.177.27 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 k8s-m02 Ready <none> 37m v1.23.1 192.168.177.28 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 From Minikube SSH session, with k8s-m02 node IP, and NodePort 32298 $ curl 192.168.177.28:32298 CLIENT VALUES: client_address=10.244.1.1 command=GET real path=/ query=nil request_version=1.1 request_uri=http://192.168.177.28:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=192.168.177.28:32298 user-agent=curl/7.78.0 BODY: -no body in request-$ with k8s node IP and NodePort 32298 $ curl 192.168.177.27:32298 CLIENT VALUES: client_address=10.244.0.1 command=GET real path=/ query=nil request_version=1.1 request_uri=http://192.168.177.27:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=192.168.177.27:32298 user-agent=curl/7.78.0 BODY: -no body in request-$ ClusterIP Service Let's delete the service pradeep@learnk8s$ kubectl delete svc demo service \"demo\" deleted and expose the same deployment as a ClusterIP (the default ServiceType). If we don't specify any type, its the ClusterIP. pradeep@learnk8s$ kubectl expose deployment demo --port=8080 service/demo exposed pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo ClusterIP 10.103.185.242 <none> 8080/TCP 5s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 45m Note the ClusterIP assigned to this service 10.103.185.242 . Access Pods with ClusterIP pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.103.185.242:8080 CLIENT VALUES: client_address=192.168.177.27 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.103.185.242:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.103.185.242:8080 user-agent=curl/7.78.0 BODY: -no body in request-$ LoadBalancer Service pradeep@learnk8s$ kubectl delete service demo service \"demo\" deleted pradeep@learnk8s$ kubectl expose deployment demo \u2013type=LoadBalancer --port=8080 service/demo exposed pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo LoadBalancer 10.98.252.85 <pending> 8080:30646/TCP 3s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 50m Open another terminal and initiate minikube tunnel pradeep@learnk8s$ minikube tunnel -p k8s Password: Status: machine: k8s pid: 95721 route: 10.96.0.0/12 -> 192.168.177.27 minikube: Running services: [demo] errors: minikube: no errors router: no errors loadbalancer emulator: no errors Return to the first terminal, Verify that EXTERNAL-IP is populated now pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo LoadBalancer 10.98.252.85 10.98.252.85 8080:30646/TCP 3m27s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 53m NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. Access Pods with LoadBalancer Service pradeep@learnk8s$ minikube service demo -p k8s |-----------|------|-------------|-----------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|------|-------------|-----------------------------| | default | demo | 8080 | http://192.168.177.27:30646 | |-----------|------|-------------|-----------------------------| \ud83c\udf89 Opening service default/demo in default browser.. Deleting Resources pradeep@learnk8s$ kubectl delete pod demo-75f9c7566f-fct76 pod \"demo-75f9c7566f-fct76\" deleted pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-75f9c7566f-b45x7 0/1 ContainerCreating 0 3s demo-75f9c7566f-kz9kb 1/1 Running 0 55m demo-75f9c7566f-nmjw2 1/1 Running 0 65m pradeep@learnk8s$ kubectl get all NAME READY STATUS RESTARTS AGE pod/demo-75f9c7566f-b45x7 1/1 Running 0 2m29s pod/demo-75f9c7566f-kz9kb 1/1 Running 0 57m pod/demo-75f9c7566f-nmjw2 1/1 Running 0 67m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 77m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/demo 3/3 3 3 67m NAME DESIRED CURRENT READY AGE replicaset.apps/demo-75f9c7566f 3 3 3 67m pradeep@learnk8s$ kubectl delete deployment demo deployment.apps \"demo\" deleted Deleting a deployment, deletes all the associated pods, replicaset as well. pradeep@learnk8s$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 78m Creating Resources in a Declarative Way\u2014YAML pradeep@learnk8s$ kubectl run nginx --image=nginx --dry-run=client -o yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: nginx name: nginx spec: containers: - image: nginx name: nginx resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} pradeep@learnk8s$ kubectl run nginx --image=nginx --dry-run=client -o yaml > demopod.yaml pradeep@learnk8s$ kubectl create -f demopod.yaml pod/nginx created pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 50s pradeep@learnk8s$ kubectl create deployment demo --image=nginx --replicas=3 --dry-run=client -o yaml > demodeploy.yaml pradeep@learnk8s$ cat demodeploy.yaml apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: demo name: demo spec: replicas: 3 selector: matchLabels: app: demo strategy: {} template: metadata: creationTimestamp: null labels: app: demo spec: containers: - image: nginx name: nginx resources: {} status: {} pradeep@learnk8s$ kubectl create -f demodeploy.yaml deployment.apps/demo created pradeep@learnk8s$ kubectl get all NAME READY STATUS RESTARTS AGE pod/demo-6c54f77c95-6g7zq 1/1 Running 0 71s pod/demo-6c54f77c95-sb4c9 1/1 Running 0 71s pod/demo-6c54f77c95-w2bsw 1/1 Running 0 71s pod/nginx 1/1 Running 0 5m45s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 90m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/demo 3/3 3 3 71s NAME DESIRED CURRENT READY AGE replicaset.apps/demo-6c54f77c95 3 3 3 71s Show Labels pradeep@learnk8s$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS demo-6c54f77c95-6g7zq 1/1 Running 0 3m32s app=demo,pod-template-hash=6c54f77c95 demo-6c54f77c95-sb4c9 1/1 Running 0 3m32s app=demo,pod-template-hash=6c54f77c95 demo-6c54f77c95-w2bsw 1/1 Running 0 3m32s app=demo,pod-template-hash=6c54f77c95 nginx 1/1 Running 0 8m6s run=nginx pradeep@learnk8s$ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s Ready control-plane,master 92m v1.23.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s,kubernetes.io/os=linux,minikube.k8s.io/commit=3e64b11ed75e56e4898ea85f96b2e4af0301f43d,minikube.k8s.io/name=k8s,minikube.k8s.io/updated_at=2022_02_04T05_49_32_0700,minikube.k8s.io/version=v1.25.1,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-m02 Ready <none> 91m v1.23.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m02,kubernetes.io/os=linux Apply Labels pradeep@learnk8s$ kubectl label pods nginx new-label=awesome pod/nginx labeled pradeep@learnk8s$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS demo-6c54f77c95-6g7zq 1/1 Running 0 8m47s app=demo,pod-template-hash=6c54f77c95 demo-6c54f77c95-sb4c9 1/1 Running 0 8m47s app=demo,pod-template-hash=6c54f77c95 demo-6c54f77c95-w2bsw 1/1 Running 0 8m47s app=demo,pod-template-hash=6c54f77c95 nginx 1/1 Running 0 13m new-label=awesome,run=nginx Label Selector pradeep@learnk8s$ kubectl get pods --selector=app=demo NAME READY STATUS RESTARTS AGE demo-6c54f77c95-6g7zq 1/1 Running 0 10m demo-6c54f77c95-sb4c9 1/1 Running 0 10m demo-6c54f77c95-w2bsw 1/1 Running 0 10m pradeep@learnk8s$ kubectl get pods --selector=run=nginx NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 15m pradeep@learnk8s$ kubectl get pods --selector=new-label=awesome NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 15m Describe Resources pradeep@learnk8s$ kubectl describe pods nginx Name: nginx Namespace: default Priority: 0 Node: k8s-m02/192.168.177.28 Start Time: Fri, 04 Feb 2022 07:13:50 +0530 Labels: new-label=awesome run=nginx Annotations: <none> Status: Running IP: 10.244.1.6 IPs: IP: 10.244.1.6 Containers: nginx: Container ID: docker://32bfdccc6c984f51a4c4092e4027ff7f1e53dd518938196c95d5427a44a90e40 Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: <none> Host Port: <none> State: Running Started: Fri, 04 Feb 2022 07:13:56 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9sstv (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-9sstv: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18m default-scheduler Successfully assigned default/nginx to k8s-m02 Normal Pulling 18m kubelet Pulling image \"nginx\" Normal Pulled 18m kubelet Successfully pulled image \"nginx\" in 4.864287771s Normal Created 18m kubelet Created container nginx Normal Started 18m kubelet Started container nginx Kubectl Explain! pradeep@learnk8s$ kubectl explain pod.spec KIND: Pod VERSION: v1 RESOURCE: spec <Object> DESCRIPTION: Specification of the desired behavior of the pod. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status PodSpec is a description of a pod. FIELDS: <SNIP> containers <[]Object> -required- List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. <SNIP> nodeName <string> NodeName is a request to schedule this pod onto a specific node. If it is non-empty, the scheduler simply schedules this pod onto that node, assuming that it fits resource requirements. <SNIP> Scheduling Manual Scheduling Modify the Pod definition file, to includ the nodeName in the spec. pradeep@learnk8s$ cat manual-sched-pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx-manual spec: nodeName: k8s containers: - image: nginx name: nginx pradeep@learnk8s$ kubectl create -f manual-sched-pod.yaml pod/nginx-manual created pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-6c54f77c95-6g7zq 1/1 Running 0 20m demo-6c54f77c95-sb4c9 1/1 Running 0 20m demo-6c54f77c95-w2bsw 1/1 Running 0 20m nginx 1/1 Running 0 25m nginx-manual 1/1 Running 0 48s pradeep@learnk8s$ kubectl get pods -o wide | grep manual nginx-manual 1/1 Running 0 2m9s 10.244.0.5 k8s <none> <none> Node Selector You can constrain a Pod so that it can only run on particular set of Node(s). nodeSelector provides a very simple way to constrain pods to nodes with particular labels. First, let\u2019s label one of the nodes with disktype=ssd . pradeep@learnk8s$ kubectl label nodes k8s-m02 disktype=ssd node/k8s-m02 labeled pradeep@learnk8s$ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s Ready control-plane,master 3d11h v1.23.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s,kubernetes.io/os=linux,minikube.k8s.io/commit=3e64b11ed75e56e4898ea85f96b2e4af0301f43d,minikube.k8s.io/name=k8s,minikube.k8s.io/updated_at=2022_02_07T17_03_56_0700,minikube.k8s.io/version=v1.25.1,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-m02 Ready <none> 4m3s v1.23.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m02,kubernetes.io/os=linux pradeep@learnk8s$ Now, let's create a pod with nodeSelector spec set to the label that we assigned, so that the pod gets scheduled on this node. pradeep@learnk8s$ cat pod-node-selector.yaml apiVersion: v1 kind: Pod metadata: name: nginx-node-selector labels: env: test spec: containers: - name: nginx-node-selector image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd Finally, verify the node on which this pod is running. pradeep@learnk8s$ kubectl get pods -o wide | grep selector nginx-node-selector 1/1 Running 0 17s 10.244.1.2 k8s-m02 <none> <none> As expected, this new pod is running on the node( k8s-m02 ) with the label specified by the nodeSelector pod spec. If you describe this pod, you will see the configured Node-Selectors. pradeep@learnk8s$ kubectl describe pod nginx-node-selector | grep Node-Selectors Node-Selectors: disktype=ssd Node Affinity There are currently two types of node affinity, called requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution . You can think of them as \"hard\" and \"soft\" respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (similar to nodeSelector but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee. The IgnoredDuringExecution part of the names means that, similar to how nodeSelector works, if labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod continues to run on the node. pradeep@learnk8s$ cat pod-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: disktype operator: In values: - ssd containers: - name: with-node-affinity image: nginx pradeep@learnk8s$ kubectl create -f pod-node-affinity.yaml pod/with-node-affinity created pradeep@learnk8s$ kubectl get pods -o wide | grep affinity with-node-affinity 1/1 Running 0 9s 10.244.1.3 k8s-m02 <none> <none> Taints and Tolerations Taints allow a node to repel a set of pods. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints. Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. pradeep@learnk8s$ kubectl describe nodes | grep Taint Taints: <none> Taints: <none> pradeep@learnk8s$ kubectl taint nodes k8s key1=value1:NoSchedule node/k8s tainted pradeep@learnk8s$ kubectl describe nodes | grep Taint Taints: key1=value1:NoSchedule Taints: <none> pradeep@learnk8s$ kubectl taint nodes k8s-m02 key2=value2:NoSchedule node/k8s-m02 tainted pradeep@learnk8s$ kubectl describe nodes | grep Taint Taints: key1=value1:NoSchedule Taints: key2=value2:NoSchedule pradeep@learnk8s$ cat pod-toleration.yaml apiVersion: v1 kind: Pod metadata: name: nginx-taint-demo labels: env: test spec: containers: - name: nginx-taint-demo image: nginx imagePullPolicy: IfNotPresent tolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoSchedule\" pradeep@learnk8s$ kubectl create -f pod-toleration.yaml pod/nginx-taint-demo created pradeep@learnk8s$ kubectl get pods -o wide | grep taint-demo nginx-taint-demo 1/1 Running 0 9m3s 10.244.0.15 k8s <none> <none> pradeep@learnk8s$ cat pod-toleration-2.yaml apiVersion: v1 kind: Pod metadata: name: nginx-taint-demo-2 labels: env: test spec: containers: - name: nginx-taint-demo-2 image: nginx imagePullPolicy: IfNotPresent tolerations: - key: \"key2\" operator: \"Equal\" value: \"value2\" effect: \"NoSchedule\" pradeep@learnk8s$ kubectl create -f pod-toleration-2.yaml pod/nginx-taint-demo-2 created pradeep@learnk8s$ kubectl get pods -o wide | grep taint-demo nginx-taint-demo 1/1 Running 0 11m 10.244.0.15 k8s <none> <none> nginx-taint-demo-2 1/1 Running 0 9m35s 10.244.2.5 k8s-m02 <none> <none> pradeep@learnk8s$ cat pod-no-toleration.yaml apiVersion: v1 kind: Pod metadata: name: nginx-no-tolerate labels: env: test spec: containers: - name: nginx-no-tolerate image: nginx imagePullPolicy: IfNotPresent pradeep@learnk8s$ kubectl create -f pod-no-toleration.yaml pod/nginx-no-tolerate created pradeep@learnk8s$ kubectl get pods -o wide | grep no-tolerate nginx-no-tolerate 0/1 Pending 0 10m <none> <none> <none> <none> The Pod got created but is in Pending state, it is not Running yet. Let's find out why?! pradeep@learnk8s$ kubectl describe pods nginx-no-tolerate Name: nginx-no-tolerate Namespace: default Priority: 0 Node: <none> Labels: env=test Annotations: <none> Status: Pending IP: IPs: <none> Containers: nginx-no-tolerate: Image: nginx Port: <none> Host Port: <none> Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6mz6d (ro) Conditions: Type Status PodScheduled False Volumes: kube-api-access-6mz6d: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 43s (x11 over 11m) default-scheduler 0/2 nodes are available: 1 node(s) had taint {key1: value1}, that the pod didn't tolerate, 1 node(s) had taint {key2: value2}, that the pod didn't tolerate. Look at the Reason: FailedScheduling , none of the nodes (0/2) are available because both have some taint that our pod couldn't tolerate! Now, let's delete the Taint on one of the nodes and try creating the Pod again. To untaint a node, just add a - at the end of the taint that you plan to remove. pradeep@learnk8s$ kubectl taint node k8s key1=value1:NoSchedule- node/k8s untainted pradeep@learnk8s$ kubectl describe nodes | grep Taint Taints: <none> Taints: key2=value2:NoSchedule Now the nginx-no-tolerate pod changes to Running state and is scheduled on the k8s node which does not have any Taints at the moment. pradeep@learnk8s$ kubectl get pods -o wide | grep no-tolerate nginx-no-tolerate 1/1 Running 0 16h 10.244.0.16 k8s <none> <none> Resource Limits Earlier, we have created a namespace called prod . Let's describe it to see if any resource quota applied to it. pradeep@learnk8s$ kubectl describe namespaces prod Name: prod Labels: kubernetes.io/metadata.name=prod Annotations: <none> Status: Active No resource quota. No LimitRange resource. Let's define a quota using the example shown here: pradeep@learnk8s$ kubectl create resourcequota -h Create a resource quota with the specified name, hard limits, and optional scopes. Aliases: quota, resourcequota Examples: # Create a new resource quota named my-quota kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10 # Create a new resource quota named best-effort kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort pradeep@learnk8s$ kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3 -n prod resourcequota/my-quota created pradeep@learnk8s$ kubectl describe ns prod Name: prod Labels: kubernetes.io/metadata.name=prod Annotations: <none> Status: Active Resource Quotas Name: my-quota Resource Used Hard -------- --- --- cpu 0 1 memory 0 1G pods 0 2 services 0 3 No LimitRange resource. pradeep@learnk8s$ kubectl get quota -n prod NAME AGE REQUEST LIMIT my-quota 4m40s cpu: 0/1, memory: 0/1G, pods: 0/2, services: 0/3 pradeep@learnk8s$ kubectl describe quota -n prod Name: my-quota Namespace: prod Resource Used Hard -------- ---- ---- cpu 0 1 memory 0 1G pods 0 2 services 0 3 Let's try create a pod in this namespace and see what happens? pradeep@learnk8s$ kubectl run nginx-without-cpu --image=nginx -n prod Error from server (Forbidden): pods \"nginx-without-cpu\" is forbidden: failed quota: my-quota: must specify cpu,memory pradeep@learnk8s$ Pod creation failed with an error (Forbidden). Let's try create a deployment in this namespace and see what happens? pradeep@learnk8s$ kubectl create deployment test-quota --image=nginx --replicas=3 -n prod deployment.apps/test-quota created pradeep@learnk8s$ kubectl get deploy -n prod NAME READY UP-TO-DATE AVAILABLE AGE test-quota 0/3 0 0 47s pradeep@learnk8s$ kubectl get events -n prod LAST SEEN TYPE REASON OBJECT MESSAGE 104s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-bgsxd\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-w9p4z\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-swwgq\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-tjlsb\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-ll2m9\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-c72mn\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-hqzf4\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-bdh4w\" is forbidden: failed quota: my-quota: must specify cpu,memory 102s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-gggw2\" is forbidden: failed quota: my-quota: must specify cpu,memory 21s Warning FailedCreate replicaset/test-quota-666dfd598f (combined from similar events): Error creating: pods \"test-quota-666dfd598f-b5rh6\" is forbidden: failed quota: my-quota: must specify cpu,memory 104s Normal ScalingReplicaSet deployment/test-quota Scaled up replica set test-quota-666dfd598f to 3 pradeep@learnk8s$ cat pod-quota.yaml apiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo namespace: prod spec: containers: - name: quota-mem-cpu-demo-ctr image: nginx resources: limits: memory: \"800Mi\" cpu: \"800m\" requests: memory: \"600Mi\" cpu: \"400m\" pradeep@learnk8s$ kubectl create -f pod-quota.yaml pod/quota-mem-cpu-demo created pradeep@learnk8s$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE quota-mem-cpu-demo 1/1 Running 0 9s pradeep@learnk8s$ kubectl describe ns prod Name: prod Labels: kubernetes.io/metadata.name=prod Annotations: <none> Status: Active Resource Quotas Name: my-quota Resource Used Hard -------- --- --- cpu 400m 1 memory 600Mi 1G pods 1 2 services 0 3 No LimitRange resource. pradeep@learnk8s$ cat pod-quota-2.yaml apiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo-2 namespace: prod spec: containers: - name: quota-mem-cpu-demo-ctr-2 image: nginx resources: limits: memory: \"800Mi\" cpu: \"800m\" requests: memory: \"600Mi\" cpu: \"400m\" pradeep@learnk8s$ kubectl create -f pod-quota-2.yaml Error from server (Forbidden): error when creating \"pod-quota-2.yaml\": pods \"quota-mem-cpu-demo-2\" is forbidden: exceeded quota: my-quota, requested: memory=600Mi, used: memory=600Mi, limited: memory=1G Daemon Sets A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. pradeep@learnk8s$ kubectl get daemonsets.apps -A NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system kindnet 2 2 2 2 2 <none> 5d12h kube-system kube-proxy 2 2 2 2 2 kubernetes.io/os=linux 5d12h As seen above, there are two daemonsets in the kube-system namespace. One is named kindnet and the other is kube-proxy . If you look at the DESIRED column, both of them have a value of 2 which is equal to the number of nodes in this (minikube) cluster. Let's look at those pods now. pradeep@learnk8s$ kubectl describe daemonsets.apps kindnet -n kube-system Name: kindnet Selector: app=kindnet Node-Selector: <none> Labels: app=kindnet k8s-app=kindnet tier=node Annotations: deprecated.daemonset.template.generation: 1 Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=kindnet k8s-app=kindnet tier=node Service Account: kindnet Containers: kindnet-cni: Image: kindest/kindnetd:v20210326-1e038dc5 Port: <none> Host Port: <none> Limits: cpu: 100m memory: 50Mi Requests: cpu: 100m memory: 50Mi Environment: HOST_IP: (v1:status.hostIP) POD_IP: (v1:status.podIP) POD_SUBNET: 10.244.0.0/16 Mounts: /etc/cni/net.d from cni-cfg (rw) /lib/modules from lib-modules (ro) /run/xtables.lock from xtables-lock (rw) Volumes: cni-cfg: Type: HostPath (bare host directory volume) Path: /etc/cni/net.mk HostPathType: DirectoryOrCreate xtables-lock: Type: HostPath (bare host directory volume) Path: /run/xtables.lock HostPathType: FileOrCreate lib-modules: Type: HostPath (bare host directory volume) Path: /lib/modules HostPathType: Events: <none> pradeep@learnk8s$ kubectl describe daemonsets.apps kube-proxy -n kube-system Name: kube-proxy Selector: k8s-app=kube-proxy Node-Selector: kubernetes.io/os=linux Labels: k8s-app=kube-proxy Annotations: deprecated.daemonset.template.generation: 1 Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: k8s-app=kube-proxy Service Account: kube-proxy Containers: kube-proxy: Image: k8s.gcr.io/kube-proxy:v1.23.1 Port: <none> Host Port: <none> Command: /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME) Environment: NODE_NAME: (v1:spec.nodeName) Mounts: /lib/modules from lib-modules (ro) /run/xtables.lock from xtables-lock (rw) /var/lib/kube-proxy from kube-proxy (rw) Volumes: kube-proxy: Type: ConfigMap (a volume populated by a ConfigMap) Name: kube-proxy Optional: false xtables-lock: Type: HostPath (bare host directory volume) Path: /run/xtables.lock HostPathType: FileOrCreate lib-modules: Type: HostPath (bare host directory volume) Path: /lib/modules HostPathType: Priority Class Name: system-node-critical Events: <none> As seen in this output, there are two pods, one pod on each node. The pod names follow the convention of <daemonsetname>-<randomidentifier> . pradeep@learnk8s$ kubectl get pods -A -o wide -o wide| grep kube-proxy kube-system kube-proxy-fszkr 1/1 Running 1 (2d ago) 5d12h 192.168.177.28 k8s-m02 <none> <none> kube-system kube-proxy-m747v 1/1 Running 1 5d12h 192.168.177.27 k8s <none> <none> Similarly, the pods from the other daemonset\u2014kindnet. pradeep@learnk8s$ kubectl get pods -A -o wide -o wide| grep kindnet kube-system kindnet-jpxdd 1/1 Running 1 5d12h 192.168.177.27 k8s <none> <none> kube-system kindnet-p77mb 1/1 Running 1 (2d ago) 5d12h 192.168.177.28 k8s-m02 <none> <none> A simple way to create a DaemonSet is to first generate a YAML file for a Deployment and remove the replicas, strategy fields. Change the kind from Deployment to DaemonSet. Here is an example (modfied from the demodeployment.yaml): pradeep@learnk8s$ cat demodaemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: demo-ds name: demo-ds spec: selector: matchLabels: app: demo-ds template: metadata: labels: app: demo-ds spec: containers: - image: nginx name: nginx-ds resources: {} pradeep@learnk8s$ kubectl create -f demodaemonset.yaml daemonset.apps/demo-ds created pradeep@learnk8s$ kubectl get ds NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE demo-ds 2 2 0 2 0 <none> 6s pradeep@learnk8s$ kubectl get pods -o wide | grep demo-ds demo-ds-gtcf7 1/1 Running 0 28s 10.244.1.5 k8s-m02 <none> <none> demo-ds-kkw4g 1/1 Running 0 28s 10.244.0.11 k8s <none> <none> Static Pods Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Unlike Pods that are managed by the control plane; instead, the kubelet watches each static Pod (and restarts it if it fails). The Pod names will be suffixed with the node hostname with a leading hyphen. Manifests are standard Pod definitions in JSON or YAML format in a specific directory. Use the staticPodPath: <the directory> field in the kubelet configuration file, which periodically scans the directory and creates/deletes static Pods as YAML/JSON files appear/disappear there. Let's explore our minikube environment to see if any Static Pods are there. First SSH to the minikube node and search for the kubelet process. The result shows all configuration parameteres used by the kubelet . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ps -aux | grep kubelet root 4244 11.6 4.5 1946788 100436 ? Ssl Feb08 238:40 /var/lib/minikube/binaries/v1.23.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --cni-conf-dir=/etc/cni/net.mk --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=k8s --housekeeping-interval=5m --kubeconfig=/etc/kubernetes/kubelet.conf --network-plugin=cni --node-ip=192.168.177.27 root 412294 15.3 13.5 1042556 296888 ? Ssl 01:40 10:27 kube-apiserver --advertise-address=192.168.177.27 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-account-signing-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key docker 429542 0.0 0.0 3348 448 pts/0 S+ 02:48 0:00 grep kubelet As seen above, the kubelet config settings are present in --config=/var/lib/kubelet/config.yaml . You can explore this file to get the staticPodPath . $ more /var/lib/kubelet/config.yaml | grep static staticPodPath: /etc/kubernetes/manifests Now that we know the staticPodPath , lets see what manifests are currently defined. $ ls /etc/kubernetes/manifests/ etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml $ There are four manifest files defined in this location. Looking at the name of the files, you can relate them to the Kubernetes core components. Let us re-visit the kube-system namespace and get all the pods running there. $ exit logout pradeep@learnk8s$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-64897985d-r9tzv 1/1 Running 8 (114m ago) 6d2h etcd-k8s 1/1 Running 2 (114m ago) 6d2h kindnet-jpxdd 1/1 Running 5 (8h ago) 6d2h kindnet-p77mb 1/1 Running 50 (118m ago) 6d2h kube-apiserver-k8s 1/1 Running 3 (8h ago) 6d2h kube-controller-manager-k8s 1/1 Running 4 (113m ago) 6d2h kube-proxy-fszkr 1/1 Running 1 (2d15h ago) 6d2h kube-proxy-m747v 1/1 Running 1 6d2h kube-scheduler-k8s 1/1 Running 2 (8h ago) 6d2h storage-provisioner 1/1 Running 49 (81m ago) 6d2h Pay special attention to the -k8s string in the name of the Pods: etcd-k8s , kube-apiserver-k8s , kube-controller-manager-k8 , and kube-scheduler-k8s . As mentioned earlier, The StaticPod names will be suffixed with the node hostname ( k8s in our case) with a leading hyphen. Create a Static Pod in the worker node Now that we have seen the static pods deployed by the system during cluster setup, let us manually deploy an nginx pod in the second node ( k8s-m02 ) of our minikube cluster k8s . For this first, we need to SSH to that node: k8s-m02 . To do this, add -n k8s-m02 option to the minikube ssh command that we have been using so far. pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a show eth0 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether be:02:3c:97:9d:85 brd ff:ff:ff:ff:ff:ff inet 192.168.177.28/24 brd 192.168.177.255 scope global dynamic eth0 valid_lft 47485sec preferred_lft 47485sec This confirms that we logged in to the k8s-m02 node which has the IP address: 192.168.177.28. Create a new file called nginx.yaml in the staicPodPath location, that is /etc/kubernetes/manifests/ . $ sudo vi /etc/kubernetes/manifests/nginx.yaml $ cat /etc/kubernetes/manifests/nginx.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: nginx name: nginx spec: containers: - image: nginx name: nginx resources: {} dnsPolicy: ClusterFirst restartPolicy: Always $ exit logout By the presence of this file in the k8s-m02 node, the kubelet creates this Pod automatically now. To verify, pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-6c54f77c95-mgz7f 1/1 Running 1 5d22h demo-6c54f77c95-q679r 1/1 Running 1 4d22h demo-6c54f77c95-qqzbf 1/1 Running 1 4d22h demo-6c54f77c95-vjgc2 1/1 Running 1 4d22h demo-6c54f77c95-wv78b 1/1 Running 1 5d22h demo-ds-gtcf7 1/1 Running 0 14h demo-ds-kkw4g 1/1 Running 0 14h nginx-k8s-m02 1/1 Running 0 18s nginx-manual 1/1 Running 1 4d21h nginx-no-tolerate 1/1 Running 1 4d14h nginx-node-selector 1/1 Running 0 2d15h nginx-taint-demo 1/1 Running 1 4d14h with-node-affinity 1/1 Running 0 2d15h Our nginx static pod is successfully Running on k8s-m02 node, as seen above, as pod named nginx-k8s-m02 . Another way to confirm that this Pod is indeed running on the k8s-m02 node is pradeep@learnk8s$ kubectl get pods -o wide | grep m02 demo-ds-gtcf7 1/1 Running 0 14h 10.244.1.5 k8s-m02 <none> <none> nginx-k8s-m02 1/1 Running 0 18m 10.244.1.6 k8s-m02 <none> <none> nginx-node-selector 1/1 Running 0 2d15h 10.244.1.2 k8s-m02 <none> <none> with-node-affinity 1/1 Running 0 2d15h 10.244.1.3 k8s-m02 <none> <none> Multiple Schedulers Kubernetes ships with a default scheduler ( kube-scheduler ) that we discussed earlier. If the default scheduler does not suit your needs you can implement your own scheduler. Moreover, you can even run multiple schedulers simultaneously alongside the default scheduler and instruct Kubernetes what scheduler to use for each of your pods. We can use the staticPod manifest file located at /etc/kubernetes/manifests/kube-scheduler.yaml to create our own scheduler. First, let us take a look at the definition of the default scheduler. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ cat /etc/kubernetes/manifests/ etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml $ sudo cat /etc/kubernetes/manifests/kube-scheduler.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: kube-scheduler tier: control-plane name: kube-scheduler namespace: kube-system spec: containers: - command: - kube-scheduler - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=false image: k8s.gcr.io/kube-scheduler:v1.23.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 10259 scheme: HTTPS initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 15 name: kube-scheduler resources: requests: cpu: 100m startupProbe: failureThreshold: 24 httpGet: host: 127.0.0.1 path: /healthz port: 10259 scheme: HTTPS initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 15 volumeMounts: - mountPath: /etc/kubernetes/scheduler.conf name: kubeconfig readOnly: true hostNetwork: true priorityClassName: system-node-critical securityContext: seccompProfile: type: RuntimeDefault volumes: - hostPath: path: /etc/kubernetes/scheduler.conf type: FileOrCreate name: kubeconfig status: {} $ The default scheduler listens on port 10259 , so for our custom scheduler, we have to choose another port, for example 10282 . Here is the definition of the new scheduler called my-scheduler , which is listening on the secure-port 10282 . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ sudo cat /etc/kubernetes/manifests/my-scheduler.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: my-scheduler tier: control-plane name: my-scheduler namespace: kube-system spec: containers: - command: - kube-scheduler - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=false - --secure-port=10282 image: k8s.gcr.io/kube-scheduler:v1.23.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 10282 scheme: HTTPS initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 15 name: my-scheduler resources: requests: cpu: 100m startupProbe: failureThreshold: 24 httpGet: host: 127.0.0.1 path: /healthz port: 10282 scheme: HTTPS initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 15 volumeMounts: - mountPath: /etc/kubernetes/scheduler.conf name: kubeconfig readOnly: true hostNetwork: true priorityClassName: system-node-critical securityContext: seccompProfile: type: RuntimeDefault volumes: - hostPath: path: /etc/kubernetes/scheduler.conf type: FileOrCreate name: kubeconfig status: {} $ Verify that both schedulers kube-scheduler-k8s and my-scheduler-k8s are in running state. pradeep@learnk8s$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-64897985d-r9tzv 1/1 Running 8 (3h2m ago) 6d3h etcd-k8s 1/1 Running 2 (3h2m ago) 6d3h kindnet-jpxdd 1/1 Running 5 (9h ago) 6d3h kindnet-p77mb 1/1 Running 50 (3h6m ago) 6d3h kube-apiserver-k8s 1/1 Running 3 (9h ago) 6d3h kube-controller-manager-k8s 1/1 Running 4 (3h1m ago) 6d3h kube-proxy-fszkr 1/1 Running 1 (2d16h ago) 6d3h kube-proxy-m747v 1/1 Running 1 6d3h kube-scheduler-k8s 1/1 Running 0 14m my-scheduler-k8s 1/1 Running 0 2m14s storage-provisioner 1/1 Running 49 (149m ago) 6d3h Now, it is time to deploy a Pod that makes use of this new scheduler. We just need to add one line for the schedulerName in the Pod spec. pradeep@learnk8s$ cat pod-my-scheduler.yaml apiVersion: v1 kind: Pod metadata: labels: run: nginx-my-scheduler name: nginx-my-scheduler spec: schedulerName: my-scheduler containers: - image: nginx name: nginx-my-scheduler resources: {} pradeep@learnk8s$ kubectl create -f pod-my-scheduler.yaml pod/nginx-my-scheduler created pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-6c54f77c95-mgz7f 1/1 Running 1 6d demo-6c54f77c95-q679r 1/1 Running 1 5d demo-6c54f77c95-qqzbf 1/1 Running 1 5d demo-6c54f77c95-vjgc2 1/1 Running 1 5d demo-6c54f77c95-wv78b 1/1 Running 1 6d demo-ds-gtcf7 1/1 Running 0 16h demo-ds-kkw4g 1/1 Running 0 16h nginx-k8s-m02 1/1 Running 0 137m nginx-manual 1/1 Running 1 4d23h nginx-my-scheduler 0/1 Pending 0 2s nginx-no-tolerate 1/1 Running 1 4d17h nginx-node-selector 1/1 Running 0 2d17h nginx-taint-demo 1/1 Running 1 4d17h with-node-affinity 1/1 Running 0 2d17h :memo: NOTE: There seems to be some issue with this, the Pod is in Pending state, I could not find any events. In the recent Kubernetes version 1.23, there seems to be some changes related to Scheduler definition. I will have to park this aside for some time and continue investigating. Logging and Monitoring To monitor cluster components, we need to deploy metrics-server . Metrics Server is a cluster-wide aggregator of resource usage data. Resource usage metrics, such as container CPU and memory usage, are available in Kubernetes through the Metrics API. These metrics can be accessed either directly by the user with the kubectl top command, or by a controller in the cluster, for example Horizontal Pod Autoscaler, to make decisions. Through the Metrics API, you can get the amount of resource currently used by a given node or a given pod. The minikube tool includes a set of built-in addons that can be enabled, disabled and opened in the local Kubernetes environment. pradeep@learnk8s$ minikube addons list -p k8s |-----------------------------|---------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|---------|--------------|--------------------------------| | ambassador | k8s | disabled | third-party (ambassador) | | auto-pause | k8s | disabled | google | | csi-hostpath-driver | k8s | disabled | kubernetes | | dashboard | k8s | disabled | kubernetes | | default-storageclass | k8s | enabled \u2705 | kubernetes | | efk | k8s | disabled | third-party (elastic) | | freshpod | k8s | disabled | google | | gcp-auth | k8s | disabled | google | | gvisor | k8s | disabled | google | | helm-tiller | k8s | disabled | third-party (helm) | | ingress | k8s | disabled | unknown (third-party) | | ingress-dns | k8s | disabled | google | | istio | k8s | disabled | third-party (istio) | | istio-provisioner | k8s | disabled | third-party (istio) | | kubevirt | k8s | disabled | third-party (kubevirt) | | logviewer | k8s | disabled | unknown (third-party) | | metallb | k8s | disabled | third-party (metallb) | | metrics-server | k8s | disabled | kubernetes | | nvidia-driver-installer | k8s | disabled | google | | nvidia-gpu-device-plugin | k8s | disabled | third-party (nvidia) | | olm | k8s | disabled | third-party (operator | | | | | framework) | | pod-security-policy | k8s | disabled | unknown (third-party) | | portainer | k8s | disabled | portainer.io | | registry | k8s | disabled | google | | registry-aliases | k8s | disabled | unknown (third-party) | | registry-creds | k8s | disabled | third-party (upmc enterprises) | | storage-provisioner | k8s | enabled \u2705 | google | | storage-provisioner-gluster | k8s | disabled | unknown (third-party) | | volumesnapshots | k8s | disabled | kubernetes | |-----------------------------|---------|--------------|--------------------------------| \ud83d\udca1 To see addons list for other profiles use: `minikube addons -p name list` Let us enable the metric-server addon now in our cluster. pradeep@learnk8s$ minikube addons enable metrics-server -p k8s \u25aa Using image k8s.gcr.io/metrics-server/metrics-server:v0.4.2 \ud83c\udf1f The 'metrics-server' addon is enabled Verify pradeep@learnk8s$ minikube addons list -p k8s |-----------------------------|---------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|---------|--------------|--------------------------------| | ambassador | k8s | disabled | third-party (ambassador) | | auto-pause | k8s | disabled | google | | csi-hostpath-driver | k8s | disabled | kubernetes | | dashboard | k8s | disabled | kubernetes | | default-storageclass | k8s | enabled \u2705 | kubernetes | | efk | k8s | disabled | third-party (elastic) | | freshpod | k8s | disabled | google | | gcp-auth | k8s | disabled | google | | gvisor | k8s | disabled | google | | helm-tiller | k8s | disabled | third-party (helm) | | ingress | k8s | disabled | unknown (third-party) | | ingress-dns | k8s | disabled | google | | istio | k8s | disabled | third-party (istio) | | istio-provisioner | k8s | disabled | third-party (istio) | | kubevirt | k8s | disabled | third-party (kubevirt) | | logviewer | k8s | disabled | unknown (third-party) | | metallb | k8s | disabled | third-party (metallb) | | metrics-server | k8s | enabled \u2705 | kubernetes | | nvidia-driver-installer | k8s | disabled | google | | nvidia-gpu-device-plugin | k8s | disabled | third-party (nvidia) | | olm | k8s | disabled | third-party (operator | | | | | framework) | | pod-security-policy | k8s | disabled | unknown (third-party) | | portainer | k8s | disabled | portainer.io | | registry | k8s | disabled | google | | registry-aliases | k8s | disabled | unknown (third-party) | | registry-creds | k8s | disabled | third-party (upmc enterprises) | | storage-provisioner | k8s | enabled \u2705 | google | | storage-provisioner-gluster | k8s | disabled | unknown (third-party) | | volumesnapshots | k8s | disabled | kubernetes | |-----------------------------|---------|--------------|--------------------------------| \ud83d\udca1 To see addons list for other profiles use: `minikube addons -p name list` pradeep@learnk8s$ kubectl get pod,svc,rs,deploy -n kube-system NAME READY STATUS RESTARTS AGE pod/coredns-64897985d-r9tzv 1/1 Running 8 (5h34m ago) 6d6h pod/etcd-k8s 1/1 Running 2 (5h34m ago) 6d6h pod/kindnet-jpxdd 1/1 Running 5 (11h ago) 6d6h pod/kindnet-p77mb 1/1 Running 50 (5h38m ago) 6d6h pod/kube-apiserver-k8s 1/1 Running 3 (11h ago) 6d6h pod/kube-controller-manager-k8s 1/1 Running 4 (5h34m ago) 6d6h pod/kube-proxy-fszkr 1/1 Running 1 (2d19h ago) 6d6h pod/kube-proxy-m747v 1/1 Running 1 6d6h pod/kube-scheduler-k8s 1/1 Running 0 166m pod/metrics-server-6b76bd68b6-7nbnx 1/1 Running 0 2m49s pod/my-scheduler-k8s 1/1 Running 0 89m pod/storage-provisioner 1/1 Running 51 (53m ago) 6d6h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 6d6h service/metrics-server ClusterIP 10.106.249.50 <none> 443/TCP 2m49s NAME DESIRED CURRENT READY AGE replicaset.apps/coredns-64897985d 1 1 1 6d6h replicaset.apps/metrics-server-6b76bd68b6 1 1 1 2m50s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/coredns 1/1 1 1 6d6h deployment.apps/metrics-server 1/1 1 1 2m50s As seen above, metric-server is deployed as a Deployment, and exposed as ClusterIP service. Use kubectl top node command to see CPU and memory utilization of each node in the cluster. pradeep@learnk8s$ kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s 334m 16% 1635Mi 76% k8s-m02 86m 4% 981Mi 45% Use kubectl top pod -A command to see CPU and memory utilization of each node in the cluster. pradeep@learnk8s$ kubectl top pod -A NAMESPACE NAME CPU(cores) MEMORY(bytes) default demo-6c54f77c95-mgz7f 0m 3Mi default demo-6c54f77c95-q679r 0m 3Mi default demo-6c54f77c95-qqzbf 0m 3Mi default demo-6c54f77c95-vjgc2 0m 3Mi default demo-6c54f77c95-wv78b 0m 3Mi default demo-ds-gtcf7 0m 3Mi default demo-ds-kkw4g 0m 3Mi default nginx-k8s-m02 0m 3Mi default nginx-manual 0m 3Mi default nginx-no-tolerate 0m 7Mi default nginx-node-selector 0m 9Mi default nginx-taint-demo 0m 3Mi default with-node-affinity 0m 3Mi kube-system coredns-64897985d-r9tzv 0m 12Mi kube-system etcd-k8s 0m 44Mi kube-system kindnet-jpxdd 0m 8Mi kube-system kindnet-p77mb 0m 7Mi kube-system kube-apiserver-k8s 0m 235Mi kube-system kube-controller-manager-k8s 0m 42Mi kube-system kube-proxy-fszkr 0m 16Mi kube-system kube-proxy-m747v 0m 22Mi kube-system kube-scheduler-k8s 0m 14Mi kube-system metrics-server-6b76bd68b6-7nbnx 0m 4Mi kube-system my-scheduler-k8s 0m 13Mi kube-system storage-provisioner 0m 8Mi prod quota-mem-cpu-demo 0m 3Mi You can also use the --sory-by field to display in descending order. pradeep@learnk8s$ kubectl top pod -A --sort-by=memory NAMESPACE NAME CPU(cores) MEMORY(bytes) kube-system kube-apiserver-k8s 0m 235Mi kube-system etcd-k8s 0m 44Mi kube-system kube-controller-manager-k8s 0m 42Mi kube-system kube-proxy-m747v 0m 22Mi kube-system kube-proxy-fszkr 0m 16Mi kube-system kube-scheduler-k8s 0m 14Mi kube-system my-scheduler-k8s 0m 13Mi kube-system coredns-64897985d-r9tzv 0m 12Mi default nginx-node-selector 0m 9Mi kube-system kindnet-jpxdd 0m 8Mi kube-system storage-provisioner 0m 8Mi default nginx-no-tolerate 0m 7Mi kube-system kindnet-p77mb 0m 7Mi kube-system metrics-server-6b76bd68b6-7nbnx 0m 4Mi default demo-ds-kkw4g 0m 3Mi default nginx-taint-demo 0m 3Mi default demo-6c54f77c95-mgz7f 0m 3Mi default nginx-manual 0m 3Mi default with-node-affinity 0m 3Mi default demo-ds-gtcf7 0m 3Mi default demo-6c54f77c95-wv78b 0m 3Mi default demo-6c54f77c95-q679r 0m 3Mi default demo-6c54f77c95-qqzbf 0m 3Mi default demo-6c54f77c95-vjgc2 0m 3Mi default nginx-k8s-m02 0m 3Mi prod quota-mem-cpu-demo 0m 3Mi Examine Pod Logs Use the kubectl logs <pod> name to look at the logs of the affected container pradeep@learnk8s$ kubectl logs nginx-manual /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh /docker-entrypoint.sh: Configuration complete; ready for start up 2022/02/07 11:34:31 [notice] 1#1: using the \"epoll\" event method 2022/02/07 11:34:31 [notice] 1#1: nginx/1.21.6 2022/02/07 11:34:31 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 2022/02/07 11:34:31 [notice] 1#1: OS: Linux 4.19.202 2022/02/07 11:34:31 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576 2022/02/07 11:34:31 [notice] 1#1: start worker processes 2022/02/07 11:34:31 [notice] 1#1: start worker process 32 2022/02/07 11:34:31 [notice] 1#1: start worker process 33 Kubectl exec If the container image includes debugging utilities, as is the case with images built from Linux and Windows OS base images, you can run commands inside a specific container with kubectl exec . Here is an example to login to the nginx-manual container and execute commands from the shell mode. pradeep@learnk8s$ kubectl exec -it nginx-manual -- sh # uname -a Linux nginx-manual 4.19.202 #1 SMP Thu Dec 23 10:44:17 UTC 2021 x86_64 GNU/Linux # exit 0 Application Life Cycle Management I ran into some issue with my cluster and had to rebuild from scratch. Here is how my new cluster looks like: pradeep@learnk8s$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s Ready control-plane,master 4m42s v1.23.1 192.168.177.29 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 k8s-m02 Ready <none> 4m21s v1.23.1 192.168.177.30 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 pradeep@learnk8s$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-64897985d-l9zmg 1/1 Running 0 4m34s kube-system etcd-k8s 1/1 Running 1 4m45s kube-system kindnet-4d4xb 1/1 Running 0 50s kube-system kindnet-z27gn 1/1 Running 0 4m30s kube-system kube-apiserver-k8s 1/1 Running 1 4m45s kube-system kube-controller-manager-k8s 1/1 Running 1 4m45s kube-system kube-proxy-n75w8 1/1 Running 0 4m34s kube-system kube-proxy-v2mq5 1/1 Running 0 4m30s kube-system kube-scheduler-k8s 1/1 Running 1 4m45s kube-system storage-provisioner 1/1 Running 0 4m43s Let us now create a deployment called kodekloud with image webapp-color and four replicas. Here is the Docker image that we are going to use: https://hub.docker.com/r/kodekloud/webapp-color/tags?page=1&ordering=-last_updated. It has many versions of the same image (like v1, v2, and v3). pradeep@learnk8s$ kubectl create deployment kodekloud --image=kodekloud/webapp-color --replicas=4 deployment.apps/kodekloud created pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kodekloud-589c9f4b47-6fbqg 1/1 Running 0 47s 10.244.1.8 k8s-m02 <none> <none> kodekloud-589c9f4b47-c75mz 1/1 Running 0 47s 10.244.1.7 k8s-m02 <none> <none> kodekloud-589c9f4b47-s54tz 1/1 Running 0 47s 10.244.0.7 k8s <none> <none> kodekloud-589c9f4b47-z2sqs 1/1 Running 0 47s 10.244.0.6 k8s <none> <none> pradeep@learnk8s$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 53s Verify the current Image being used by the deployment pradeep@learnk8s$ kubectl describe deployment kodekloud | grep Image Image: kodekloud/webapp-color pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.8:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-589c9f4b47-6fbqg!</h1> </div>$ curl 10.244.1.7:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-589c9f4b47-c75mz!</h1> </div>$ curl 10.244.0.7:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #2980b9;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-589c9f4b47-s54tz!</h1> </div>$ curl 10.244.0.6:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #be2edd;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-589c9f4b47-z2sqs!</h1> </div>$ Rolling Update Also, find out the strategy used by this deployment pradeep@learnk8s$ kubectl describe deployment kodekloud | grep Strategy StrategyType: RollingUpdate RollingUpdateStrategy: 25% max unavailable, 25% max surge With RollingUpdate strategy, Pods are upgraded few at a time. Now update the deployment to use a different version of the application kodekloud/webapp-color:v2 . For this, we need not delete or re-create the existing deployment. We can edit the deployment and modify the image name. Save with Esc + wq. pradeep@learnk8s$ kubectl edit deployments kodekloud # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" creationTimestamp: \"2022-02-15T07:13:38Z\" generation: 1 labels: app: kodekloud name: kodekloud namespace: default resourceVersion: \"1592\" uid: e5728c54-8768-4d5c-b1b0-c3e33cf62062 spec: progressDeadlineSeconds: 600 replicas: 4 revisionHistoryLimit: 10 selector: matchLabels: app: kodekloud strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: kodekloud spec: containers: - image: kodekloud/webapp-color:v2 imagePullPolicy: Always name: webapp-color resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler -- INSERT -- pradeep@learnk8s$ kubectl edit deployments kodekloud deployment.apps/kodekloud edited The pods should be recreated with new image. Looking at the AGE column, we can conclude that all four pods got re-created few seconds ago. If you notice carefully, the replicates is having a different Identifier now (changed from 589c9f4b47 to 8477b7849). pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE kodekloud-8477b7849-4lgg2 1/1 Running 0 66s kodekloud-8477b7849-b7grh 1/1 Running 0 44s kodekloud-8477b7849-ndf4w 1/1 Running 0 66s kodekloud-8477b7849-ww5qn 1/1 Running 0 47s pradeep@learnk8s$ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 16m If we look at the Events section of this deployment, we can clearly see that the deployment-controller has Scaled Up the new replica set kodekloud-8477b7849 gradually from 1 to 4 and at the same time, old replica set kodekloud-589c9f4b47 got Scaled down from 4 to 0. This is becuase of the RollingUpdateStrategy: 25% max unavailable, 25% max surge . For this current strategy, only one Pod can be down during the upgrade process (25%, meaning 1 out of total 4 pods in the deployment). pradeep@learnk8s$ kubectl describe deployments.apps kodekloud Name: kodekloud Namespace: default CreationTimestamp: Tue, 15 Feb 2022 12:43:38 +0530 Labels: app=kodekloud Annotations: deployment.kubernetes.io/revision: 2 Selector: app=kodekloud Replicas: 4 desired | 4 updated | 4 total | 4 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=kodekloud Containers: webapp-color: Image: kodekloud/webapp-color:v2 Port: <none> Host Port: <none> Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: kodekloud-8477b7849 (4/4 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 23m deployment-controller Scaled up replica set kodekloud-589c9f4b47 to 4 Normal ScalingReplicaSet 8m22s deployment-controller Scaled up replica set kodekloud-8477b7849 to 1 Normal ScalingReplicaSet 8m22s deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 3 Normal ScalingReplicaSet 8m22s deployment-controller Scaled up replica set kodekloud-8477b7849 to 2 Normal ScalingReplicaSet 8m4s deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 2 Normal ScalingReplicaSet 8m3s deployment-controller Scaled up replica set kodekloud-8477b7849 to 3 Normal ScalingReplicaSet 8m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 1 Normal ScalingReplicaSet 8m deployment-controller Scaled up replica set kodekloud-8477b7849 to 4 Normal ScalingReplicaSet 7m55s deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 0 Verify the Image being used by the deployment now. It says, v2 . pradeep@learnk8s$kubectl describe deployment kodekloud| grep Image Image: kodekloud/webapp-color:v2 Let's collect the new IP address of the Pods and verify the application from the minikube node. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kodekloud-8477b7849-4lgg2 1/1 Running 0 4m2s 10.244.0.8 k8s <none> <none> kodekloud-8477b7849-b7grh 1/1 Running 0 3m40s 10.244.0.9 k8s <none> <none> kodekloud-8477b7849-ndf4w 1/1 Running 0 4m2s 10.244.1.9 k8s-m02 <none> <none> kodekloud-8477b7849-ww5qn 1/1 Running 0 3m43s 10.244.1.10 k8s-m02 <none> <none> As seen here, the application is now running the v2 version as indicated by the new headers: <h2> Application Version: v2 </h2> pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.0.8:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-8477b7849-4lgg2!</h1> <h2> Application Version: v2 </h2> </div>$ curl 10.244.0.9:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-8477b7849-b7grh!</h1> <h2> Application Version: v2 </h2> </div>$ curl 10.244.1.9:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-8477b7849-ndf4w!</h1> <h2> Application Version: v2 </h2> </div>$ curl 10.244.1.10:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-8477b7849-ww5qn!</h1> <h2> Application Version: v2 </h2> </div>$ Another test is to verify the color of the web-app, it says green . $ curl 10.244.0.8:8080/color green$ curl 10.244.0.9:8080/color green$ curl 10.244.1.9:8080/color green$ curl 10.244.1.10:8080/color green$ Recreate There is another deployment strategy called Recreate . Let us edit the deployment again to use another version, v3, this time and change the strategy from RollingUpdate to Recreate . pradeep@learnk8s$ kubectl edit deployment kodekloud deployment.apps/kodekloud edited # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"2\" creationTimestamp: \"2022-02-15T07:13:38Z\" generation: 2 labels: app: kodekloud name: kodekloud namespace: default resourceVersion: \"2408\" uid: e5728c54-8768-4d5c-b1b0-c3e33cf62062 spec: progressDeadlineSeconds: 600 replicas: 4 revisionHistoryLimit: 10 selector: matchLabels: app: kodekloud strategy: type: Recreate template: metadata: creationTimestamp: null labels: app: kodekloud spec: containers: - image: kodekloud/webapp-color:v3 imagePullPolicy: Always name: webapp-color resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: -- INSERT -- pradeep@learnk8s$ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 33m pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kodekloud-676c6b9fcd-6bm6z 1/1 Running 0 53s 10.244.1.12 k8s-m02 <none> <none> kodekloud-676c6b9fcd-dggk7 1/1 Running 0 53s 10.244.0.10 k8s <none> <none> kodekloud-676c6b9fcd-m2q7x 1/1 Running 0 53s 10.244.0.11 k8s <none> <none> kodekloud-676c6b9fcd-p8tnl 1/1 Running 0 53s 10.244.1.11 k8s-m02 <none> <none> If we look at the latest events of the deployment, the deployment-controller Scaled down replica set kodekloud-8477b7849 to 0 at a time and Scaled up new replica set kodekloud-676c6b9fcd to 4 . pradeep@learnk8s$ kubectl describe deployments.apps Name: kodekloud Namespace: default CreationTimestamp: Tue, 15 Feb 2022 12:43:38 +0530 Labels: app=kodekloud Annotations: deployment.kubernetes.io/revision: 3 Selector: app=kodekloud Replicas: 4 desired | 4 updated | 4 total | 4 available | 0 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=kodekloud Containers: webapp-color: Image: kodekloud/webapp-color:v3 Port: <none> Host Port: <none> Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: kodekloud-676c6b9fcd (4/4 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 35m deployment-controller Scaled up replica set kodekloud-589c9f4b47 to 4 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set kodekloud-8477b7849 to 1 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 3 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set kodekloud-8477b7849 to 2 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 2 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set kodekloud-8477b7849 to 3 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 1 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set kodekloud-8477b7849 to 4 Normal ScalingReplicaSet 19m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 0 Normal ScalingReplicaSet 2m58s deployment-controller Scaled down replica set kodekloud-8477b7849 to 0 Normal ScalingReplicaSet 2m27s deployment-controller Scaled up replica set kodekloud-676c6b9fcd to 4 Just to verify, let us connect to one of the pods from the new version and check the output. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.12:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #e74c3c;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-676c6b9fcd-6bm6z!</h1> <h2> Application Version: v3 </h2> </div>$ From this, it is clear that the application got upgraded to latest version. Also, there is another test to verify the color used by the web-app. All four pods returned red . $ curl 10.244.1.12:8080/color red$ curl 10.244.1.11:8080/color red$ curl 10.244.0.11:8080/color red$ curl 10.244.0.10:8080/color red$ Commands and Arguments Let us make use of help, to understand the usage of commands and arguments. As shown here, with the help of --command option we can pass a different command and custom arguments to our container. pradeep@learnk8s$ kubectl run -h Create and run a particular image in a pod. Examples: <SNIP> # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command kubectl run nginx --image=nginx -- <arg1> <arg2> ... <argN> # Start the nginx pod using a different command and custom arguments kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN> Options: <SNIP> As an example, let us try to create a Ubuntu container that just prints date. pradeep@learnk8s$ kubectl run ubuntu-date --image=ubuntu --command date pod/ubuntu-date created Look at the Command header under Containers section. pradeep@learnk8s$ kubectl describe pods ubuntu-date Name: ubuntu-date Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Tue, 15 Feb 2022 13:55:40 +0530 Labels: run=ubuntu-date Annotations: <none> Status: Running IP: 10.244.1.21 IPs: IP: 10.244.1.21 Containers: ubuntu-date: Container ID: docker://3c6c897fe4ad67e0a90540ec086901eb63a5f8ab1d4afae85b7d0a734e363a9c Image: ubuntu Image ID: docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be Port: <none> Host Port: <none> Command: date State: Terminated Reason: Completed Exit Code: 0 Started: Tue, 15 Feb 2022 13:55:45 +0530 Finished: Tue, 15 Feb 2022 13:55:45 +0530 Ready: False Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-87bh2 (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: kube-api-access-87bh2: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9s default-scheduler Successfully assigned default/ubuntu-date to k8s-m02 Normal Pulled 4s kubelet Successfully pulled image \"ubuntu\" in 4.705982332s Normal Created 4s kubelet Created container ubuntu-date Normal Started 4s kubelet Started container ubuntu-date Normal Pulling 3s (x2 over 8s) kubelet Pulling image \"ubuntu\" The pod is Terminated with status Completed . pradeep@learnk8s$ kubectl get pods | grep date ubuntu-date 0/1 CrashLoopBackOff 3 (55s ago) 111s Let us create another container that uses an argument as well, in addition to the command. pradeep@learnk8s$ kubectl run ubuntu-sleep --image=ubuntu --command sleep 5000 pod/ubuntu-sleep created pradeep@learnk8s$ kubectl describe pods ubuntu-sleep Name: ubuntu-sleep Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Tue, 15 Feb 2022 13:59:46 +0530 Labels: run=ubuntu-sleep Annotations: <none> Status: Running IP: 10.244.1.22 IPs: IP: 10.244.1.22 Containers: ubuntu-sleep: Container ID: docker://4a12725e6951fd785eadb5bb692b25755ecfac7c664d2eb97305f46369f9d898 Image: ubuntu Image ID: docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be Port: <none> Host Port: <none> Command: sleep 5000 State: Running Started: Tue, 15 Feb 2022 13:59:56 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gpjg5 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-gpjg5: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 26s default-scheduler Successfully assigned default/ubuntu-sleep to k8s-m02 Normal Pulling 25s kubelet Pulling image \"ubuntu\" Normal Pulled 17s kubelet Successfully pulled image \"ubuntu\" in 8.387291792s Normal Created 17s kubelet Created container ubuntu-sleep Normal Started 16s kubelet Started container ubuntu-sleep Let us take a look at the definition of this Pod with command and arguments in YAML. pradeep@learnk8s$ kubectl get pods ubuntu-sleep -o yaml > pod-command-arg.yaml pradeep@learnk8s$ cat pod-command-arg.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: \"2022-02-15T08:29:46Z\" labels: run: ubuntu-sleep name: ubuntu-sleep namespace: default resourceVersion: \"5745\" uid: 971c6c08-da5e-4360-901b-7b8ca65a693a spec: containers: - command: - sleep - \"5000\" image: ubuntu imagePullPolicy: Always name: ubuntu-sleep resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-gpjg5 readOnly: true dnsPolicy: ClusterFirst <SNIP> Now let us use the same kodecloud/web-app image and try to pass the color argument while initializing the pod. Looking at the source code of this container kodekloudhub/webapp-color , we can find out that, A color can be specified in two ways: either as a command line argument or as an environment variable. from flask import Flask from flask import render_template import socket import random import os import argparse app = Flask(__name__) color_codes = { \"red\": \"#e74c3c\", \"green\": \"#16a085\", \"blue\": \"#2980b9\", \"blue2\": \"#30336b\", \"pink\": \"#be2edd\", \"darkblue\": \"#130f40\" } SUPPORTED_COLORS = \",\".join(color_codes.keys()) # Get color from Environment variable COLOR_FROM_ENV = os.environ.get('APP_COLOR') # Generate a random color COLOR = random.choice([\"red\", \"green\", \"blue\", \"blue2\", \"darkblue\", \"pink\"]) @app.route(\"/\") def main(): # return 'Hello' return render_template('hello.html', name=socket.gethostname(), color=color_codes[COLOR]) if __name__ == \"__main__\": print(\" This is a sample web application that displays a colored background. \\n\" \" A color can be specified in two ways. \\n\" \"\\n\" \" 1. As a command line argument with --color as the argument. Accepts one of \" + SUPPORTED_COLORS + \" \\n\" \" 2. As an Environment variable APP_COLOR. Accepts one of \" + SUPPORTED_COLORS + \" \\n\" \" 3. If none of the above then a random color is picked from the above list. \\n\" \" Note: Command line argument precedes over environment variable.\\n\" \"\\n\" \"\") # Check for Command Line Parameters for color parser = argparse.ArgumentParser() parser.add_argument('--color', required=False) args = parser.parse_args() if args.color: print(\"Color from command line argument =\" + args.color) COLOR = args.color if COLOR_FROM_ENV: print(\"A color was set through environment variable -\" + COLOR_FROM_ENV + \". However, color from command line argument takes precendence.\") elif COLOR_FROM_ENV: print(\"No Command line argument. Color from environment variable =\" + COLOR_FROM_ENV) COLOR = COLOR_FROM_ENV else: print(\"No command line argument or environment variable. Picking a Random Color =\" + COLOR) # Check if input color is a supported one if COLOR not in color_codes: print(\"Color not supported. Received '\" + COLOR + \"' expected one of \" + SUPPORTED_COLORS) exit(1) # Run Flask Application app.run(host=\"0.0.0.0\", port=8080) Let us try the first method that we just discussed: as a command argument. Create a pod using kodekloud/webapp-color:v3 but pass the --color argument to change the default color from red to blue . Earlier, when we ran this image (with defaults), the color was red . This can also be confirmed by looking at the Dockerfile definition. pradeep@learnk8s$ kubectl run kodekloud-change-color --image=kodekloud/webapp-color:v3 --dry-run=client -o yaml -- \"--color\" \"blue\" > pod-change-color.yaml Now we can see we are passing the --color and blue as arguements to this container. pradeep@learnk8s$ cat pod-change-color.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: kodekloud-change-color name: kodekloud-change-color spec: containers: - args: - --color - blue image: kodekloud/webapp-color:v3 name: kodekloud-change-color resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} Let us create a pod from this definition file. pradeep@learnk8s$ kubectl create -f pod-change-color.yaml pod/kodekloud-change-color created pradeep@learnk8s$ kubectl describe pods kodekloud-change-color Name: kodekloud-change-color Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Tue, 15 Feb 2022 16:31:54 +0530 Labels: run=kodekloud-change-color Annotations: <none> Status: Running IP: 10.244.1.29 IPs: IP: 10.244.1.29 Containers: kodekloud-change-color: Container ID: docker://3c6bf9642cb90fcc2f969a3043f3879d9a42917d7e15ed26905b2a340461609e Image: kodekloud/webapp-color:v3 Image ID: docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177 Port: <none> Host Port: <none> Args: --color blue State: Running Started: Tue, 15 Feb 2022 16:31:57 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6l7n4 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-6l7n4: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned default/kodekloud-change-color to k8s-m02 Normal Pulled 10s kubelet Container image \"kodekloud/webapp-color:v3\" already present on machine Normal Created 10s kubelet Created container kodekloud-change-color Normal Started 9s kubelet Started container kodekloud-change-color pradeep@learnk8s$ kubectl get pods -o wide | grep color kodekloud-change-color 1/1 Running 0 40s 10.244.1.29 k8s-m02 <none> <none> Verify the color of the webapp now, it should say blue instead of the default red . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.29:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #2980b9;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-change-color!</h1> <h2> Application Version: v3 </h2> </div>$ $ curl 10.244.1.29:8080/color blue$ exit logout It is confirmed the arguments that we passed worked! (\"background: #2980b9 indicates blue, as defined in the color_code, \"blue\": \"#2980b9\"). Environment Variables Now, it is time to try the other metod of passing arguments, via environment variables. From the kubectl run examples, we can see that environment variables can be set with the --env option. pradeep@learnk8s$ kubectl run -h Create and run a particular image in a pod. Examples: <SNIP> # Start a hazelcast pod and set environment variables \"DNS_DOMAIN=cluster\" and \"POD_NAMESPACE=default\" in the container kubectl run hazelcast --image=hazelcast/hazelcast --env=\"DNS_DOMAIN=cluster\" --env=\"POD_NAMESPACE=default\" <SNIP> Let us redploy the same image kodekloud/webapp-color:v3 but this time changing the color of the app with environment variable. pradeep@learnk8s$ kubectl run kodekloud-env-color --image=kodekloud/webapp-color:v3 --env=APP_COLOR=pink pod/kodekloud-env-color created pradeep@learnk8s$ kubectl get pods -o wide | grep env kodekloud-env-color 1/1 Running 0 12s 10.244.1.30 k8s-m02 <none> <none> pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.30:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #be2edd;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-env-color!</h1> <h2> Application Version: v3 </h2> </div>$ curl 10.244.1.30:8080/color pink$ $ exit logout The curl tests confirm that the application is using the pink color now, which is passed with the environment variable, APP_COLOR . If we look at the Pod description, Environment section, APP_COLOR: pink is seen as expected. pradeep@learnk8s$ kubectl describe pod kodekloud-env-color Name: kodekloud-env-color Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Tue, 15 Feb 2022 19:22:07 +0530 Labels: run=kodekloud-env-color Annotations: <none> Status: Running IP: 10.244.1.30 IPs: IP: 10.244.1.30 Containers: kodekloud-env-color: Container ID: docker://376016abd3ab8f65faa794cb746831f37c3ac16c0436598b08f9365bdd6176d4 Image: kodekloud/webapp-color:v3 Image ID: docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177 Port: <none> Host Port: <none> State: Running Started: Tue, 15 Feb 2022 19:22:08 +0530 Ready: True Restart Count: 0 Environment: APP_COLOR: pink Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xj9jz (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-xj9jz: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m26s default-scheduler Successfully assigned default/kodekloud-env-color to k8s-m02 Normal Pulled 2m25s kubelet Container image \"kodekloud/webapp-color:v3\" already present on machine Normal Created 2m25s kubelet Created container kodekloud-env-color Normal Started 2m25s kubelet Started container kodekloud-env-color Save the current running pod as a YAML file. pradeep@learnk8s$ kubectl get pods kodekloud-env-color -o yaml > pod-env-variable.yaml As this is a running Pod, the YAML output shows a lot of other details on Status, which is not required for now (to change the environment variable). pradeep@learnk8s$ cat pod-env-variable.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: \"2022-02-15T13:52:07Z\" labels: run: kodekloud-env-color name: kodekloud-env-color namespace: default resourceVersion: \"15077\" uid: bd7c01ae-553e-4f77-b589-6af6560dcc88 spec: containers: - env: - name: APP_COLOR value: pink image: kodekloud/webapp-color:v3 imagePullPolicy: IfNotPresent name: kodekloud-env-color resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-xj9jz readOnly: true dnsPolicy: ClusterFirst <SNIP> Create another copy of this YAML file and modify the definition to change the color fo the App to darkblue . pradeep@learnk8s$ cp pod-env-variable.yaml pod-env-variable-2.yaml pradeep@learnk8s$ vi pod-env-variable-2.yaml The modified version looks like this. pradeep@learnk8s$ cat pod-env-variable-2.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: \"2022-02-15T13:52:07Z\" labels: run: kodekloud-env-color-2 name: kodekloud-env-color-2 namespace: default spec: containers: - env: - name: APP_COLOR value: darkblue image: kodekloud/webapp-color:v3 imagePullPolicy: IfNotPresent name: kodekloud-env-color-2 resources: {} pradeep@learnk8s$ kubectl create -f pod-env-variable-2.yaml pod/kodekloud-env-color-2 created pradeep@learnk8s$ kubectl get pods -o wide | grep env kodekloud-env-color 1/1 Running 0 14m 10.244.1.30 k8s-m02 <none> <none> kodekloud-env-color-2 1/1 Running 0 16s 10.244.1.31 k8s-m02 <none> <none> We can verify that this new pod is using the darkblue color. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.31:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #130f40;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-env-color-2!</h1> <h2> Application Version: v3 </h2> </div>$ curl 10.244.1.31:8080/color darkblue$ $ curl 10.244.1.31:8080/color darkblue$ $ exit logout ConfigMaps As seen in the explanation, ConfigMaps hold the configuration data for pods to consume. For example, we could store the color of the Webapp from the previous example, in the form of a ConfigMap. pradeep@learnk8s$ kubectl explain configmaps KIND: ConfigMap VERSION: v1 DESCRIPTION: ConfigMap holds configuration data for pods to consume. Before we create any configmaps, let us verify if we have any configmaps already in our cluster. pradeep@learnk8s$ kubectl get configmaps NAME DATA AGE kube-root-ca.crt 1 7h16m There is one configmap named kube-root-ca.crt already present with one DATA. To see what is stored in this configmap, we can describe it and look at the Annotations. pradeep@learnk8s$ kubectl describe configmaps Name: kube-root-ca.crt Namespace: default Labels: <none> Annotations: kubernetes.io/description: Contains a CA bundle that can be used to verify the kube-apiserver when using internal endpoints such as the internal service IP or kubern... Data ==== ca.crt: ---- -----BEGIN CERTIFICATE----- MIIDBjCCAe6gAwIBAgIBATANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p a3ViZUNBMB4XDTIxMDYyODA0NTM1NloXDTMxMDYyNzA0NTM1NlowFTETMBEGA1UE AxMKbWluaWt1YmVDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKZu VYaX2I0BbQ7le2fCQiYL0UaoadaYm0UtRoYHCkGovRH4MBJlJipYp7IVZ+4oOgqZ VtBC2o9oRzXnWEM6pJyTsrMrEhtDyTxAeCLY5iVwkLRCxrqxvsCVeGsTaF+0SMCy PUXFaC20jrDVAxAUXs26Le0Fl2BjOv9k9K2hEScRuC0ogkF/oL+aC3BoGpdFmPVG S+R18kS1UyBGpUWkktyXhtEAmZVdHn+PGeMW2W3cuHyF4OEqQiXE5xcXt6TQG64F qgmE5xISVDp1/6VjlJeiVMqlyqZVm7dEKdtwcx6p8UKqnr7nL14STkzv6pDywk5v egCFa7sP+amk41K6VMECAwEAAaNhMF8wDgYDVR0PAQH/BAQDAgKkMB0GA1UdJQQW MBQGCCsGAQUFBwMCBggrBgEFBQcDATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQW BBSL+jGni3bexvU9wL8lBR0FeLmCQDANBgkqhkiG9w0BAQsFAAOCAQEAd/sJdAwa 6hOYM52vQj0CiYfNqFAs1rK7klxzpPVa07sYIO8n8FjyzGTead+FxzeuxnOBxibu y/447esB/RPE3d7hv+piQqT9FD7H/lskpHyIvffz4ai15P3DFtKQeY464bxVnynQ eK4dvzySrqxDs5Q5mMC1PQn9ap7VvIRnz1wEr6hlHMqJ31G58rmnZ5V+eJBBUYvG a39/4Q1PIMyk98T6QmJDTuLQngD8QJagcRMm4D2mzdieomt9jmIsrd5jINMhsC6d pL/PMobEfOxGsZxSKW9RC4/mabcuo+Dty+xAN/cYlOrq6zGSatvQXu/60iDSx8un D9XARLPvbRrYnA== -----END CERTIFICATE----- BinaryData ==== Events: <none> We can create configmaps in multiple different ways. Let us use the help option to see some examples. pradeep@learnk8s$ kubectl create configmap -h Create a config map based on a file, directory, or specified literal value. A single config map may package one or more key/value pairs. When creating a config map based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key, you may specify an alternate key. When creating a config map based on a directory, each file whose basename is a valid key in the directory will be packaged into the config map. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc). Aliases: configmap, cm Examples: # Create a new config map named my-config based on folder bar kubectl create configmap my-config --from-file=path/to/bar # Create a new config map named my-config with specified keys instead of file basenames on disk kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt # Create a new config map named my-config with key1=config1 and key2=config2 kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2 # Create a new config map named my-config from the key=value pairs in the file kubectl create configmap my-config --from-file=path/to/bar # Create a new config map named my-config from an env file kubectl create configmap my-config --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env <SNIP> Using this examples, let us create a configmap named webapp-color using --from-literal option to set the color to green . pradeep@learnk8s$ kubectl create configmap webapp-color --from-literal=APP_COLOR=green configmap/webapp-color created pradeep@learnk8s$ kubectl get cm NAME DATA AGE kube-root-ca.crt 1 7h26m webapp-color 1 5s pradeep@learnk8s$ kubectl describe cm webapp-color Name: webapp-color Namespace: default Labels: <none> Annotations: <none> Data ==== APP_COLOR: ---- green BinaryData ==== Events: <none> To use this configmap in a Pod definition, we need to make use of -envFrom and -configMapRef options in the Pod Spec. pradeep@learnk8s$ cat pod-config-map.yaml apiVersion: v1 kind: Pod metadata: name: kodekloud-cm spec: containers: - name: kodekloud-cm image: kodekloud/webapp-color:v3 envFrom: - configMapRef: name: webapp-color restartPolicy: Never pradeep@learnk8s$ kubectl create -f pod-config-map.yaml pod/kodekloud-cm created pradeep@learnk8s$ kubectl get pods -o wide | grep cm kodekloud-cm 1/1 Running 0 106s 10.244.0.14 k8s <none> <none> Verify that app color is changed to green . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.0.14:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-cm!</h1> <h2> Application Version: v3 </h2> </div>$ curl 10.244.0.14:8080/color green$ $ exit logout If we describe this pod, we can see the following section. Environment Variables from: webapp-color ConfigMap Optional: false pradeep@learnk8s$ kubectl describe pod kodekloud-cm Name: kodekloud-cm Namespace: default Priority: 0 Node: k8s/192.168.177.29 Start Time: Tue, 15 Feb 2022 20:09:51 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.0.14 IPs: IP: 10.244.0.14 Containers: kodekloud-cm: Container ID: docker://53b0a746b4c95c65bbdce09f416ca6762ec9ce78d2a9618187a86d80465ec349 Image: kodekloud/webapp-color:v3 Image ID: docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177 Port: <none> Host Port: <none> State: Running Started: Tue, 15 Feb 2022 20:09:53 +0530 Ready: True Restart Count: 0 Environment Variables from: webapp-color ConfigMap Optional: false Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sq9q4 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-sq9q4: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 10h default-scheduler Successfully assigned default/kodekloud-cm to k8s Normal Pulled 10h kubelet Container image \"kodekloud/webapp-color:v3\" already present on machine Normal Created 10h kubelet Created container kodekloud-cm Normal Started 10h kubelet Started container kodekloud-cm Warning NodeNotReady 9h (x2 over 10h) node-controller Node is not ready Secrets Secret holds secret data of a certain type. pradeep@learnk8s$ kubectl explain secrets KIND: Secret VERSION: v1 DESCRIPTION: Secret holds secret data of a certain type. The total bytes of the values in the Data field must be less than MaxSecretSize bytes. <SNIP> Verify if there are any secrets present in our cluster already! pradeep@learnk8s$ kubectl get secrets NAME TYPE DATA AGE default-token-b2xs6 kubernetes.io/service-account-token 3 17h There is one secret of type service-account-token . Describe it to see additional details of the secret. pradeep@learnk8s$ kubectl describe secrets Name: default-token-b2xs6 Namespace: default Labels: <none> Annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 5500622d-56e8-47c9-9440-7882c1d35512 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1111 bytes namespace: 7 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IkJGbThhZWVoY01TV3VDZ2hMU1RMenFlY0o2ckdsY2N3ZjB2ZFl5QWEtQm8ifQ.eyJpc3MiOiJrdWJlck5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VddC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tYjJ4czYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMjaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjU1MDA2MjJkLTU2ZTgtNDdjOS05NDQwLTc4ODJjMWQzNTUxMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.NGKbj5mw1EsZm4E14zDDpS2vNgL_L0WuGgP7Ex6k2TrXqKaO4wxX-ca3US1iSaUeYOIM4DLnXjDbXbP5J6YV-8ke--Yxhkks8iC_3tA9k1Q0YvK0RXS0T4WsL9i12sD44i-9LoJpL4Zpu3qJO-s4V5Plg9ifC69NpKoEu3CoOTMGmOX7DqDmGotogl5BJHflZekweX8GMOGP5WAv1FkUcROWyhj8wVMEFOzZIS8O8ssF67wHukWuUZ3IYDpCHy3QjQSKOWgeGGfFSDUXfW_mEQCE1ryl_c_-3TK7kGtlqN4l8aKrd2iBF-1MP4mdbq9WKDGBJDlZlGbHgPOZe2eBMw How to create secrets? Secrets can be of three types in Kubernetes: Docker Registry, Generic TLS. pradeep@learnk8s$ kubectl create secret -h Create a secret using specified subcommand. Available Commands: docker-registry Create a secret for use with a Docker registry generic Create a secret from a local file, directory, or literal value tls Create a TLS secret Usage: kubectl create secret [flags] [options] Use \"kubectl <command> --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). pradeep@learnk8s$ kubectl create secret generic -h Create a secret based on a file, directory, or specified literal value. A single secret may package one or more key/value pairs. When creating a secret based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key or you wish to chose your own, you may specify an alternate key. When creating a secret based on a directory, each file whose basename is a valid key in the directory will be packaged into the secret. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc). Examples: # Create a new secret named my-secret with keys for each file in folder bar kubectl create secret generic my-secret --from-file=path/to/bar # Create a new secret named my-secret with specified keys instead of names on disk kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub # Create a new secret named my-secret with key1=supersecret and key2=topsecret kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret # Create a new secret named my-secret using a combination of a file and a literal kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret # Create a new secret named my-secret from env files kubectl create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env <SNIP> Create a secret called webapp-color-secret and store the webapp color blue in it. pradeep@learnk8s$ kubectl create secret generic webapp-color-secret --from-literal=APP_COLOR=blue secret/webapp-color-secret created pradeep@learnk8s$ kubectl get secrets NAME TYPE DATA AGE default-token-b2xs6 kubernetes.io/service-account-token 3 18h webapp-color-secret Opaque 1 5s pradeep@learnk8s$ kubectl describe secrets webapp-color-secret Name: webapp-color-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== APP_COLOR: 4 bytes It is time to use this secret in a Pod definition. We use the secrets in the same way we used configmaps. Instead of configMapRef , it would be secretRef now in the envFrom section. pradeep@learnk8s$ cat pod-secret.yaml apiVersion: v1 kind: Pod metadata: name: kodekloud-secret spec: containers: - name: kodekloud-secret image: kodekloud/webapp-color:v3 envFrom: - secretRef: name: webapp-color-secret restartPolicy: Never pradeep@learnk8s$ kubectl create -f pod-secret.yaml pod/kodekloud-secret created pradeep@learnk8s$ kubectl get pods -o wide| grep secret kodekloud-secret 1/1 Running 0 114s 10.244.1.32 k8s-m02 <none> <none> To verify the color, login to the minikube node. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.32:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #2980b9;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-secret!</h1> <h2> Application Version: v3 </h2> </div>$ curl 10.244.1.32:8080/color blue$ exit logout We can see that the app is using the blue color as defined in the secret. As another final confirmation, we can describe this pod, to see Environment Variables from: webapp-color-secret Secret Optional: false pradeep@learnk8s$ kubectl describe pods kodekloud-secret Name: kodekloud-secret Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Wed, 16 Feb 2022 06:47:47 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.32 IPs: IP: 10.244.1.32 Containers: kodekloud-secret: Container ID: docker://799356d294bd0e1e6d37143ae8b12babcd9d50410d879ed099032dd4ceaec68a Image: kodekloud/webapp-color:v3 Image ID: docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177 Port: <none> Host Port: <none> State: Running Started: Wed, 16 Feb 2022 06:47:49 +0530 Ready: True Restart Count: 0 Environment Variables from: webapp-color-secret Secret Optional: false Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xw2ml (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-xw2ml: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned default/kodekloud-secret to k8s-m02 Normal Pulled 17s kubelet Container image \"kodekloud/webapp-color:v3\" already present on machine Normal Created 16s kubelet Created container kodekloud-secret Normal Started 16s kubelet Started container kodekloud-secret Multi Container Pods As per Kubernetes documentation, The primary reason that Pods can have multiple containers is to support helper applications that assist a primary application. Typical examples of helper applications are data pullers, data pushers, and proxies. Helper and primary applications often need to communicate with each other. Typically this is done through a shared filesystem, as shown in this example. We have not yet discussed volumes, but for now, it is sufficient to know that it is a storage component. pradeep@learnk8s$ cat multi-container-pod.yaml apiVersion: v1 kind: Pod metadata: name: multi-containers spec: restartPolicy: Never volumes: - name: common-data emptyDir: {} containers: - name: nginx image: nginx volumeMounts: - name: common-data mountPath: /usr/share/nginx/html - name: ubuntu image: ubuntu volumeMounts: - name: common-data mountPath: /ubuntu-data command: [\"/bin/sh\"] args: [\"-c\", \"echo Hello from the Ubuntu container which is visible in the nginx container as they are sharing the same storage common-data > /ubuntu-data/index.html\"] pradeep@learnk8s$ kubectl create -f multi-container-pod.yaml pod/multi-containers created pradeep@learnk8s$ kubectl get pods -o wide | grep multi multi-containers 1/2 NotReady 0 8m3s 10.244.1.34 k8s-m02 <none> <none> If you notice, though we have defined two containers (nginx and ubuntu) in this Pod definition, there is only a single IP (10.244.1.34 in this case). This is important to understand. All containers in a pod share the same network resources. Also, READY column shows 1/2 meaning, there are two containers but only one is Running. Let us find out more on this by describing it. pradeep@learnk8s$ kubectl describe pods multi-containers Name: multi-containers Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Wed, 16 Feb 2022 07:19:50 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.34 IPs: IP: 10.244.1.34 Containers: nginx: Container ID: docker://1cb2560ba88bf7f5a100c74246f10adecf6ce026d9f6031e43947c32ba021d3c Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: <none> Host Port: <none> State: Running Started: Wed, 16 Feb 2022 07:20:09 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /usr/share/nginx/html from common-data (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7jtgb (ro) ubuntu: Container ID: docker://d896bd674b06da660ee2e179d104af43344628dcf49edeb90630dc33fa63197e Image: ubuntu Image ID: docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be Port: <none> Host Port: <none> Command: /bin/sh Args: -c echo Hello from the Ubuntu container which is visible in the nginx container as they are sharing the same storage common-data > /ubuntu-data/index.html State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 16 Feb 2022 07:20:20 +0530 Finished: Wed, 16 Feb 2022 07:20:20 +0530 Ready: False Restart Count: 0 Environment: <none> Mounts: /ubuntu-data from common-data (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7jtgb (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: common-data: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: SizeLimit: <unset> kube-api-access-7jtgb: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m8s default-scheduler Successfully assigned default/multi-containers to k8s-m02 Normal Pulling 3m6s kubelet Pulling image \"nginx\" Normal Pulled 2m49s kubelet Successfully pulled image \"nginx\" in 17.095268831s Normal Created 2m49s kubelet Created container nginx Normal Started 2m49s kubelet Started container nginx Normal Pulling 2m49s kubelet Pulling image \"ubuntu\" Normal Pulled 2m38s kubelet Successfully pulled image \"ubuntu\" in 10.692871602s Normal Created 2m38s kubelet Created container ubuntu Normal Started 2m38s kubelet Started container ubuntu We can see that the nginx container is Running but the ubuntu container is terminated with Reason as Completed. That means, Ubuntu container has finished its task. The task that we have given to this container is to echo a message and write that to a file. That's all. To continue the verification, login to the nginx container. Remember the kubectl exec command?! pradeep@learnk8s$ kubectl exec -it multi-containers -c nginx -- /bin/bash root@multi-containers:/# curl localhost Hello from the Ubuntu container which is visible in the nginx container as they are sharing the same storage common-data root@multi-containers:/# exit exit The one difference that you might have noticed is the use of -c option here. When there are multiple containers in a pod, we need to be specific about which container we want to work with, by specifying its name. In this case , we want to login to the shell (bash) of the nginx container, and henc the -c nginx option. Also, we specify the command (/bin/bash) that we want to execute after the two dashes followed by a space ( -- ). We logged in to the nginx container, but the data written by the ubuntu container is visible here as both of them are using the common storage volume called common-data . Init Containers Earlier, we saw that a Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started. Init containers are exactly like regular containers, except: Init containers always run to completion. Each init container must complete successfully before the next one starts. If init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed. Here is an example with one InitContainer called wait-for-service which is basically looking for the presence of a service called my-service . It does this check by performing nslookup . Till the kebernetes service called my-service is present, the name resolution will not succeed and this initContainer will not be finishing. Once the service is created, the initContainer job is done and it proceeds to the main container, in this example, it is the ubuntu container. pradeep@learnk8s$ more pod-with-init-container.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-init-container spec: containers: - name: ubuntu image: ubuntu command: ['sh', '-c', 'echo The ubuntu container is running! && sleep 3600'] initContainers: - name: wait-for-service image: busybox:1.28 command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"] pradeep@learnk8s$ kubectl create -f pod-with-init-container.yaml pod/pod-with-init-container created Once we created the pod with initContainer, we can see the state as Init:0/1 meaning there is one init container but it is not yet ready. pradeep@learnk8s$ kubectl get pods -o wide | grep init pod-with-init-container 0/1 Init:0/1 0 7s <none> k8s <none> <none> Waited for some more time, still it is in Init:0/1 state. pradeep@learnk8s$ kubectl get pods -o wide | grep init pod-with-init-container 0/1 Init:0/1 0 96s 10.244.0.15 k8s <none> <none> Let us describe this pod to get additional details. pradeep@learnk8s$ kubectl describe pods pod-with-init-container Name: pod-with-init-container Namespace: default Priority: 0 Node: k8s/192.168.177.29 Start Time: Wed, 16 Feb 2022 07:48:13 +0530 Labels: <none> Annotations: <none> Status: Pending IP: IPs: <none> Init Containers: wait-for-service: Container ID: Image: busybox:1.28 Image ID: Port: <none> Host Port: <none> Command: sh -c until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done State: Waiting Reason: PodInitializing Ready: False Restart Count: 0 Environment: <none> apiVersion: v1 Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro) Containers: ubuntu: Container ID: Image: ubuntu Image ID: Port: <none> Host Port: <none> Command: sh -c echo The ubuntu container is running! && sleep 3600 State: Waiting Reason: PodInitializing Ready: False Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro) Conditions: Type Status Initialized False Ready False ContainersReady False PodScheduled True Volumes: kube-api-access-7gp7r: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 17s default-scheduler Successfully assigned default/pod-with-init-container to k8s Normal Pulling 14s kubelet Pulling image \"busybox:1.28\" As seen above, Init Container is in Waiting state and overall Pod status is Pending. To make the InitiContainer successful, let us create a service called myservice . Here is the sample definition of the service. pradeep@learnk8s$ cat init-container-service.yaml apiVersion: v1 kind: Service metadata: name: myservice spec: ports: - protocol: TCP port: 80 targetPort: 9376 Create the myservice from the YAML file. pradeep@learnk8s$ kubectl create -f init-container-service.yaml service/myservice created Once the service is created, verify if there is any change in the main Pod status. It seems there is a change. pradeep@learnk8s$ kubectl get pods -o wide | grep init pod-with-init-container 0/1 PodInitializing 0 108s 10.244.0.15 k8s <none> <none> It is not yet in running state, it might take some more time. In the meanwhile, let us inspect the logs of the InitContainer that we created wait-for-service . Remember the kubectl logs command?! and the -c option to specify a container name when there are multiple containers in a pod! pradeep@learnk8s$ kubectl logs pod-with-init-container -c wait-for-service Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local nslookup: can't resolve 'myservice.default.svc.cluster.local' waiting for myservice Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local waiting for myservice nslookup: can't resolve 'myservice.default.svc.cluster.local' Server: 10.96.0.10 nslookup: can't resolve 'myservice.default.svc.cluster.local' Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local <SNIP> nslookup: can't resolve 'myservice.default.svc.cluster.local' waiting for myservice Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: myservice.default.svc.cluster.local Address 1: 10.111.136.80 myservice.default.svc.cluster.local A lot of similar output is omitted for brevity, but we can clearly see that this container is continuosly checking for the resolution of myservice.default.svc.cluster.local but nslookup: can't resolve it. Finally, after we created the myservice Service, name resolution was successful ( the last line). It ( myservice.default.svc.cluster.local ) is resolved to the IP address of 10.111.136.80. This is nothing but the ClusterIP assigned to the myservice . pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 19h myservice ClusterIP 10.111.136.80 <none> 80/TCP 9m13s Going back to the main Pod, we can see it is in Running State. pradeep@learnk8s$ kubectl get pods -o wide | grep init pod-with-init-container 1/1 Running 0 12m 10.244.0.15 k8s <none> <none> If we describe it one more time, Init Container is in Terminated State with Reason as Completed. And the Ubuntu container is Running. pradeep@learnk8s$ kubectl describe pods pod-with-init-container Name: pod-with-init-container Namespace: default Priority: 0 Node: k8s/192.168.177.29 Start Time: Wed, 16 Feb 2022 07:48:13 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.0.15 IPs: IP: 10.244.0.15 Init Containers: wait-for-service: Container ID: docker://8b5668914aeef2c9c43eebf3ebf7b7289eff4efa96166198fc0c6db8cdfd358b Image: busybox:1.28 Image ID: docker-pullable://busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 Port: <none> Host Port: <none> Command: sh -c until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 16 Feb 2022 07:48:52 +0530 Finished: Wed, 16 Feb 2022 07:49:52 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro) Containers: ubuntu: Container ID: docker://60caee7150e608901a2fda93fdc6758c8aa5208ec69320ed8c7982d57efde9fb Image: ubuntu Image ID: docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be Port: <none> Host Port: <none> Command: sh -c echo The ubuntu container is running! && sleep 3600 State: Running Started: Wed, 16 Feb 2022 07:59:33 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-7gp7r: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12m default-scheduler Successfully assigned default/pod-with-init-container to k8s Normal Pulling 12m kubelet Pulling image \"busybox:1.28\" Normal Pulled 11m kubelet Successfully pulled image \"busybox:1.28\" in 35.840326348s Normal Created 11m kubelet Created container wait-for-service Normal Started 11m kubelet Started container wait-for-service Normal Pulling 10m kubelet Pulling image \"ubuntu\" Normal Pulled 50s kubelet Successfully pulled image \"ubuntu\" in 9m39.673051136s Normal Created 50s kubelet Created container ubuntu Normal Started 50s kubelet Started container ubuntu :new: :date: Rollout Let us take a look at the current deployments. pradeep@learnk8s$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 13d To manage the rollout of a resource like deployments, kubernetes provides the rollout option, with which we can view rollout history, see the status of the rollout and even undo a previous rollout. Here is the detailed help information. pradeep@learnk8s$ kubectl rollout -h Manage the rollout of a resource. Valid resource types include: * deployments * daemonsets * statefulsets Examples: # Rollback to the previous deployment kubectl rollout undo deployment/abc # Check the rollout status of a daemonset kubectl rollout status daemonset/foo Available Commands: history View rollout history pause Mark the provided resource as paused restart Restart a resource resume Resume a paused resource status Show the status of the rollout undo Undo a previous rollout Usage: kubectl rollout SUBCOMMAND [options] Use \"kubectl <command> --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). Let us use this option to view the history of our kodekloud deployment. pradeep@learnk8s$ kubectl rollout history deployment kodekloud deployment.apps/kodekloud REVISION CHANGE-CAUSE 1 <none> 3 <none> 4 <none> We do see that there are multiple revisions to this kodekloud deployment, but we do not see any CHANGE-CAUSE . Even in the description, we do see only one line related to deployment.kubernetes.io/revision . Pay attention to the Annotations section. pradeep@learnk8s$ kubectl describe deployments.apps Name: kodekloud Namespace: default CreationTimestamp: Tue, 15 Feb 2022 12:43:38 +0530 Labels: app=kodekloud Annotations: deployment.kubernetes.io/revision: 4 Selector: app=kodekloud Replicas: 4 desired | 4 updated | 4 total | 4 available | 0 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=kodekloud Containers: webapp-color: Image: kodekloud/webapp-color:v2 Port: <none> Host Port: <none> Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailable OldReplicaSets: <none> NewReplicaSet: kodekloud-8477b7849 (4/4 replicas created) Events: <none> Create a new deployment pradeep@learnk8s$ cat nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.20 ports: - containerPort: 80 ```` ```sh pradeep@learnk8s$ kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deployment created pradeep@learnk8s$ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 13d nginx-deployment 3/3 3 3 31s pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 1 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.20 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-7b96fbf5d8 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 33s deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 33s deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Update Deployment (using Kubectl Set) Earlier we have used kubectl edit to make changes to an existing deployment. Here we will explore another option kubectl set . pradeep@learnk8s$ kubectl set -h Configure application resources. These commands help you make changes to existing application resources. Available Commands: env Update environment variables on a pod template image Update the image of a pod template resources Update resource requests/limits on objects with pod templates selector Set the selector on a resource serviceaccount Update the service account of a resource subject Update the user, group, or service account in a role binding or cluster role binding Usage: kubectl set SUBCOMMAND [options] Use \"kubectl <command> --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). Let us set the image for our nginx-deployment to a new version 1.21 . pradeep@learnk8s$ kubectl set image deployment/nginx-deployment nginx=nginx:1.21 deployment.apps/nginx-deployment image updated We can see the rollout status like this, using the kubectl rollout status command. pradeep@learnk8s$ kubectl rollout status deployment/nginx-deployment deployment \"nginx-deployment\" successfully rolled out Describe the deployment pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 2 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-5778cd94ff (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 10m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 10m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 3m18s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 3m12s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 3m12s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 3m10s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 3m10s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 3m3s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 Kubectl Annotate Annotations are like labels (key/value pairs), they store arbitrary string values. pradeep@learnk8s$ kubectl annotate -h Update the annotations on one or more resources. All Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and system extensions may use annotations to store their own data. Attempting to set an annotation that already exists will fail unless --overwrite is set. If --resource-version is specified and does not match the current resource version on the server the command will fail. Use \"kubectl api-resources\" for a complete list of supported resources. Examples: # Update pod 'foo' with the annotation 'description' and the value 'my frontend' # If the same annotation is set multiple times, only the last value will be applied kubectl annotate pods foo description='my frontend' # Update a pod identified by type and name in \"pod.json\" kubectl annotate -f pod.json description='my frontend' # Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any existing value kubectl annotate --overwrite pods foo description='my frontend running nginx' # Update all pods in the namespace kubectl annotate pods --all description='my frontend running nginx' # Update pod 'foo' only if the resource is unchanged from version 1 kubectl annotate pods foo description='my frontend running nginx' --resource-version=1 # Update pod 'foo' by removing an annotation named 'description' if it exists # Does not require the --overwrite flag kubectl annotate pods foo description- Options: --all=false: Select all resources, in the namespace of the specified resource types. -A, --all-namespaces=false: If true, check the specified action in all namespaces. --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats. --dry-run='none': Must be \"none\", \"server\", or \"client\". If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource. --field-manager='kubectl-annotate': Name of the manager used to track field ownership. --field-selector='': Selector (field query) to filter on, supports '=', '==', and '!='.(e.g. --field-selector key1=value1,key2=value2). The server only supports a limited number of field queries per type. -f, --filename=[]: Filename, directory, or URL to files identifying the resource to update the annotation -k, --kustomize='': Process the kustomization directory. This flag can't be used together with -f or -R. --list=false: If true, display the annotations for a given resource. --local=false: If true, annotation will NOT contact api-server but run locally. -o, --output='': Output format. One of: json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file. --overwrite=false: If true, allow annotations to be overwritten, otherwise reject annotation updates that overwrite existing annotations. -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory. --resource-version='': If non-empty, the annotation update will only succeed if this is the current resource-version for the object. Only valid when specifying a single resource. -l, --selector='': Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). --show-managed-fields=false: If true, keep the managedFields when printing objects in JSON or YAML format. --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview]. Usage: kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version] [options] Use \"kubectl options\" for a list of global command-line options (applies to all commands). pradeep@learnk8s$ kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.21\" deployment.apps/nginx-deployment annotated pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 1 <none> 2 image updated to 1.21 Look at the Annotations section of the description. pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 2 kubernetes.io/change-cause: image updated to 1.21 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-5778cd94ff (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 14m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 14m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 6m57s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 6m51s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 6m51s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 6m49s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 6m49s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 6m42s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 We can see the details of a particular revision using the --revision option. pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment --revision=1 deployment.apps/nginx-deployment with revision #1 Pod Template: Labels: app=nginx pod-template-hash=7b96fbf5d8 Containers: nginx: Image: nginx:1.20 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment --revision=2 deployment.apps/nginx-deployment with revision #2 Pod Template: Labels: app=nginx pod-template-hash=5778cd94ff Annotations: kubernetes.io/change-cause: image updated to 1.21 Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Rollback If you've decided to undo the current rollout and rollback to the previous revision, you can do so. pradeep@learnk8s$ kubectl rollout undo deployment/nginx-deployment deployment.apps/nginx-deployment rolled back pradeep@learnk8s$ kubectl rollout status deployment nginx-deployment deployment \"nginx-deployment\" successfully rolled out pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 2 image updated to 1.21 3 <none> pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 3 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.20 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-7b96fbf5d8 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 20m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 13m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 12m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 12m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 12m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 Normal ScalingReplicaSet 50s deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 48s deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 48s deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 46s deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 45s (x2 over 20m) deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 43s deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 0 Let us annotate revision 3. pradeep@learnk8s$ kubectl annotate deployments.apps nginx-deployment kubernetes.io/change-cause=\"image rolledback to 1.20\" deployment.apps/nginx-deployment annotated pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 2 image updated to 1.21 3 image rolledback to 1.20 What happens if we rollback the deployment again? pradeep@learnk8s$ kubectl rollout undo deployment/nginx-deployment deployment.apps/nginx-deployment rolled back pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 3 image rolledback to 1.20 4 image updated to 1.21 pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment --revision=4 deployment.apps/nginx-deployment with revision #4 Pod Template: Labels: app=nginx pod-template-hash=5778cd94ff Annotations: kubernetes.io/change-cause: image updated to 1.21 Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 4 kubernetes.io/change-cause: image updated to 1.21 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-5778cd94ff (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 31m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 23m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 23m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 23m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 11m (x2 over 31m) deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 0 Normal ScalingReplicaSet 2m27s (x2 over 24m) deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 2m25s (x2 over 23m) deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 2m25s (x2 over 23m) deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 2m20s (x3 over 2m23s) deployment-controller (combined from similar events): Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 Let us update the deployment one more time, finally to the latest version. pradeep@learnk8s$ kubectl set image deployment/nginx-deployment nginx=nginx:latest deployment.apps/nginx-deployment image updated Check the rollout history, and note that the existing annotation continued to the latest revision. pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 3 image rolledback to 1.20 4 image updated to 1.21 5 image updated to 1.21 Change the annotation pradeep@learnk8s$ kubectl annotate deployments.apps nginx-deployment kubernetes.io/change-cause=\"image updated to the latest\" deployment.apps/nginx-deployment annotated pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 3 image rolledback to 1.20 4 image updated to 1.21 5 image updated to the latest pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 5 kubernetes.io/change-cause: image updated to the latest Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:latest Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-67dffbbbb (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 40m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 32m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 32m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 32m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 20m (x2 over 40m) deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 0 Normal ScalingReplicaSet 11m (x2 over 33m) deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 11m (x2 over 33m) deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 11m (x2 over 33m) deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 2m52s (x9 over 11m) deployment-controller (combined from similar events): Scaled down replica set nginx-deployment-5778cd94ff to 0 pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE kodekloud-8477b7849-blzrs 1/1 Running 1 13d kodekloud-8477b7849-m65m8 1/1 Running 1 13d kodekloud-8477b7849-p7psw 1/1 Running 0 32h kodekloud-8477b7849-vf9zs 1/1 Running 0 32h kodekloud-cm 0/1 ContainerStatusUnknown 1 13d nginx-deployment-67dffbbbb-7ttxq 1/1 Running 0 3m35s nginx-deployment-67dffbbbb-8dtsf 1/1 Running 0 3m38s nginx-deployment-67dffbbbb-sz5bb 1/1 Running 0 3m40s pod-with-hostpath-volume 0/1 ContainerCreating 1 8d pod-with-init-container 1/1 Running 45 (4m46s ago) 12d security-context-demo-cap 1/1 Running 15 (5m16s ago) 9d pradeep@learnk8s$ kubectl get rs NAME DESIRED CURRENT READY AGE kodekloud-589c9f4b47 0 0 0 13d kodekloud-676c6b9fcd 0 0 0 13d kodekloud-8477b7849 4 4 4 13d nginx-deployment-5778cd94ff 0 0 0 34m nginx-deployment-67dffbbbb 3 3 3 3m52s nginx-deployment-7b96fbf5d8 0 0 0 41m pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment --revision=5 deployment.apps/nginx-deployment with revision #5 Pod Template: Labels: app=nginx pod-template-hash=67dffbbbb Annotations: kubernetes.io/change-cause: image updated to the latest Containers: nginx: Image: nginx:latest Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Security Certificates Take a look at the description of the kube-apiserver pod in the kube-system namespace. Pay attention to the kube-apiserver command arguments. It is a long list!! Primarily, there are many pairs of certfile and keyfile for apiserver, etcd, kubectl etc. In Minikube setup, most of these are located in /var/lib/minikube/certs/ . It would be different when the cluster is setup with kubeadm which we have not discussed yet. pradeep@learnk8s$ kubectl describe -n kube-system pods kube-apiserver-k8s Name: kube-apiserver-k8s Namespace: kube-system Priority: 2000001000 Priority Class Name: system-node-critical Node: k8s/192.168.177.29 Start Time: Tue, 15 Feb 2022 12:28:03 +0530 Labels: component=kube-apiserver tier=control-plane Annotations: kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.177.29:8443 kubernetes.io/config.hash: cca804e910f3a6e748c66a6963d63fdd kubernetes.io/config.mirror: cca804e910f3a6e748c66a6963d63fdd kubernetes.io/config.seen: 2022-02-15T06:58:02.427574419Z kubernetes.io/config.source: file seccomp.security.alpha.kubernetes.io/pod: runtime/default Status: Running IP: 192.168.177.29 IPs: IP: 192.168.177.29 Controlled By: Node/k8s Containers: kube-apiserver: Container ID: docker://0ec9bc91aa13e593b1518fac7a4f9f39c7e16a0e478c2362336b8c050f9c085c Image: k8s.gcr.io/kube-apiserver:v1.23.1 Image ID: docker-pullable://k8s.gcr.io/kube-apiserver@sha256:f54681a71cce62cbc1b13ebb3dbf1d880f849112789811f98b6aebd2caa2f255 Port: <none> Host Port: <none> Command: kube-apiserver --advertise-address=192.168.177.29 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-account-signing-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key State: Running Started: Tue, 15 Feb 2022 12:27:50 +0530 Ready: True Restart Count: 1 Requests: cpu: 250m Liveness: http-get https://192.168.177.29:8443/livez delay=10s timeout=15s period=10s #success=1 #failure=8 Readiness: http-get https://192.168.177.29:8443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3 Startup: http-get https://192.168.177.29:8443/livez delay=10s timeout=15s period=10s #success=1 #failure=24 Environment: <none> Mounts: /etc/ssl/certs from ca-certs (ro) /usr/share/ca-certificates from usr-share-ca-certificates (ro) /var/lib/minikube/certs from k8s-certs (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: ca-certs: Type: HostPath (bare host directory volume) Path: /etc/ssl/certs HostPathType: DirectoryOrCreate k8s-certs: Type: HostPath (bare host directory volume) Path: /var/lib/minikube/certs HostPathType: DirectoryOrCreate usr-share-ca-certificates: Type: HostPath (bare host directory volume) Path: /usr/share/ca-certificates HostPathType: DirectoryOrCreate QoS Class: Burstable Node-Selectors: <none> Tolerations: :NoExecute op=Exists Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 178m (x5 over 30h) kubelet Liveness probe failed: Get \"https://192.168.177.29:8443/livez\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 177m (x17 over 30h) kubelet Readiness probe failed: Get \"https://192.168.177.29:8443/readyz\": net/http: TLS handshake timeout Warning Unhealthy 170m (x7 over 27h) kubelet Readiness probe failed: Get \"https://192.168.177.29:8443/readyz\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 170m (x6 over 27h) kubelet Liveness probe failed: Get \"https://192.168.177.29:8443/livez\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 170m (x8 over 30h) kubelet Readiness probe failed: Get \"https://192.168.177.29:8443/readyz\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) We can login to the minikube node and use the openssl x509 command to view the actual certificate. Here is an example. Pass the certificate path as -in argument and use -text to display the certificate in plain-text. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ openssl x509 -in /var/lib/minikube/certs/apiserver.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 2 (0x2) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = minikubeCA Validity Not Before: Feb 14 06:54:36 2022 GMT Not After : Feb 14 06:54:36 2025 GMT Subject: O = system:masters, CN = minikube Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: 00:c7:e1:19:9b:17:43:df:ff:31:4e:fe:66:7c:4b: c4:1a:63:e6:5d:a6:a2:85:1c:11:b9:5a:72:42:50: 8b:25:a4:f0:89:eb:24:bf:3b:a6:e6:79:26:b8:18: ed:9b:7b:76:01:68:a4:b1:1b:39:d8:b8:36:14:21: 44:b5:26:57:a3:a6:d3:55:e2:8c:32:5b:55:71:1e: 47:3e:56:b6:e8:92:86:af:aa:90:d1:4a:5a:36:ac: a7:4f:a4:6c:09:a6:16:3b:e7:76:bc:41:18:89:7e: be:87:df:c7:a9:ee:b7:da:34:43:ae:9f:37:cd:5d: 8d:e2:71:5c:e6:4c:e4:60:46:8c:b1:ef:7d:90:4b: 51:c3:e3:7f:a7:84:fe:06:28:1a:28:18:fd:9a:00: b0:a7:d9:c9:b1:61:c9:d7:81:2d:c1:5d:5b:d2:f3: f6:13:e4:d8:7f:d6:5c:c0:39:56:b1:14:04:f6:b7: ea:9b:50:d7:aa:4d:f2:20:89:8b:8b:bc:81:b0:91: 4f:9b:f2:b9:69:b5:ce:80:67:a4:9e:f3:ba:17:03: f6:89:ee:22:0e:8d:65:61:ef:16:96:67:dc:d7:4f: ea:aa:36:7a:0c:59:53:2d:2a:fd:01:0f:93:15:fa: 8d:42:94:da:f1:0d:c8:8e:6b:15:b3:8b:4f:de:1d: 03:2b Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Authority Key Identifier: keyid:8B:FA:31:A7:8B:76:DE:C6:F5:3D:C0:BF:25:05:1D:05:78:B9:82:40 X509v3 Subject Alternative Name: DNS:minikubeCA, DNS:control-plane.minikube.internal, DNS:kubernetes.default.svc.cluster.local, DNS:kubernetes.default.svc, DNS:kubernetes.default, DNS:kubernetes, DNS:localhost, IP Address:192.168.177.29, IP Address:10.96.0.1, IP Address:127.0.0.1, IP Address:10.0.0.1 Signature Algorithm: sha256WithRSAEncryption 8e:94:63:81:ad:57:80:84:2d:89:8b:3c:af:7c:13:d1:6c:49: 53:53:61:cc:cb:bc:9d:63:93:9b:4e:b2:0e:a0:e3:9d:22:e4: 4e:a9:de:75:88:05:23:46:bb:75:4c:be:ff:ba:68:e3:19:d0: 15:b2:6a:01:5d:5b:ea:d0:a2:2d:53:80:99:25:e9:4f:f0:1a: 65:47:c3:e4:8e:06:6c:db:23:55:57:64:f3:0d:5a:4a:e8:63: b2:b6:57:00:13:85:29:fe:e0:de:06:d6:e3:ec:f3:96:1d:5c: e7:03:8f:46:d9:bf:6b:f5:dd:1a:41:db:15:23:14:36:42:c3: c7:34:28:2e:a3:c4:e8:99:29:6c:28:9b:40:35:aa:58:0e:4a: b4:fd:0b:b4:11:a6:c5:f4:10:97:9b:c8:1c:ec:ea:a0:77:7c: c2:b1:70:c6:7b:85:34:8a:36:b0:ca:35:6a:7c:1c:e9:4e:08: 9c:f9:be:de:41:ce:84:5e:51:60:52:e0:63:89:a7:18:1f:23: 3e:f2:8e:0c:d6:9d:d2:38:04:cd:cc:2c:2e:70:c8:57:99:2b: 3e:ba:08:1f:86:f4:0f:39:63:55:71:33:bc:49:ac:44:cf:e6: 4f:27:dd:78:45:88:13:a7:57:d1:a3:09:76:cb:06:00:4b:84: df:ac:cb:0e -----BEGIN CERTIFICATE----- MIID3DCCAsSgAwIBAgIBAjANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p a3ViZUNBMB4XDTIyMDIxNDA2NTQzNloXDTI1MDIxNDA2NTQzNlowLDEXMBUGA1UE ChMOc3lzdGVtOm1hc3RlcnMxETAPBgNVBAMTCG1pbmlrdWJlMIIBIjANBgkqhkiG 9w0BAQEFAAOCAQ8AMIIBCgKCAQEAx+EZmxdD3/8xTv5mfEvEGmPmXaaihRwRuVpy QlCLJaTwieskvzum5nkmuBjtm3t2AWigsRs52Lg2FCFEtSZXo6bTVeKMMltVcR5H Pla26JKGr6qQ0UpaNqynT6RsCaYWO+d2vEEYiX6+h9/Hqe632jRDrp83zV2N4nFc 5kzkYEaMse99kEtRw+N/p4T+BigaKBj9mgCwp9nJsWHJ14EtwV1b0vP2E+TYf9Zc wDlWsRQE9rfqm1DXqk3yIImLi7yBsJFPm/K5abXOgGeknvO6FwP2ie4iDo1lYe8W lmfc10/qqjZ6DFlTLSr9AQ+TFfqNQpTa8Q3IjmsVs4tP3h0DKwIDAQABo4IBHjCC ARowDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEFBQcD AjAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFIv6MadLdt7G9T3AvyUFHQV4uYJA MIG5BgNVHREEgbEwgb6CCm1pbmlrdWJlQ0GCH2NvbnRyb2wtcGxhbmUubWluaWt1 YmUuaW50ZXJuYWyCJGt1YmVybmV0ZXMuZGVmYXVsdC5zdmMuY2x1c3Rlci5sb2Nh bIIWa3ViZXJuZXRlcy5kZWZhdWx0LnN2Y4ISa3ViZXJuZXRlcy5kZWZhdWx0ggpr dWJlcm5ldGVzgglsb2NhbGhvc3SHBMCosR2HBApgAAGHBH8AAAGHBAoAAAEwDQYJ KoZIhvcNAQELBQADggEBAI6UY4GtV4CELYmLPK98E9FsSVNTYczLvJ1jk5tOsg6g 450i5E6p3nWIBSNGu3VMvv+6aOMZ0BWyagFdW+rQoi1TgJkl6U/wGmVHw+SOBmzb I1VXZPMNWkroY7K2VwAThSn+4N4G1uPs85YdXOcDj0bZv2v13Roh2xUjFDZCw8c0 KC6jxOiZKWwom0A1qlgOSrT9C7QRpsX0EJtbyBzs6qB3fMKxcMZ7hTSKNrDKNWp8 HOlOCJz5vt5BzoReUWBS4GOJpxgfIz7yjgzWndI4BM3MLC5wyFeZKz66CB+G9A85 Y1VxM7xJrETP5k8n3XhFiBOnV9GjCXbLBgBLhN+syw4= -----END CERTIFICATE----- $ We can see that the Issuer: CN = minikubeCA , Subject: O = system:masters, CN = minikube and Subject Alternative Name: DNS:minikubeCA, DNS:control-plane.minikube.internal, DNS:kubernetes.default.svc.cluster.local, DNS:kubernetes.default.svc, DNS:kubernetes.default, DNS:kubernetes, DNS:localhost, IP Address:192.168.177.29, IP Address:10.96.0.1, IP Address:127.0.0.1, IP Address:10.0.0.1 . Similarly, let us take a look at the description of the etcd pod in the kube-system namespace and pay attention to the etcd command arguments. Again a long list, but for now, just look for cert and key files. pradeep@learnk8s$ kubectl describe -n kube-system pods etcd-k8s Name: etcd-k8s Namespace: kube-system Priority: 2000001000 Priority Class Name: system-node-critical Node: k8s/192.168.177.29 Start Time: Tue, 15 Feb 2022 12:28:03 +0530 Labels: component=etcd tier=control-plane Annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.177.29:2379 kubernetes.io/config.hash: 553a1d887eba16384375f475194d677c kubernetes.io/config.mirror: 553a1d887eba16384375f475194d677c kubernetes.io/config.seen: 2022-02-15T06:58:02.427571167Z kubernetes.io/config.source: file seccomp.security.alpha.kubernetes.io/pod: runtime/default Status: Running IP: 192.168.177.29 IPs: IP: 192.168.177.29 Controlled By: Node/k8s Containers: etcd: Container ID: docker://7ad2e9b1c5fb22610382e026ecea82c4def63655f4771b398416b6dfd7b88374 Image: k8s.gcr.io/etcd:3.5.1-0 Image ID: docker-pullable://k8s.gcr.io/etcd@sha256:64b9ea357325d5db9f8a723dcf503b5a449177b17ac87d69481e126bb724c263 Port: <none> Host Port: <none> Command: etcd --advertise-client-urls=https://192.168.177.29:2379 --cert-file=/var/lib/minikube/certs/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/minikube/etcd --initial-advertise-peer-urls=https://192.168.177.29:2380 --initial-cluster=k8s=https://192.168.177.29:2380 --key-file=/var/lib/minikube/certs/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.177.29:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.177.29:2380 --name=k8s --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/var/lib/minikube/certs/etcd/peer.key --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt --proxy-refresh-interval=70000 --snapshot-count=10000 --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt State: Running Started: Tue, 15 Feb 2022 12:27:49 +0530 Ready: True Restart Count: 1 Requests: cpu: 100m memory: 100Mi Liveness: http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8 Startup: http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24 Environment: <none> Mounts: /var/lib/minikube/certs/etcd from etcd-certs (rw) /var/lib/minikube/etcd from etcd-data (rw) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: etcd-certs: Type: HostPath (bare host directory volume) Path: /var/lib/minikube/certs/etcd HostPathType: DirectoryOrCreate etcd-data: Type: HostPath (bare host directory volume) Path: /var/lib/minikube/etcd HostPathType: DirectoryOrCreate QoS Class: Burstable Node-Selectors: <none> Tolerations: :NoExecute op=Exists Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 3h18m (x12 over 28h) kubelet Liveness probe failed: Get \"http://127.0.0.1:2381/health\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) ETCD can have its own Certificate Authority (CA). Let us verify if both api-server and etcd are using the same CA certificate or different. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ openssl x509 -in /var/lib/minikube/certs/etcd/ca.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 0 (0x0) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = etcd-ca Validity Not Before: Feb 15 06:54:39 2022 GMT Not After : Feb 13 06:54:39 2032 GMT Subject: CN = etcd-ca Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: 00:c2:8c:1f:12:73:c7:89:bc:f9:8d:14:c2:5e:61: 2f:ae:d4:f0:48:89:f7:cc:bc:be:a6:19:9d:a8:4d: 85:99:59:d6:96:57:35:9e:6c:62:39:31:12:e6:ee: 32:f9:a4:aa:ce:91:8b:5f:1d:7d:99:cc:b5:fc:c7: 14:75:f0:a2:43:4f:fd:e6:e7:b3:ab:11:c2:5f:db: a8:72:b8:80:2f:66:5f:98:21:3b:06:af:b9:09:69: 94:1a:06:33:96:2f:1c:c7:f8:b2:ca:bd:87:d9:13: 36:c5:f6:de:aa:6a:81:c2:d4:94:2b:9e:63:dc:56: 27:b4:32:31:1d:49:ab:69:0e:dc:d1:14:d3:bb:f1: 43:80:19:31:73:29:7e:7b:d4:3b:2d:cf:14:7f:3b: 3c:84:4b:21:a4:2d:a6:59:79:bb:6f:1d:dc:5c:d5: 44:fd:3f:bd:34:b4:33:38:0d:cc:76:48:e1:de:53: 02:2d:54:79:44:22:64:f5:a6:39:1e:87:24:6c:91: 3c:5f:eb:7f:1f:84:38:e0:96:19:1b:46:9d:fc:e6: 79:98:c7:2e:6e:2a:2e:63:f0:28:42:57:16:14:45: f3:de:bf:32:8e:d9:49:e5:ab:a5:06:6e:0d:d2:9c: 8b:65:40:46:17:43:3e:d2:46:53:bf:22:89:72:d0: 6f:49 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment, Certificate Sign X509v3 Basic Constraints: critical CA:TRUE X509v3 Subject Key Identifier: E3:7B:E5:78:30:93:55:DB:08:E4:EB:D0:2E:0C:30:6C:DB:17:40:40 X509v3 Subject Alternative Name: DNS:etcd-ca Signature Algorithm: sha256WithRSAEncryption 1a:29:48:3a:dc:24:57:35:20:b7:21:31:ed:c5:12:88:d5:79: 7e:42:ca:21:12:92:64:49:5e:eb:2e:60:ce:16:71:5f:43:76: 09:97:3d:6c:19:15:16:b0:6d:fc:c9:84:14:e7:5b:c2:c8:d4: 24:77:bf:fd:87:3d:e4:c9:e4:39:49:ba:f6:41:bb:a0:9c:97: ef:71:b0:46:a1:86:dc:00:6b:19:26:39:32:26:c4:0c:70:4e: bf:1b:6b:93:55:54:d7:97:89:07:5e:e9:3b:63:a5:49:4e:6c: 21:aa:75:8b:b1:a9:94:68:bf:1c:2e:cf:84:09:b0:52:03:62: 72:54:b0:e2:c8:63:88:31:c0:1e:de:38:89:39:25:92:df:b9: 1d:56:fb:c5:3b:71:fa:4e:70:e7:ec:1b:c5:fc:39:bb:71:90: ab:d3:36:c1:80:c5:30:6f:4d:8b:7c:8a:ee:24:15:f5:fc:5c: 63:47:51:a0:9f:eb:30:ee:4e:95:a7:10:41:10:44:37:1e:19: 0d:37:65:f5:94:66:4a:93:5e:fb:df:f3:24:28:17:4e:7e:7f: 4f:d0:97:3a:24:b2:95:27:42:6f:42:0d:32:c7:b6:a6:a2:0f: 66:df:91:e9:af:c7:66:a9:eb:01:d4:74:ae:2c:1f:72:b8:40: 5e:15:d6:bc -----BEGIN CERTIFICATE----- MIIC9TCCAd2gAwIBAgIBADANBgkqhkiG9w0BAQsFADASMRAwDgYDVQQDEwdldGNk LWNhMB4XDTIyMDIxNTA2NTQzOVoXDTMyMDIxMzA2NTQzOVowEjEQMA4GA1UEAxMH ZXRjZC1jYTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMKMHxJzx4m8 +Y0Uwl5hL67U8EiJ98y8vqYZnahNhZlZ1pZXNZ5sYjkxEubuMvmkqs6Ri18dfZnM tfzHFHXwokNP/ebns6sRwl/bqHK4gC9mX5ghOwabuQlplBoGM5YvHMf4ssq9h9kT NsX23qpqgcLUlCueY9xWJ7QyMR1Jq2kO3NEU07vxQ4AZMXMpfnvUOy3PFH87PIRL IaQtpll5u28d3FzVRP0/vTS0MzgNzHZI4d5TAi1UeUQiZPWmOR6HJGyRPF/rfx+E OOCWGRtGnfzmeZjHLm4qLmPwKEJXFhRF896/Mo7ZSeWrpQZuDdKci2VARhdDPtJG U78iiXLQb0kCAwEAAaNWMFQwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB/wQFMAMB Af8wHQYDVR0OBBYEFON75Xgwk1XbCOTr0C4MMGzbF0BAMBIGA1UdEQQLMAmCB2V0 Y2QtY2EwDQYJKoZIhvcNAQELBQADggEBABopSDrfJFc1ILchMe3FEojVeX5CyiES kmRJXusuYM4WcV9DdgmXPWwZFRawbfzJhBTnW8LI1CR3v/2HPeTJ5DlJuvZBu6Cc l+9xsEahhtwAaxkmOTImxAxwTr8ba5NVVNeXiQde6TtjpUlObCGqdYuxqZRovxwu z4QJsFIDYnJUsOLIY4gxwB7eOIk5JZLfuR1W+8U7cfpOcOfsG8X8ObtxkKvTNsGA xTBvTYt8iu4kFfX8XGNHUaCf6zDuTpWnEEEQRDceGQ03ZfWUZkqTXvvf8yQoF05+ f0/QlzokspUnQm9CDTLHtqaiD2bfkemvx2ap6wHUdK4sH3K4QF4V1rw= -----END CERTIFICATE----- $ We can see it is a different Issuer: CN = etcd-ca, and Subject: CN = etcd-ca, X509v3 Subject Alternative Name: DNS:etcd-ca. If we look at the ETCD server certificate subject details, we can see the CN as k8s and subject alternative names of k8s , localhost and the node IP address 192.168.177.29 . $ openssl x509 -in /var/lib/minikube/certs/etcd/server.crt -text | grep -e Subject -e DNS Subject: CN = k8s Subject Public Key Info: X509v3 Subject Alternative Name: DNS:k8s, DNS:localhost, IP Address:192.168.177.29, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 $ Also, we can see that this certificate is valid for one year. openssl x509 -in /var/lib/minikube/certs/etcd/server.crt -text | grep -A 2 Validity Validity Not Before: Feb 15 06:54:39 2022 GMT Not After : Feb 15 06:54:39 2023 GMT $ Where as the minikubeCA certificate is valid for ten years. $ openssl x509 -in /var/lib/minikube/certs/ca.crt -text | grep -A 2 Validity Validity Not Before: Jun 28 04:53:56 2021 GMT Not After : Jun 27 04:53:56 2031 GMT Certificates API Let us create a new user called pradeep and generate a key pair. Once the keypair is generated, created a certificate signing request to be submitted to the Kubernetes Cluster CA. $ sudo adduser -G wheel pradeep Changing password for pradeep New password: Bad password: similar to username Retype password: passwd: password for pradeep changed by root $ su - pradeep Password: $ mkdir .certs $ cd .certs/ $ openssl genrsa -out pradeep.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) ........+++++ ......................................................................................................+++++ e is 65537 (0x010001) $ openssl req -new -key pradeep.key -out pradeep.csr -subj \"/CN=pradeep\" $ exit logout We need to convert this CSR file to base64 format and trim newline character, which needs to be passed in the CSR request. $ ls pradeep.csr pradeep.key $ cat pradeep.csr | base64 | tr -d \"\\n\" LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=$ $ Create an YAML manifest for the CertificateSigningRequest API resource. pradeep@learnk8s$ cat <<EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: pradeep spec: request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo= expirationSeconds: 86400 # one day usages: - client auth signerName: kubernetes.io/kube-apiserver-client EOF certificatesigningrequest.certificates.k8s.io/pradeep created Verify that the new request is in Pending state. pradeep@learnk8s$ kubectl get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION pradeep 4s kubernetes.io/kube-apiserver-client minikube-user 24h Pending As we know this is a valid request, we can go ahead and approve this. pradeep@learnk8s$ kubectl certificate approve pradeep certificatesigningrequest.certificates.k8s.io/pradeep approved pradeep@learnk8s$ kubectl get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION pradeep 66s kubernetes.io/kube-apiserver-client minikube-user 24h Approved,Issued pradeep@learnk8s$ kubectl describe csr Name: pradeep Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"certificates.k8s.io/v1\",\"kind\":\"CertificateSigningRequest\",\"metadata\":{\"annotations\":{},\"name\":\"pradeep\"},\"spec\":{\"expirationSeconds\":86400,\"request\":\"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=\",\"signerName\":\"kubernetes.io/kube-apiserver-client\",\"usages\":[\"client auth\"]}} CreationTimestamp: Thu, 17 Feb 2022 11:17:16 +0530 Requesting User: minikube-user Signer: kubernetes.io/kube-apiserver-client Requested Duration: 24h Status: Approved,Issued Subject: Common Name: pradeep Serial Number: Events: <none> Kube Config Kubectl command uses a configuration file that contains cluster and context information along with users . This configuration file can be viewed directly using the common commands like cat , or kubectl config view command. Usually the configuraiton is stored in a filed called config in the hidden directory .kube inside the user home directory ( /Users/pradeep/ in this case). Here we can see the same output using both methods. Using the kubectl config view command: pradeep@learnk8s$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /Users/pradeep/.minikube/ca.crt extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: cluster_info server: https://192.168.177.29:8443 name: k8s contexts: - context: cluster: k8s extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: context_info namespace: default user: k8s name: k8s current-context: k8s kind: Config preferences: {} users: - name: k8s user: client-certificate: /Users/pradeep/.minikube/profiles/k8s/client.crt client-key: /Users/pradeep/.minikube/profiles/k8s/client.key Using the standard cat command with full path Users/pradeep/.kube/config : pradeep@learnk8s$ cat /Users/pradeep/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: /Users/pradeep/.minikube/ca.crt extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: cluster_info server: https://192.168.177.29:8443 name: k8s contexts: - context: cluster: k8s extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: context_info namespace: default user: k8s name: k8s current-context: k8s kind: Config preferences: {} users: - name: k8s user: client-certificate: /Users/pradeep/.minikube/profiles/k8s/client.crt client-key: /Users/pradeep/.minikube/profiles/k8s/client.key The three important sections from this file are: Cluster: k8s Context: k8s User: k8s When we setup Minikube, all of this work is done for us automatically. We can either get or set or even delete all of these three settings with the kubectl config command as seen here. pradeep@learnk8s$ kubectl config -h Modify kubeconfig files using subcommands like \"kubectl config set current-context my-context\" The loading order follows these rules: 1. If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes place. 2. If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list. 3. Otherwise, ${HOME}/.kube/config is used and no merging takes place. Available Commands: current-context Display the current-context delete-cluster Delete the specified cluster from the kubeconfig delete-context Delete the specified context from the kubeconfig delete-user Delete the specified user from the kubeconfig get-clusters Display clusters defined in the kubeconfig get-contexts Describe one or many contexts get-users Display users defined in the kubeconfig rename-context Rename a context from the kubeconfig file set Set an individual value in a kubeconfig file set-cluster Set a cluster entry in kubeconfig set-context Set a context entry in kubeconfig set-credentials Set a user entry in kubeconfig unset Unset an individual value in a kubeconfig file use-context Set the current-context in a kubeconfig file view Display merged kubeconfig settings or a specified kubeconfig file Usage: kubectl config SUBCOMMAND [options] Use \"kubectl <command> --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). Here is the output for all three get commands. While working with multiple clusters, this becomes really useful and we have to keep switching contexts. This is how we work in production environments. For our little minikube environment also, when we created multiple profiles with the -p option (if you forget what it is, refer to the Initial setup section), minikube automatically does this context switching! Here is that last line from the minikube start command output. \ud83c\udfc4 Done! kubectl is now configured to use \"k8s\" cluster and \"default\" namespace by default pradeep@learnk8s$ kubectl config get-users NAME k8s pradeep@learnk8s$ kubectl config get-clusters NAME k8s pradeep@learnk8s$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * k8s k8s k8s default We can see that the current user is k8s , how can we change this to some other user, like pradeep user? For that we need the signed certificate for that user first. We can retrieve the signed/issued certificate using this. Pay attention to the Status section, there is the certificate. pradeep@learnk8s$ kubectl get csr/pradeep -o yaml apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"certificates.k8s.io/v1\",\"kind\":\"CertificateSigningRequest\",\"metadata\":{\"annotations\":{},\"name\":\"pradeep\"},\"spec\":{\"expirationSeconds\":86400,\"request\":\"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=\",\"signerName\":\"kubernetes.io/kube-apiserver-client\",\"usages\":[\"client auth\"]}} creationTimestamp: \"2022-02-17T05:47:16Z\" name: pradeep resourceVersion: \"63056\" uid: 585f7670-134c-4062-a649-442054314407 spec: expirationSeconds: 86400 groups: - system:masters - system:authenticated request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo= signerName: kubernetes.io/kube-apiserver-client usages: - client auth username: minikube-user status: certificate: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMrRENDQWVDZ0F3SUJBZ0lSQUlHNEdtUk5tdmczZENXZHhFcTBIUEV3RFFZSktvWklodmNOQVFFTEJRQXcKRlRFVE1CRUdBMVVFQXhNS2JXbHVhV3QxWW1WRFFUQWVGdzB5TWpBeU1UY3dOVFF6TWpCYUZ3MHlNakF5TVRndwpOVFF6TWpCYU1CSXhFREFPQmdOVkJBTVRCM0J5WVdSbFpYQXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRREZ1S3BLRFBoUWhDNlpJOWlDbXpiM01YVE9KWU5mOWRzbG45T2VNenRWcEd3d1BaQWMKeExTa0tDWmhsV0JMaXYraGppTFN6cldETFlENWF0cTUwV05Lc3JGSkROOVBTZVJnMDUxNHBkcFhoRWlYUDVKMgpSZFJUOEIxS3NwUmJjZmZSYVpMTVBXK1d0MFZYZE1zOC9rYS9WeGFXRWhvTW1mUHFnL1Y2Ly9EbkdlT1pqZ2M0CllZeWNlRnhLREIvOFlZeC9rRHJHWFliK2dwZnJlMkZZZmtvTExTNGt3KzJ1clhzYitSRkxVcU5tS2dXWnN3NlgKdDBiaXpWVjhIN3A3ajcwa3pNS0d1Z2dzY1V3Vi9wR0x0ajVIRUtqMHN0N0gwODNKWkpuakpyaVZBNzMyQzlhdQpxZ1pNT0pJUkt5MWxGQ3FYaTZ4cU52NklERXFxekZLcmR5UEhBZ01CQUFHalJqQkVNQk1HQTFVZEpRUU1NQW9HCkNDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUNNQUF3SHdZRFZSMGpCQmd3Rm9BVWkvb3hwNHQyM3NiMVBjQy8KSlFVZEJYaTVna0F3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUp3R1dVR1IzRVd3bnc2cVdoUVJRTHY1aUFuOQpESmVhb3o1M25VVmhuV0tzRWdETXhzcTgvcHRVK0NJalgvR3NXS2hSaE1nU3Q0ZHR1cEswMHROVnB3cGRQTExQCm5kWHRzZmhmQmRPV1hNWDVBSmNXNTQ2Sk02eVhkQ1VXbnBmSE5LTTJ5bnRqb0FPSXp4azNhUGcxeDNuckdCZVkKQzZ6UmhRdWRYTitwdFFNK0lRa0pYVzF4TWhSeVRoNHU2bXI3c2RTTGdQZ0dsaXZGNGE2Ukx3YWttOWdoNWJtOApIUkxYZWpuTVo0TFVUOExYWmgyR0tRenRrR1QwMkgxanNDMTZpYnJpbmNFSFpxUXVvd0Z3TUMxSUdhS1l0d1BkCnMwVFQxcFZBR3N1VzRlYUpPT3B2MmtpcVVITkExLzBrb1IwYnpscFhObFZIZzVySlY1dFBtWXp4YjRjPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== conditions: - lastTransitionTime: \"2022-02-17T05:48:20Z\" lastUpdateTime: \"2022-02-17T05:48:20Z\" message: This CSR was approved by kubectl certificate approve. reason: KubectlApprove status: \"True\" type: Approved The certificate value is in Base64-encoded format under status.certificate. Let us export the issued certificate from the CertificateSigningRequest. pradeep@learnk8s$ echo \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMrRENDQWVDZ0F3SUJBZ0lSQUlHNEdtUk5tdmczZENXZHhFcTBIUEV3RFFZSktvWklodmNOQVFFTEJRQXcKRlRFVE1CRUdBMVVFQXhNS2JXbHVhV3QxWW1WRFFUQWVGdzB5TWpBeU1UY3dOVFF6TWpCYUZ3MHlNakF5TVRndwpOVFF6TWpCYU1CSXhFREFPQmdOVkJBTVRCM0J5WVdSbFpYQXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRREZ1S3BLRFBoUWhDNlpJOWlDbXpiM01YVE9KWU5mOWRzbG45T2VNenRWcEd3d1BaQWMKeExTa0tDWmhsV0JMaXYraGppTFN6cldETFlENWF0cTUwV05Lc3JGSkROOVBTZVJnMDUxNHBkcFhoRWlYUDVKMgpSZFJUOEIxS3NwUmJjZmZSYVpMTVBXK1d0MFZYZE1zOC9rYS9WeGFXRWhvTW1mUHFnL1Y2Ly9EbkdlT1pqZ2M0CllZeWNlRnhLREIvOFlZeC9rRHJHWFliK2dwZnJlMkZZZmtvTExTNGt3KzJ1clhzYitSRkxVcU5tS2dXWnN3NlgKdDBiaXpWVjhIN3A3ajcwa3pNS0d1Z2dzY1V3Vi9wR0x0ajVIRUtqMHN0N0gwODNKWkpuakpyaVZBNzMyQzlhdQpxZ1pNT0pJUkt5MWxGQ3FYaTZ4cU52NklERXFxekZLcmR5UEhBZ01CQUFHalJqQkVNQk1HQTFVZEpRUU1NQW9HCkNDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUNNQUF3SHdZRFZSMGpCQmd3Rm9BVWkvb3hwNHQyM3NiMVBjQy8KSlFVZEJYaTVna0F3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUp3R1dVR1IzRVd3bnc2cVdoUVJRTHY1aUFuOQpESmVhb3o1M25VVmhuV0tzRWdETXhzcTgvcHRVK0NJalgvR3NXS2hSaE1nU3Q0ZHR1cEswMHROVnB3cGRQTExQCm5kWHRzZmhmQmRPV1hNWDVBSmNXNTQ2Sk02eVhkQ1VXbnBmSE5LTTJ5bnRqb0FPSXp4azNhUGcxeDNuckdCZVkKQzZ6UmhRdWRYTitwdFFNK0lRa0pYVzF4TWhSeVRoNHU2bXI3c2RTTGdQZ0dsaXZGNGE2Ukx3YWttOWdoNWJtOApIUkxYZWpuTVo0TFVUOExYWmgyR0tRenRrR1QwMkgxanNDMTZpYnJpbmNFSFpxUXVvd0Z3TUMxSUdhS1l0d1BkCnMwVFQxcFZBR3N1VzRlYUpPT3B2MmtpcVVITkExLzBrb1IwYnpscFhObFZIZzVySlY1dFBtWXp4YjRjPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\" | base64 -d > pradeep.crt Just to confirm, let us view the certificate. pradeep@learnk8s$ cat pradeep.crt -----BEGIN CERTIFICATE----- MIIC+DCCAeCgAwIBAgIRAIG4GmRNmvg3dCWdxEq0HPEwDQYJKoZIhvcNAQELBQAw FTETMBEGA1UEAxMKbWluaWt1YmVDQTAeFw0yMjAyMTcwNTQzMjBaFw0yMjAyMTgw NTQzMjBaMBIxEDAOBgNVBAMTB3ByYWRlZXAwggEiMA0GCSqGSIb3DQEBAQUAA4IB DwAwggEKAoIBAQDFuKpKDPhQhC6ZI9iCmzb3MXTOJYNf9dsln9OeMztVpGwwPZAc xLSkKCZhlWBLiv+hjiLSzrWDLYD5atq50WNKsrFJDN9PSeRg0514pdpXhEiXP5J2 RdRT8B1KspRbcffRaZLMPW+Wt0VXdMs8/ka/VxaWEhoMmfPqg/V6//DnGeOZjgc4 YYyceFxKDB/8YYx/kDrGXYb+gpfre2FYfkoLLS4kw+2urXsb+RFLUqNmKgWZsw6X t0bizVV8H7p7j70kzMKGuggscUwV/pGLtj5HEKj0st7H083JZJnjJriVA732C9au qgZMOJIRKy1lFCqXi6xqNv6IDEqqzFKrdyPHAgMBAAGjRjBEMBMGA1UdJQQMMAoG CCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUi/oxp4t23sb1PcC/ JQUdBXi5gkAwDQYJKoZIhvcNAQELBQADggEBAJwGWUGR3EWwnw6qWhQRQLv5iAn9 DJeaoz53nUVhnWKsEgDMxsq8/ptU+CIjX/GsWKhRhMgSt4dtupK00tNVpwpdPLLP ndXtsfhfBdOWXMX5AJcW546JM6yXdCUWnpfHNKM2yntjoAOIzxk3aPg1x3nrGBeY C6zRhQudXN+ptQM+IQkJXW1xMhRyTh4u6mr7sdSLgPgGlivF4a6RLwakm9gh5bm8 HRLXejnMZ4LUT8LXZh2GKQztkGT02H1jsC16ibrincEHZqQuowFwMC1IGaKYtwPd s0TT1pVAGsuW4eaJOOpv2kiqUHNA1/0koR0bzlpXNlVHg5rJV5tPmYzxb4c= -----END CERTIFICATE----- Roles and RoleBindings With the certificate created it is time to define the Role and RoleBinding for this user pradeep to access Kubernetes cluster resources. First let us create a role called developer who can create, get, list and update all pods. pradeep@learnk8s$ kubectl create role developer --verb=create --verb=get --verb=list --verb=update --verb=delete --resource=pods role.rbac.authorization.k8s.io/developer created Next, we have to bind this role to the user, by creating a role binding and giving it a name, in this case developer-pradeep . pradeep@learnk8s$ kubectl create rolebinding developer-pradeep --role=developer --user=pradeep rolebinding.rbac.authorization.k8s.io/developer-pradeep created To verify the role: pradeep@learnk8s$ kubectl get roles NAME CREATED AT developer 2022-02-17T06:25:08Z Rolebinding: pradeep@learnk8s$ kubectl get rolebindings NAME ROLE AGE developer-pradeep Role/developer 26s The last step is to add this user pradeep into the kubeconfig file. For this, first, you need to add new credentials: pradeep@learnk8s$ kubectl config set-credentials pradeep --client-key=pradeep.key --client-certificate=pradeep.crt --embed-certs=true User \"pradeep\" set. Then, you need to add the context: pradeep@learnk8s$ kubectl config set-context pradeep --cluster=k8s --user=pradeep Context \"pradeep\" created. Let us check the available contexts first. pradeep@learnk8s$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * k8s k8s k8s default pradeep k8s pradeep We can see there are two, k8s the old (default) one and the pradeep context which is new. Looking at the first column CURRENT , the * mark is present for the k8s context, indicating that it is the active context. With this change, let us take another look at the kubectl config file. pradeep@learnk8s$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /Users/pradeep/.minikube/ca.crt extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: cluster_info server: https://192.168.177.29:8443 name: k8s contexts: - context: cluster: k8s extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: context_info namespace: default user: k8s name: k8s - context: cluster: k8s user: pradeep name: pradeep current-context: pradeep kind: Config preferences: {} users: - name: k8s user: client-certificate: /Users/pradeep/.minikube/profiles/k8s/client.crt client-key: /Users/pradeep/.minikube/profiles/k8s/client.key - name: pradeep user: client-certificate-data: REDACTED client-key-data: REDACTED We can see that , there is still one cluster, but two contexts and two users. It is time to test all the work done in the last half an hour by switching the context. pradeep@learnk8s$ kubectl config use-context pradeep Switched to context \"pradeep\". Verify it one more way. pradeep@learnk8s$ kubectl config current-context pradeep Now that we are sure of the current-context, let us get the pods as this new user pradeep . pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE kodekloud-8477b7849-blzrs 1/1 Running 0 46h kodekloud-8477b7849-jkfmr 1/1 Running 0 46h kodekloud-8477b7849-m65m8 1/1 Running 0 46h kodekloud-8477b7849-rgbmn 1/1 Running 0 46h kodekloud-change-color 1/1 Running 0 43h kodekloud-cm 1/1 Running 0 39h kodekloud-env-color 1/1 Running 0 40h kodekloud-env-color-2 1/1 Running 0 40h kodekloud-secret 1/1 Running 0 29h multi-containers 1/2 NotReady 0 28h pod-with-init-container 1/1 Running 14 (12m ago) 28h Yay! :clap: We could get the pods as user pradeep . Let us try to get the nodes this time. pradeep@learnk8s$ kubectl get nodes Error from server (Forbidden): nodes is forbidden: User \"pradeep\" cannot list resource \"nodes\" in API group \"\" at the cluster scope Try accessing deployments. pradeep@learnk8s$ kubectl get deployments Error from server (Forbidden): deployments.apps is forbidden: User \"pradeep\" cannot list resource \"deployments\" in API group \"apps\" in the namespace \"default\" What happened? It is Forbidden. But, this is expected, right?! We only gave permission for Pods. Isn't it? We can't verify that (if we don't remember what we have given for this user/role) from the current context. pradeep@learnk8s$ kubectl describe roles developer Error from server (Forbidden): roles.rbac.authorization.k8s.io \"developer\" is forbidden: User \"pradeep\" cannot get resource \"roles\" in API group \"rbac.authorization.k8s.io\" in the namespace \"default\" So, we have to switch the context back to k8s . pradeep@learnk8s$ kubectl config use-context k8s Switched to context \"k8s\". Describe the developer role and check what all Resources this role has access to. pradeep@learnk8s$ kubectl describe roles developer Name: developer Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [create get list update delete] This confirms that, any user who is given this role of developer can only work with pods . Here is the same in YAML form. pradeep@learnk8s$ kubectl get roles developer -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: \"2022-02-17T06:25:08Z\" name: developer namespace: default resourceVersion: \"64839\" uid: 9eae4036-0858-4c09-8c06-7532d74ca1aa rules: - apiGroups: - \"\" resources: - pods verbs: - create - get - list - update - delete How can we give access to deployments for this developer? We just need to add additional resource of deployments. Let us edit the developer role. pradeep@learnk8s$ kubectl edit role developer role.rbac.authorization.k8s.io/developer edited Describe the role after editing it, just to confirm we have additional access to deployments. We have modified few Verbs as well (removed some access!) pradeep@learnk8s$ kubectl describe role developer Name: developer Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- deployments [] [] [get list] pods [] [] [get list] This is modified YAML version of the developer role. pradeep@learnk8s$ kubectl get role developer -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: \"2022-02-17T06:25:08Z\" name: developer namespace: default resourceVersion: \"66960\" uid: 9eae4036-0858-4c09-8c06-7532d74ca1aa rules: - apiGroups: - \"\" resources: - deployments - pods verbs: - get - list Now switch back to the pradeep context and verify if you can get the nodes. pradeep@learnk8s$ kubectl config use-context pradeep Switched to context \"pradeep\". Let us see if pradeep can get deployments. pradeep@learnk8s$ kubectl get deployments Error from server (Forbidden): deployments.apps is forbidden: User \"pradeep\" cannot list resource \"deployments\" in API group \"apps\" in the namespace \"default\" Still Forbidden. It is do with the apiGroups settings. Currently we did not specify anything, it was left blank. We can edit again, change the apiGroups to any with the * . Though in production environment, we might want to be more specific and only configured limited groups as needed. pradeep@learnk8s$ kubectl config use-context k8s Switched to context \"k8s\". pradeep@learnk8s$ kubectl edit role developer role.rbac.authorization.k8s.io/developer edited Look at the apiGroups value of * . pradeep@learnk8s$ kubectl get role developer -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: \"2022-02-17T06:25:08Z\" name: developer namespace: default resourceVersion: \"67224\" uid: 9eae4036-0858-4c09-8c06-7532d74ca1aa rules: - apiGroups: - '*' resources: - deployments - pods verbs: - get - list To test if this is working or not for pradeep user, there is another away, without switching context. We can make use of the --as option (impersonation). Before modifying the apiGroup setting, it was like this, for the same test. pradeep@learnk8s$ kubectl get deployment --as pradeep Error from server (Forbidden): deployments.apps is forbidden: User \"pradeep\" cannot list resource \"deployments\" in API group \"apps\" in the namespace \"default\" Now that we modified, pradeep@learnk8s$ kubectl get deployment --as pradeep NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 2d Though this test confirms that pradeep can get the deployments, let us switch context and verify one more time. pradeep@learnk8s$ kubectl config use-context pradeep Switched to context \"pradeep\". pradeep@learnk8s$ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 2d :tada: Congratulations! Cluster Roles and Cluster Role Bindings If we notice, user pradeep still can't get the nodes information. It is becuase, he still does not have access to resources at the cluster scope. pradeep@learnk8s$ kubectl get nodes Error from server (Forbidden): nodes is forbidden: User \"pradeep\" cannot list resource \"nodes\" in API group \"\" at the cluster scope To get this access, we need to create a Cluste role and Cluste role binding. pradeep@learnk8s$ kubectl config use-context k8s Switched to context \"k8s\". pradeep@learnk8s$ kubectl create clusterrole pradeep-cluster --verb=get,list,watch, --resource=nodes Warning: '' is not a standard resource verb clusterrole.rbac.authorization.k8s.io/pradeep-cluster created Get all available clusterroles. There are many but we can see our newly created clusterrole pradeep-cluster . pradeep@learnk8s$ kubectl get clusterrole NAME CREATED AT admin 2022-02-15T06:57:58Z cluster-admin 2022-02-15T06:57:58Z edit 2022-02-15T06:57:58Z kindnet 2022-02-15T06:58:03Z kubeadm:get-nodes 2022-02-15T06:58:01Z pradeep-cluster 2022-02-17T07:33:43Z system:aggregate-to-admin 2022-02-15T06:57:58Z system:aggregate-to-edit 2022-02-15T06:57:58Z system:aggregate-to-view 2022-02-15T06:57:58Z system:auth-delegator 2022-02-15T06:57:58Z system:basic-user 2022-02-15T06:57:58Z system:certificates.k8s.io:certificatesigningrequests:nodeclient 2022-02-15T06:57:58Z system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 2022-02-15T06:57:58Z system:certificates.k8s.io:kube-apiserver-client-approver 2022-02-15T06:57:58Z system:certificates.k8s.io:kube-apiserver-client-kubelet-approver 2022-02-15T06:57:58Z system:certificates.k8s.io:kubelet-serving-approver 2022-02-15T06:57:58Z system:certificates.k8s.io:legacy-unknown-approver 2022-02-15T06:57:58Z system:controller:attachdetach-controller 2022-02-15T06:57:58Z system:controller:certificate-controller 2022-02-15T06:57:58Z system:controller:clusterrole-aggregation-controller 2022-02-15T06:57:58Z system:controller:cronjob-controller 2022-02-15T06:57:58Z system:controller:daemon-set-controller 2022-02-15T06:57:58Z system:controller:deployment-controller 2022-02-15T06:57:58Z system:controller:disruption-controller 2022-02-15T06:57:58Z system:controller:endpoint-controller 2022-02-15T06:57:58Z system:controller:endpointslice-controller 2022-02-15T06:57:58Z system:controller:endpointslicemirroring-controller 2022-02-15T06:57:58Z system:controller:ephemeral-volume-controller 2022-02-15T06:57:58Z system:controller:expand-controller 2022-02-15T06:57:58Z system:controller:generic-garbage-collector 2022-02-15T06:57:58Z system:controller:horizontal-pod-autoscaler 2022-02-15T06:57:58Z system:controller:job-controller 2022-02-15T06:57:58Z system:controller:namespace-controller 2022-02-15T06:57:58Z system:controller:node-controller 2022-02-15T06:57:58Z system:controller:persistent-volume-binder 2022-02-15T06:57:58Z system:controller:pod-garbage-collector 2022-02-15T06:57:58Z system:controller:pv-protection-controller 2022-02-15T06:57:58Z system:controller:pvc-protection-controller 2022-02-15T06:57:58Z system:controller:replicaset-controller 2022-02-15T06:57:58Z system:controller:replication-controller 2022-02-15T06:57:58Z system:controller:resourcequota-controller 2022-02-15T06:57:58Z system:controller:root-ca-cert-publisher 2022-02-15T06:57:58Z system:controller:route-controller 2022-02-15T06:57:58Z system:controller:service-account-controller 2022-02-15T06:57:58Z system:controller:service-controller 2022-02-15T06:57:58Z system:controller:statefulset-controller 2022-02-15T06:57:58Z system:controller:ttl-after-finished-controller 2022-02-15T06:57:58Z system:controller:ttl-controller 2022-02-15T06:57:58Z system:coredns 2022-02-15T06:58:01Z system:discovery 2022-02-15T06:57:58Z system:heapster 2022-02-15T06:57:58Z system:kube-aggregator 2022-02-15T06:57:58Z system:kube-controller-manager 2022-02-15T06:57:58Z system:kube-dns 2022-02-15T06:57:58Z system:kube-scheduler 2022-02-15T06:57:58Z system:kubelet-api-admin 2022-02-15T06:57:58Z system:monitoring 2022-02-15T06:57:58Z system:node 2022-02-15T06:57:58Z system:node-bootstrapper 2022-02-15T06:57:58Z system:node-problem-detector 2022-02-15T06:57:58Z system:node-proxier 2022-02-15T06:57:58Z system:persistent-volume-provisioner 2022-02-15T06:57:58Z system:public-info-viewer 2022-02-15T06:57:58Z system:service-account-issuer-discovery 2022-02-15T06:57:58Z system:volume-scheduler 2022-02-15T06:57:58Z view 2022-02-15T06:57:58Z Create a clusterrolebinding called pradeep-cluster-binding and bind the user pradeep and clusterrole pradeep-cluster . pradeep@learnk8s$ kubectl create clusterrolebinding pradeep-cluster-binding --clusterrole=pradeep-cluster --user=pradeep clusterrolebinding.rbac.authorization.k8s.io/pradeep-cluster-binding created Get all available clusterrolebindings. pradeep@learnk8s$ kubectl get clusterrolebindings.rbac.authorization.k8s.io NAME ROLE AGE cluster-admin ClusterRole/cluster-admin 2d kindnet ClusterRole/kindnet 2d kubeadm:get-nodes ClusterRole/kubeadm:get-nodes 2d kubeadm:kubelet-bootstrap ClusterRole/system:node-bootstrapper 2d kubeadm:node-autoapprove-bootstrap ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient 2d kubeadm:node-autoapprove-certificate-rotation ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 2d kubeadm:node-proxier ClusterRole/system:node-proxier 2d minikube-rbac ClusterRole/cluster-admin 2d pradeep-cluster-binding ClusterRole/pradeep-cluster 5s storage-provisioner ClusterRole/system:persistent-volume-provisioner 2d system:basic-user ClusterRole/system:basic-user 2d system:controller:attachdetach-controller ClusterRole/system:controller:attachdetach-controller 2d system:controller:certificate-controller ClusterRole/system:controller:certificate-controller 2d system:controller:clusterrole-aggregation-controller ClusterRole/system:controller:clusterrole-aggregation-controller 2d system:controller:cronjob-controller ClusterRole/system:controller:cronjob-controller 2d system:controller:daemon-set-controller ClusterRole/system:controller:daemon-set-controller 2d system:controller:deployment-controller ClusterRole/system:controller:deployment-controller 2d system:controller:disruption-controller ClusterRole/system:controller:disruption-controller 2d system:controller:endpoint-controller ClusterRole/system:controller:endpoint-controller 2d system:controller:endpointslice-controller ClusterRole/system:controller:endpointslice-controller 2d system:controller:endpointslicemirroring-controller ClusterRole/system:controller:endpointslicemirroring-controller 2d system:controller:ephemeral-volume-controller ClusterRole/system:controller:ephemeral-volume-controller 2d system:controller:expand-controller ClusterRole/system:controller:expand-controller 2d system:controller:generic-garbage-collector ClusterRole/system:controller:generic-garbage-collector 2d system:controller:horizontal-pod-autoscaler ClusterRole/system:controller:horizontal-pod-autoscaler 2d system:controller:job-controller ClusterRole/system:controller:job-controller 2d system:controller:namespace-controller ClusterRole/system:controller:namespace-controller 2d system:controller:node-controller ClusterRole/system:controller:node-controller 2d system:controller:persistent-volume-binder ClusterRole/system:controller:persistent-volume-binder 2d system:controller:pod-garbage-collector ClusterRole/system:controller:pod-garbage-collector 2d system:controller:pv-protection-controller ClusterRole/system:controller:pv-protection-controller 2d system:controller:pvc-protection-controller ClusterRole/system:controller:pvc-protection-controller 2d system:controller:replicaset-controller ClusterRole/system:controller:replicaset-controller 2d system:controller:replication-controller ClusterRole/system:controller:replication-controller 2d system:controller:resourcequota-controller ClusterRole/system:controller:resourcequota-controller 2d system:controller:root-ca-cert-publisher ClusterRole/system:controller:root-ca-cert-publisher 2d system:controller:route-controller ClusterRole/system:controller:route-controller 2d system:controller:service-account-controller ClusterRole/system:controller:service-account-controller 2d system:controller:service-controller ClusterRole/system:controller:service-controller 2d system:controller:statefulset-controller ClusterRole/system:controller:statefulset-controller 2d system:controller:ttl-after-finished-controller ClusterRole/system:controller:ttl-after-finished-controller 2d system:controller:ttl-controller ClusterRole/system:controller:ttl-controller 2d system:coredns ClusterRole/system:coredns 2d system:discovery ClusterRole/system:discovery 2d system:kube-controller-manager ClusterRole/system:kube-controller-manager 2d system:kube-dns ClusterRole/system:kube-dns 2d system:kube-scheduler ClusterRole/system:kube-scheduler 2d system:monitoring ClusterRole/system:monitoring 2d system:node ClusterRole/system:node 2d system:node-proxier ClusterRole/system:node-proxier 2d system:public-info-viewer ClusterRole/system:public-info-viewer 2d system:service-account-issuer-discovery ClusterRole/system:service-account-issuer-discovery 2d system:volume-scheduler ClusterRole/system:volume-scheduler 2d Again, there are many pre-defined but our newly created clusterrolebinding pradeep-cluster-binding is shown. With that confirmation, let us verify if user pradeep can get the nodes without context-switching. pradeep@learnk8s$ kubectl get nodes --as pradeep NAME STATUS ROLES AGE VERSION k8s Ready control-plane,master 2d v1.23.1 k8s-m02 Ready <none> 2d v1.23.1 Before switching context, let us describe these clusterrole and clusterrolebindings. ClusterRole: pradeep@learnk8s$ kubectl describe clusterrole pradeep-cluster Name: pradeep-cluster Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- nodes [] [] [get list watch ] ClusterRoleBinding: pradeep@learnk8s$ kubectl describe clusterrolebindings pradeep-cluster-binding Name: pradeep-cluster-binding Labels: <none> Annotations: <none> Role: Kind: ClusterRole Name: pradeep-cluster Subjects: Kind Name Namespace ---- ---- --------- User pradeep Switch context now. pradeep@learnk8s$ kubectl config use-context pradeep Switched to context \"pradeep\". Finally, verify user pradeep can get the nodes. pradeep@learnk8s$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s Ready control-plane,master 2d v1.23.1 k8s-m02 Ready <none> 2d v1.23.1 Now, we have accomplished our simple goal w.r.t authorization in Kubernetes. Kubectl Auth To Inspect authorization, we can make use of kubectl auth command. It will show an :+1: or :no_entry_sign:. Here is the related help and examples. pradeep@learnk8s$ kubectl auth can-i -h Check whether an action is allowed. VERB is a logical Kubernetes API verb like 'get', 'list', 'watch', 'delete', etc. TYPE is a Kubernetes resource. Shortcuts and groups will be resolved. NONRESOURCEURL is a partial URL that starts with \"/\". NAME is the name of a particular Kubernetes resource. This command pairs nicely with impersonation. See --as global flag. Examples: # Check to see if I can create pods in any namespace kubectl auth can-i create pods --all-namespaces # Check to see if I can list deployments in my current namespace kubectl auth can-i list deployments.apps # Check to see if I can do everything in my current namespace (\"*\" means all) kubectl auth can-i '*' '*' # Check to see if I can get the job named \"bar\" in namespace \"foo\" kubectl auth can-i list jobs.batch/bar -n foo # Check to see if I can read pod logs kubectl auth can-i get pods --subresource=log # Check to see if I can access the URL /logs/ kubectl auth can-i get /logs/ # List all allowed actions in namespace \"foo\" kubectl auth can-i --list --namespace=foo Options: -A, --all-namespaces=false: If true, check the specified action in all namespaces. --list=false: If true, prints all allowed actions. --no-headers=false: If true, prints allowed actions without headers -q, --quiet=false: If true, suppress output and just return the exit code. --subresource='': SubResource such as pod/log or deployment/scale Usage: kubectl auth can-i VERB [TYPE | TYPE/NAME | NONRESOURCEURL] [options] Use \"kubectl options\" for a list of global command-line options (applies to all commands). For example, from the pradeep context, pradeep@learnk8s$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE k8s k8s k8s default * pradeep k8s pradeep pradeep@learnk8s$ kubectl auth can-i '*' '*' no From the k8s context, pradeep@learnk8s$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * k8s k8s k8s default pradeep k8s pradeep pradeep@learnk8s$ kubectl auth can-i '*' '*' yes Security Context To understand security context, first let us try to create a Pod without any security context and check few things like, whoami , id , and ps command outputs to verify the current user inside the Pod. pradeep@learnk8s$ cat no-security-context-demo.yaml apiVersion: v1 kind: Pod metadata: name: no-security-context-demo spec: containers: - name: no-security-context-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] pradeep@learnk8s$ kubectl create -f no-security-context-demo.yaml pod/no-security-context-demo created Login to the Pod, and launch the shell . pradeep@learnk8s$ kubectl exec -it no-security-context-demo -- sh / # whoami root / # id uid=0(root) gid=0(root) groups=10(wheel) / # ps PID USER TIME COMMAND 1 root 0:00 sleep 1h 9 root 0:00 sh 18 root 0:00 ps / # exit 0 From all three command outputs, it is clear that it is the root user with id 0 . Now, we can make use of a security context to define privilege and access control settings for a Pod or Container. For example, based on user ID (UID) and group ID (GID). Let us modify the Pod defintion to include securityContext and create another Pod. pradeep@learnk8s$ cat security-context-demo.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: runAsUser: 1000 containers: - name: security-context-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] pradeep@learnk8s$ kubectl create -f security-context-demo.yaml pod/security-context-demo created pradeep@learnk8s$ kubectl get pods | grep security no-security-context-demo 1/1 Running 0 9m24s security-context-demo 1/1 Running 0 31s pradeep@learnk8s$ kubectl exec -it security-context-demo -- sh / $ whoami whoami: unknown uid 1000 / $ id uid=1000 gid=0(root) / $ ps PID USER TIME COMMAND 1 1000 0:00 sleep 1h 7 1000 0:00 sh 16 1000 0:00 ps / $ cat /etc/passwd root:x:0:0:root:/root:/bin/sh daemon:x:1:1:daemon:/usr/sbin:/bin/false bin:x:2:2:bin:/bin:/bin/false sys:x:3:3:sys:/dev:/bin/false sync:x:4:100:sync:/bin:/bin/sync mail:x:8:8:mail:/var/spool/mail:/bin/false www-data:x:33:33:www-data:/var/www:/bin/false operator:x:37:37:Operator:/var:/bin/false nobody:x:65534:65534:nobody:/home:/bin/false / $ We can see the difference. Now, inside the Pod, its all the user with ID 1000 as defined in our security context. As we are still inside the security-context-demo pod, let us check few things like date and try to modify the date value. / $ date Sat Feb 19 01:06:08 UTC 2022 / $ date -s 202206020937 date: can't set date: Operation not permitted Thu Jun 2 09:37:00 UTC 2022 / $ date Sat Feb 19 01:08:11 UTC 2022 / $ It says that date: can't set date: Operation not permitted and date did not change. Let us create a new Pod definition to run the container as root user with SYS_TIME capability. pradeep@learnk8s$ cat security-context-cap-demo.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo-cap spec: containers: - name: security-context-demo-cap image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] securityContext: runAsUser: 0 capabilities: add: [\"SYS_TIME\"] pradeep@learnk8s$ kubectl create -f security-context-cap-demo.yaml pod/security-context-demo-cap created pradeep@learnk8s$ kubectl exec -it security-context-demo-cap -- sh / # id uid=0(root) gid=0(root) groups=10(wheel) / # whoami root / # ps PID USER TIME COMMAND 1 root 0:00 sleep 1h 9 root 0:00 sh 18 root 0:00 ps / # date -s 202206020937 Thu Jun 2 09:37:00 UTC 2022 / # date Sat Feb 19 13:29:05 UTC 2022 This time, it is the root user and setting date worked fine (partially!). Important difference to observe is that there is no Operation not permitted warning. It seems to took the new date, but issuing a date again, still shows the current date. I haven't explored this further yet. Storage Volumes emptyDir There are many volume types. An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently. Here is an example for creating a volume of type emptyDir and use it in a Pod. pradeep@learnk8s$ more pod-with-emptydir-volume.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-emptydir-volume spec: volumes: - name: logs-volume emptyDir: {} containers: - image: nginx name: pod-with-emptydir-volume volumeMounts: - mountPath: /var/logs name: logs-volume pradeep@learnk8s$ kubectl create -f pod-with-emptydir-volume.yaml pod/pod-with-emptydir-volume created pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 9s pradeep@learnk8s$ kubectl exec -it pod-with-emptydir-volume -- /bin/sh # ls bin boot dev docker-entrypoint.d docker-entrypoint.sh etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var # cd /var/logs # ls # pwd /var/logs # touch testing-volumes.txt # ls testing-volumes.txt # exit 0 HostPath pradeep@learnk8s$ cat pod-with-hostpath-volume.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-hostpath-volume spec: containers: - image: nginx name: pod-with-hostpath-volume volumeMounts: - mountPath: /usr/share/nginx/html name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data-for-pod-with-hostpath-volume # this field is optional type: Directory pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ls $ sudo mkdir /data-for-pod-with-hostpath-volume $ sudo vi /data-for-pod-with-hostpath-volume/index.html $ sudo cat /data-for-pod-with-hostpath-volume/index.html testing Pod with hostPath volume mount! $ exit logout pradeep@learnk8s$ kubectl create -f pod-with-hostpath-volume.yaml pod/pod-with-hostpath-volume created pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 13m pod-with-hostpath-volume 0/1 ContainerCreating 0 53s Even after 50+ seconds, the pod-with-hostpath-volume seems to be not yet Running. Let us describe it to understand the reason. Pay attention to Mounts section under Containers and Volumes section. Finally look at the events. pradeep@learnk8s$ kubectl describe pods pod-with-hostpath-volume Name: pod-with-hostpath-volume Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Sun, 20 Feb 2022 18:43:42 +0530 Labels: <none> Annotations: <none> Status: Pending IP: IPs: <none> Containers: pod-with-hostpath-volume: Container ID: Image: nginx Image ID: Port: <none> Host Port: <none> State: Waiting Reason: ContainerCreating Ready: False Restart Count: 0 Environment: <none> Mounts: /usr/share/nginx/html from test-volume (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f7lsg (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: test-volume: Type: HostPath (bare host directory volume) Path: /data-for-pod-with-hostpath-volume HostPathType: Directory kube-api-access-f7lsg: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 61s default-scheduler Successfully assigned default/pod-with-hostpath-volume to k8s-m02 Warning FailedMount 28s (x7 over 60s) kubelet MountVolume.SetUp failed for volume \"test-volume\" : hostPath type check failed: /data-for-pod-with-hostpath-volume is not a directory From the events, we can see that the default-scheduler tried to create this pod on the node k8s-m02 but MountVolume.SetUp failed for volume test-volume : hostPath type check failed: /data-for-pod-with-hostpath-volume is not a directory. This is expected, right. A moment ago, we created a directory with this name on the k8s node. To fix this, we can manually schedule this pod on the k8s node with the nodeName spec. We did something like this already. Didn't we? First, let us delete the pod which is still in creatingContainer stage. pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 21m pod-with-hostpath-volume 0/1 ContainerCreating 0 9m32s pradeep@learnk8s$ kubectl delete pods pod-with-hostpath-volume pod \"pod-with-hostpath-volume\" deleted Modify the YAML definition file like this. Only thing different (from the previous definition) here is the nodeName: k8s line. pradeep@learnk8s$ cat pod-with-hostpath-volume.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-hostpath-volume spec: nodeName: k8s containers: - image: nginx name: pod-with-hostpath-volume volumeMounts: - mountPath: /usr/share/nginx/html name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data-for-pod-with-hostpath-volume # this field is optional type: Directory pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 26m pod-with-hostpath-volume 1/1 Running 0 42s This time it is running fine. If we describe this Pod, pradeep@learnk8s$ kubectl describe pods pod-with-hostpath-volume Name: pod-with-hostpath-volume Namespace: default Priority: 0 Node: k8s/192.168.177.29 Start Time: Sun, 20 Feb 2022 18:56:51 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.0.18 IPs: IP: 10.244.0.18 Containers: pod-with-hostpath-volume: Container ID: docker://f1e79dd787c5327766bbdde7b35cab8314de09e1c406fe274ee3f7c364cc5845 Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: <none> Host Port: <none> State: Running Started: Sun, 20 Feb 2022 18:57:25 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /usr/share/nginx/html from test-volume (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7fv4m (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: test-volume: Type: HostPath (bare host directory volume) Path: /data-for-pod-with-hostpath-volume HostPathType: Directory kube-api-access-7fv4m: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Pulling 50s kubelet Pulling image \"nginx\" Normal Pulled 19s kubelet Successfully pulled image \"nginx\" in 30.708250752s Normal Created 19s kubelet Created container pod-with-hostpath-volume Normal Started 18s kubelet Started container pod-with-hostpath-volume Get the IP assigned to the Pod, either from the previous command or with the -o wide option. pradeep@learnk8s$ kubectl get pods -o wide | grep volume pod-with-emptydir-volume 1/1 Running 0 38m 10.244.1.39 k8s-m02 <none> <none> pod-with-hostpath-volume 1/1 Running 0 13m 10.244.0.18 k8s <none> <none> Let us connect to this Pod on the IP address assigned 10.244.0.18 . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.0.18 testing Pod with hostPath volume mount! The test is working. The nginx container in the pod-with-hostpath-volume is readigng the data from the host directory. Config Maps A ConfigMap provides a way to inject configuration data into pods. The data stored in a ConfigMap can be referenced in a volume of type configMap and then consumed by containerized applications running in a pod. https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/ Let us try this example as it is in our Minikube setup. pradeep@learnk8s$ cat redis-pod-configmap.yaml apiVersion: v1 kind: Pod metadata: name: redis spec: containers: - name: redis image: redis:5.0.4 command: - redis-server - \"/redis-master/redis.conf\" env: - name: MASTER value: \"true\" ports: - containerPort: 6379 resources: limits: cpu: \"0.1\" volumeMounts: - mountPath: /redis-master-data name: data - mountPath: /redis-master name: config volumes: - name: data emptyDir: {} - name: config configMap: name: example-redis-config items: - key: redis-config path: redis.conf pradeep@learnk8s$ cat <<EOF >./example-redis-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: example-redis-config data: redis-config: \"\" EOF pradeep@learnk8s$ kubectl apply -f example-redis-config.yaml configmap/example-redis-config created pradeep@learnk8s$ kubectl apply -f redis-pod-configmap.yaml pod/redis created pradeep@learnk8s$ kubectl get pod/redis configmap/example-redis-config -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/redis 1/1 Running 0 53s 10.244.1.40 k8s-m02 <none> <none> NAME DATA AGE configmap/example-redis-config 1 64s Describe the configmap. pradeep@learnk8s$ kubectl describe configmap/example-redis-config Name: example-redis-config Namespace: default Labels: <none> Annotations: <none> Data ==== redis-config: ---- BinaryData ==== Events: <none> You should see an empty redis-config key. Use kubectl exec to enter the pod and run the redis-cli tool to check the current configuration: Check maxmemory and maxmemory-policy . pradeep@learnk8s$ kubectl exec -it redis -- redis-cli 127.0.0.1:6379> CONFIG GET maxmemory 1) \"maxmemory\" 2) \"0\" 127.0.0.1:6379> CONFIG GET maxmemory-policy 1) \"maxmemory-policy\" 2) \"noeviction\" 127.0.0.1:6379> Now let's add some configuration values to the example-redis-config ConfigMap: pradeep@learnk8s$ cat example-redis-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: example-redis-config data: redis-config: | maxmemory 2mb maxmemory-policy allkeys-lru Apply the updated configmap. pradeep@learnk8s$ kubectl apply -f example-redis-config.yaml configmap/example-redis-config configured Confirm configmap is updated. pradeep@learnk8s$ kubectl describe configmap/example-redis-config Name: example-redis-config Namespace: default Labels: <none> Annotations: <none> Data ==== redis-config: ---- maxmemory 2mb maxmemory-policy allkeys-lru BinaryData ==== Events: <none> Check the Redis Pod again using redis-cli via kubectl exec to see if the configuration was applied: pradeep@learnk8s$ kubectl exec -it redis -- redis-cli 127.0.0.1:6379> CONFIG GET maxmemory 1) \"maxmemory\" 2) \"0\" 127.0.0.1:6379> CONFIG GET maxmemory-policy 1) \"maxmemory-policy\" 2) \"noeviction\" 127.0.0.1:6379> exit Are the changes reflecting now? Nope! Not yet. The configuration values have not changed because the Pod needs to be restarted to grab updated values from associated ConfigMaps. Let's delete and recreate the Pod: pradeep@learnk8s$ kubectl delete pod redis pod \"redis\" deleted pradeep@learnk8s$ kubectl apply -f redis-pod-configmap.yaml pod/redis created pradeep@learnk8s$ kubectl get pods | grep redis redis 1/1 Running 0 10s pradeep@learnk8s$ kubectl exec -it redis -- redis-cli 127.0.0.1:6379> CONFIG GET maxmemory 1) \"maxmemory\" 2) \"2097152\" 127.0.0.1:6379> CONFIG GET maxmemory-policy 1) \"maxmemory-policy\" 2) \"allkeys-lru\" 127.0.0.1:6379> exit This time we can seen updated (from ConfigMap) values inside the redis container. Secrets We are re-visiting secrets again, but this time not as an Envrionment variable but as a volume Mount. pradeep@learnk8s$ kubectl create secret generic my-secret --from-literal=user=pradeep --from-literal=password=topsecret secret/my-secret created pradeep@learnk8s$ kubectl describe secrets my-secret Name: my-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== password: 9 bytes user: 7 bytes Now, let us consume this secret as a volume in a Pod. pradeep@learnk8s$ cat pod-with-secret-volume.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-secret-volume spec: containers: - name: pod-with-secret-volume image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: my-secret pradeep@learnk8s$ kubectl create -f pod-with-secret-volume.yaml pod/pod-with-secret-volume created pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 81m pod-with-hostpath-volume 1/1 Running 0 55m pod-with-secret-volume 1/1 Running 0 15s pradeep@learnk8s$ kubectl describe pod pod-with-secret-volume Name: pod-with-secret-volume Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Sun, 20 Feb 2022 19:52:10 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.42 IPs: IP: 10.244.1.42 Containers: pod-with-secret-volume: Container ID: docker://a21403023c1bb4d5348f1aa1eee282072394e3c6785a8f0ef1203f67355f357b Image: redis Image ID: docker-pullable://redis@sha256:0d9c9aed1eb385336db0bc9b976b6b49774aee3d2b9c2788a0d0d9e239986cb3 Port: <none> Host Port: <none> State: Running Started: Sun, 20 Feb 2022 19:52:24 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /etc/foo from foo (ro) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wlmjs (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: foo: Type: Secret (a volume populated by a Secret) SecretName: my-secret Optional: false kube-api-access-wlmjs: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 93s default-scheduler Successfully assigned default/pod-with-secret-volume to k8s-m02 Normal Pulling 91s kubelet Pulling image \"redis\" Normal Pulled 80s kubelet Successfully pulled image \"redis\" in 11.292924683s Normal Created 80s kubelet Created container pod-with-secret-volume Normal Started 79s kubelet Started container pod-with-secret-volume pradeep@learnk8s$ kubectl exec -it pod-with-secret-volume -- sh # ls /etc/foo password user # ls -l /etc/foo total 0 lrwxrwxrwx 1 root root 15 Feb 20 14:22 password -> ..data/password lrwxrwxrwx 1 root root 11 Feb 20 14:22 user -> ..data/user # # cat /etc/foo/user pradeep# # cat /etc/foo/password topsecret# # exit One problem here is that the password is in plain text. We can create another YAML defitinition with a slight modification. pradeep@learnk8s$ cat pod-with-secret-volume-2.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-secret-volume-2 spec: containers: - name: pod-with-secret-volume-2 image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: my-secret items: - key: user path: my-group/my-username mode: 0777 pradeep@learnk8s$ kubectl create -f pod-with-secret-volume-2.yaml pod/pod-with-secret-volume-2 created kubectl describe pod pod-with-secret-volume-2 Name: pod-with-secret-volume-2 Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Sun, 20 Feb 2022 20:10:40 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.43 IPs: IP: 10.244.1.43 Containers: pod-with-secret-volume-2: Container ID: docker://56198fa123177a7c913973c16e686d25df1eeafa0787eef39fbf5306ac7e0b1d Image: redis Image ID: docker-pullable://redis@sha256:0d9c9aed1eb385336db0bc9b976b6b49774aee3d2b9c2788a0d0d9e239986cb3 Port: <none> Host Port: <none> State: Running Started: Sun, 20 Feb 2022 20:10:47 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /etc/foo from foo (ro) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fwthr (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: foo: Type: Secret (a volume populated by a Secret) SecretName: my-secret Optional: false kube-api-access-fwthr: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 38s default-scheduler Successfully assigned default/pod-with-secret-volume-2 to k8s-m02 Normal Pulling 36s kubelet Pulling image \"redis\" Normal Pulled 32s kubelet Successfully pulled image \"redis\" in 4.744969468s Normal Created 32s kubelet Created container pod-with-secret-volume-2 Normal Started 31s kubelet Started container pod-with-secret-volume-2 pradeep@learnk8s$ kubectl exec -it pod-with-secret-volume-2 -- sh # ls /etc/foo my-group # ls -l /etc/foo total 0 lrwxrwxrwx 1 root root 15 Feb 20 14:40 my-group -> ..data/my-group # cat /etc/foo/my-group/my-username pradeep# at /etc/foo/my-group/my-password sh: 9: at: not found Only username is exposed while the password is not projected becuase the key is not exposed. pradeep@learnk8s$ cat pod-with-secret-volume-3.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-secret-volume-3 spec: containers: - name: pod-with-secret-volume-3 image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: my-secret items: - key: user mode: 0777 path: my-group/my-username - key: password mode: 0777 path: my-group/my-pass pradeep@learnk8s$ kubectl create -f pod-with-secret-volume-3.yaml pod/pod-with-secret-volume-3 created pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 110m pod-with-hostpath-volume 1/1 Running 0 84m pod-with-secret-volume 1/1 Running 0 29m pod-with-secret-volume-2 1/1 Running 0 3m40s pod-with-secret-volume-3 1/1 Running 0 16s pradeep@learnk8s$ kubectl exec -it pod-with-secret-volume-3 sh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. # ls -l /etc/foo total 0 lrwxrwxrwx 1 root root 15 Feb 20 14:51 my-group -> ..data/my-group # cat /etc/foo/my-group/my-username pradeep# # cat /etc/foo/my-group/my-pass topsecret# # exit PersistentVolume Create a directory called /mnt/data on the k8s node. Within that directory, create index.html file with some text. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ sudo mkdir /mnt/data $ sudo sh -c \"echo 'Hello from Kubernetes storage' > /mnt/data/indext.html\" $ cat /mnt/data/indext.html Hello from Kubernetes storage $ exit logout Now create a PersistentVolume named my-pv-volume of 1Gi using the hostPath pointing to the newly created directory. There are three types of accessModes , for this example use, RWO . RWO ReadWriteOnce RWX ReadWriteMany ROX ReadyOnlyMany pradeep@learnk8s$ cat persistent-volume-demo.yaml apiVersion: v1 kind: PersistentVolume metadata: name: my-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\" Create the PV using this YAML file. pradeep@learnk8s$ kubectl create -f persistent-volume-demo.yaml persistentvolume/my-pv-volume created Verify the available PVs. pradeep@learnk8s$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE my-pv-volume 1Gi RWO Retain Available manual 4s Describe the PersistentVolume for addtional details. pradeep@learnk8s$ kubectl describe pv Name: my-pv-volume Labels: type=local Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Available Claim: Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 1Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mnt/data HostPathType: Events: <none> PersistentVolumeClaim pradeep@learnk8s$ cat persistent-volume-claim-demo.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 300Mi pradeep@learnk8s$ kubectl create -f persistent-volume-claim-demo.yaml persistentvolumeclaim/my-pv-claim created pradeep@learnk8s$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-pv-claim Bound my-pv-volume 1Gi RWO manual 40s pradeep@learnk8s$ kubectl describe pvc Name: my-pv-claim Namespace: default StorageClass: manual Status: Bound Volume: my-pv-volume Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Gi Access Modes: RWO VolumeMode: Filesystem Used By: <none> Events: <none> pradeep@learnk8s$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE my-pv-volume 1Gi RWO Retain Bound default/my-pv-claim manual 12m pradeep@learnk8s$ kubectl describe pv Name: my-pv-volume Labels: type=local Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: default/my-pv-claim Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 1Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mnt/data HostPathType: Events: <none> The next step is to create a Pod that uses this PersistentVolumeClaim as a volume. Here is the configuration file for the Pod: pradeep@learnk8s$ cat my-pv-pod.yaml apiVersion: v1 kind: Pod metadata: name: my-pv-pod spec: volumes: - name: my-pv-storage persistentVolumeClaim: claimName: my-pv-claim containers: - name: my-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: my-pv-storage pradeep@learnk8s$ kubectl create -f my-pv-pod.yaml pod/my-pv-pod created The scheduler has placed this Pod on k8s-m02 node. pradeep@learnk8s$ kubectl get pods my-pv-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES my-pv-pod 1/1 Running 0 10s 10.244.1.2 k8s-m02 <none> <none> Let us describe the pod and look at the Volumes section. pradeep@learnk8s$ kubectl describe pods my-pv-pod Name: my-pv-pod Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Sun, 27 Feb 2022 22:35:45 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.2 IPs: IP: 10.244.1.2 Containers: my-pv-container: Container ID: docker://9dc92c5c1037ad4d534fe6440275f62a68f521e3bd22fe47b57c06a8b3414e24 Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: 80/TCP Host Port: 0/TCP State: Running Started: Sun, 27 Feb 2022 22:35:49 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /usr/share/nginx/html from my-pv-storage (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xk6kh (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: my-pv-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: my-pv-claim ReadOnly: false kube-api-access-xk6kh: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 19s default-scheduler Successfully assigned default/my-pv-pod to k8s-m02 Normal Pulling 18s kubelet Pulling image \"nginx\" Normal Pulled 15s kubelet Successfully pulled image \"nginx\" in 2.858388422s Normal Created 15s kubelet Created container my-pv-container Normal Started 15s kubelet Started container my-pv-container It looks like no issues, but let us verify the nginx application by logging into the pod and issuing the curl localhost command. pradeep@learnk8s$ kubectl exec -it my-pv-pod -- /bin/sh # curl localhost <html> <head><title>403 Forbidden</title></head> <body> <center><h1>403 Forbidden</h1></center> <hr><center>nginx/1.21.6</center> </body> </html> # exit Hmm! This seems to be not working. It is forbidden. But this is expected right, we did not have the volume (hostPath) on this k8s-m02 node. We created it on k8s node only. To solve this, we can add a nodeName spec to the Pod definition. Delete this pod and after adding the nodeName recreate the Pod. pradeep@learnk8s$ kubectl delete pod my-pv-pod pod \"my-pv-pod\" deleted pradeep@learnk8s$ vi my-pv-pod.yaml pradeep@learnk8s$ cat my-pv-pod.yaml apiVersion: v1 kind: Pod metadata: name: my-pv-pod spec: nodeName: k8s volumes: - name: my-pv-storage persistentVolumeClaim: claimName: my-pv-claim containers: - name: my-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: my-pv-storage pradeep@learnk8s$ kubectl create -f my-pv-pod.yaml pod/my-pv-pod created :warning: There seems to be some issue with this. Even after scheduling this pod on the node k8s , nginx container is reporting Forbidden. There are couple of issues already reported, but could not find any resolution to this problem yet. https://github.com/kubernetes/website/issues/9523 StorageClass In the previous example, we used a storageClass named manual , but that does not seem to be present. pradeep@learnk8s$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 12d There is a storage class by name standard, and is the default storage class. pradeep@learnk8s$ kubectl describe sc Name: standard IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"EnsureExists\"},\"name\":\"standard\"},\"provisioner\":\"k8s.io/minikube-hostpath\"} ,storageclass.kubernetes.io/is-default-class=true Provisioner: k8s.io/minikube-hostpath Parameters: <none> AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> :memo: Changing the storage class name to standard also did not help solve the Forbidden issue with the nginx . Strangely, inside the Pod, we can access the index.html file with cat command, but curl is not working. pradeep@learnk8s$ kubectl exec -it my-pv-pod -- /bin/bash root@my-pv-pod:/# cat /usr/share/nginx/html/indext.html Hello from Kubernetes storage pradeep@learnk8s$ kubectl explain sc KIND: StorageClass VERSION: storage.k8s.io/v1 DESCRIPTION: StorageClass describes the parameters for a class of storage for which PersistentVolumes can be dynamically provisioned. StorageClasses are non-namespaced; the name of the storage class according to etcd is in ObjectMeta.Name. FIELDS: allowVolumeExpansion <boolean> AllowVolumeExpansion shows whether the storage class allow volume expand allowedTopologies <[]Object> Restrict the node topologies where volumes can be dynamically provisioned. Each volume plugin defines its own supported topology specifications. An empty TopologySelectorTerm list means there is no topology restriction. This field is only honored by servers that enable the VolumeScheduling feature. apiVersion <string> APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources kind <string> Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata <Object> Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata mountOptions <[]string> Dynamically provisioned PersistentVolumes of this storage class are created with these mountOptions, e.g. [\"ro\", \"soft\"]. Not validated - mount of the PVs will simply fail if one is invalid. parameters <map[string]string> Parameters holds the parameters for the provisioner that should create volumes of this storage class. provisioner <string> -required- Provisioner indicates the type of the provisioner. reclaimPolicy <string> Dynamically provisioned PersistentVolumes of this storage class are created with this reclaimPolicy. Defaults to Delete. volumeBindingMode <string> VolumeBindingMode indicates how PersistentVolumeClaims should be provisioned and bound. When unset, VolumeBindingImmediate is used. This field is only honored by servers that enable the VolumeScheduling feature. Static Binding Here is the PersistentVolume definition. If we omit the storageClassName, PVC is net getting bound. As you see, there is a default storageClass named standard and PVC is using it by default. pradeep@learnk8s$ cat pv.yaml kind: PersistentVolume apiVersion: v1 metadata: name: pv spec: storageClassName: standard capacity: storage: 512m accessModes: - ReadWriteMany hostPath: path: /data/config Create the PV from the YAML file. pradeep@learnk8s$ kubectl create -f pv.yaml persistentvolume/pv created Verify the available PVs. The newly created PV should be in Available state. Currently, there are no Claims for this volume. pradeep@learnk8s$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv 512m RWX Retain Available standard 8s Describe the PV for addtional details. pradeep@learnk8s$ kubectl describe pv pv Name: pv Labels: <none> Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: standard Status: Available Claim: Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 512m Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /data/config HostPathType: Events: <none> Now, create a PVC with same accessModes: ReadWriteMany . pradeep@learnk8s$ cat pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 256m Create PVC from the YAML file. pradeep@learnk8s$ kubectl create -f pvc.yaml persistentvolumeclaim/pvc created Verify the newly created PVC. It should be in Bound state. Note the STORAGECLASS column. It is using the default one named standard . pradeep@learnk8s$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc Bound pv 512m RWX standard 4s Describe the PVC for addtional details. pradeep@learnk8s$ kubectl describe pvc Name: pvc Namespace: default StorageClass: standard Status: Bound Volume: pv Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 512m Access Modes: RWX VolumeMode: Filesystem Used By: <none> Events: <none> Now, create a Pod that uses the newly defined PVC. pradeep@learnk8s$ cat pod.yaml apiVersion: v1 kind: Pod metadata: name: app spec: containers: - image: nginx name: app volumeMounts: - mountPath: \"/data/app/config\" name: configpvc volumes: - name: configpvc persistentVolumeClaim: claimName: pvc restartPolicy: Never Create the Pod from the YAML file. pradeep@learnk8s$ kubectl create -f pod.yaml pod/app created Use the -o wide option to check the IP and NODE details. pradeep@learnk8s$ kubectl get pods app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES app 1/1 Running 0 18s 10.244.1.5 k8s-m02 <none> <none> Describe the Pod and look for Volumes section. pradeep@learnk8s$ kubectl describe pods app Name: app Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Mon, 28 Feb 2022 09:25:46 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.5 IPs: IP: 10.244.1.5 Containers: app: Container ID: docker://09871149e09a1da02a86a9e38ace82082def59aa38dffbf739e14f7694f0f79e Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: <none> Host Port: <none> State: Running Started: Mon, 28 Feb 2022 09:25:50 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /data/app/config from configpvc (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vfx8b (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: configpvc: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: pvc ReadOnly: false kube-api-access-vfx8b: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 63s default-scheduler Successfully assigned default/app to k8s-m02 Normal Pulling 62s kubelet Pulling image \"nginx\" Normal Pulled 59s kubelet Successfully pulled image \"nginx\" in 3.044922832s Normal Created 59s kubelet Created container app Normal Started 59s kubelet Started container app Login to the Container and create a file in the /data/app/config folder, which is the container MountPath. pradeep@learnk8s$ kubectl exec -it app -- /bin/sh # cd /data/app/config # ls -l total 0 # touch hello.txt # echo \"Testing PV, PVC, and SC in K8S!\" > hello.txt # exit Login to the minikube node and check if you are able to view the hello.txt file in the host. On the control plane, there is no such file in the /data/config folder. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ls /data/config/ $ ls -la /data/config/ total 8 drwxr-xr-x 2 root root 4096 Feb 28 03:33 . drwxr-xr-x 3 root root 4096 Feb 28 03:33 .. $ exit logout On the other node, k8s-m02 , there is the hello.txt file. This is because, the app pod got created in this node. pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ls /data/config hello.txt $ ls -la /data/config/ total 12 drwxr-xr-x 2 root root 4096 Feb 28 03:58 . drwxr-xr-x 3 root root 4096 Feb 28 03:55 .. -rw-r--r-- 1 root root 32 Feb 28 03:58 hello.txt $ cat /data/config/hello.txt Testing PV, PVC, and SC in K8S! $ exit logout We can also view the contents of the hello.txt file. Verify the PV, PVC, and SC details. pradeep@learnk8s$ kubectl get pv,pvc,sc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pv 512m RWX Retain Bound default/pvc standard 9m35s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/pvc Bound pv 512m RWX standard 8m17s NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/standard (default) k8s.io/minikube-hostpath Delete Immediate false 12d Describe the PV one more time, after the Pod creation, to see if there is any change. One change that we notice is that the Claim is showing default/pvc which was empty earlier. Also, Staus changed to Bound from Available . pradeep@learnk8s$ kubectl describe pv Name: pv Labels: <none> Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: standard Status: Bound Claim: default/pvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 512m Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /data/config HostPathType: Events: <none> Similarly describe the PVC and look for the changes. The Used By value has changed to app now, showing that it is used by this Pod. pradeep@learnk8s$ kubectl describe pvc Name: pvc Namespace: default StorageClass: standard Status: Bound Volume: pv Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 512m Access Modes: RWX VolumeMode: Filesystem Used By: app Events: <none> Also, look at the StorageClass description. Verify that, IsDefaultClass: Yes . pradeep@learnk8s$ kubectl describe sc Name: standard IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"EnsureExists\"},\"name\":\"standard\"},\"provisioner\":\"k8s.io/minikube-hostpath\"} ,storageclass.kubernetes.io/is-default-class=true Provisioner: k8s.io/minikube-hostpath Parameters: <none> AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Now delete the pod. pradeep@learnk8s$ kubectl delete pod app pod \"app\" deleted After deleting the pod, check if the data is persistent or not. You should still be able to view the contents of the hello.txt file on the k8s-m02 node, even after deleting the Pod, confirming that the storage is persistent now. pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ cd /data/config/ $ cat hello.txt Testing PV, PVC, and SC in K8S! $ exit logout Dynamic Binding Define a new storage class named pradeep-sc-demo in the kube-system namespace using a sample YAML file. pradeep@learnk8s$ cat sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: namespace: kube-system name: pradeep-sc-demo annotations: storageclass.beta.kubernetes.io/is-default-class: \"false\" labels: addonmanager.kubernetes.io/mode: Reconcile provisioner: k8s.io/minikube-hostpath Create the storage class from this YAML file and verify that it was created. pradeep@learnk8s$ kubectl create -f sc.yaml storageclass.storage.k8s.io/pradeep-sc-demo created Now you should see two storageclasses, both with same PROVISIONER details. Also, not the default value next to the standard class, which is not present for the newly defined storage class. pradeep@learnk8s$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE pradeep-sc-demo k8s.io/minikube-hostpath Delete Immediate false 47s standard (default) k8s.io/minikube-hostpath Delete Immediate false 12d Let us modify the PVC definition file to specify this pradeep-sc-demo StorageClass, instead of the standard . First, let us delete the existing PVC. pradeep@learnk8s$ kubectl delete pvc pvc persistentvolumeclaim \"pvc\" deleted Modify the PVC definition, to include the new storage class. pradeep@learnk8s$ cat pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc spec: storageClassName: pradeep-sc-demo accessModes: - ReadWriteMany resources: requests: storage: 256m Create the PVC from the modified definition file. pradeep@learnk8s$ kubectl create -f pvc.yaml persistentvolumeclaim/pvc created Verify the Status of the new PVC. pradeep@learnk8s$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc Pending pradeep-sc-demo 64s It is still showing Pending , to find out why let us describe it. pradeep@learnk8s$ kubectl describe pvc Name: pvc Namespace: default StorageClass: pradeep-sc-demo Status: Pending Volume: Labels: <none> Annotations: volume.beta.kubernetes.io/storage-provisioner: k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner: k8s.io/minikube-hostpath Finalizers: [kubernetes.io/pvc-protection] Capacity: Access Modes: VolumeMode: Filesystem Used By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ExternalProvisioning 2s (x8 over 76s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator We can see that the PVC is waiting for a matching volume to be created. Our existing PV is using the standard storageclass which is not matching with this PVC definition. So let us delete the existing PV and modify it to specify the new SC. pradeep@learnk8s$ kubectl delete pv pv persistentvolume \"pv\" deleted Here is the modified PV definition. pradeep@learnk8s$ cat pv.yaml kind: PersistentVolume apiVersion: v1 metadata: name: pv spec: storageClassName: pradeep-sc-demo capacity: storage: 512m accessModes: - ReadWriteMany hostPath: path: /data/config pradeep@learnk8s$ kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pv 512m RWX Retain Bound default/pvc pradeep-sc-demo 15s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/pvc Bound pv 512m RWX pradeep-sc-demo 6m29s Now that the storageclass is matching, both the PV and PVC are in Bound State. Create the app pod again using the same manifest file that we used earlier. pradeep@learnk8s$ cat pod.yaml apiVersion: v1 kind: Pod metadata: name: app spec: containers: - image: nginx name: app volumeMounts: - mountPath: \"/data/app/config\" name: configpvc volumes: - name: configpvc persistentVolumeClaim: claimName: pvc restartPolicy: Never pradeep@learnk8s$ kubectl create -f pod.yaml pod/app created pradeep@learnk8s$ kubectl get pods app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES app 1/1 Running 0 2m19s 10.244.1.6 k8s-m02 <none> <none> Shell into the Pod and create a file in the mounted directory. pradeep@learnk8s$ kubectl exec app -it -- /bin/sh # cd /data/app/config # ls -l total 4 -rw-r--r-- 1 root root 32 Feb 28 03:58 hello.txt # touch HiAgain.txt # echo \"Hello again from K8s PV,PVC, and SC!\" > HiAgain.txt # cat HiAgain.txt Hello again from K8s PV,PVC, and SC! # exit Becuase of persistent nature, we still see the old hello.txt file in this new container. We have created another file called HiAgain.txt with some sample text. pradeep@learnk8s$ kubectl get pv,pvc,sc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pv 512m RWX Retain Bound default/pvc pradeep-sc-demo 6m43s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/pvc Bound pv 512m RWX pradeep-sc-demo 12m NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/pradeep-sc-demo k8s.io/minikube-hostpath Delete Immediate false 18m storageclass.storage.k8s.io/standard (default) k8s.io/minikube-hostpath Delete Immediate false 12d Let us delete the Pod again and verify the files. pradeep@learnk8s$ kubectl delete pod app pod \"app\" deleted pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ cd /data/config/ $ ls HiAgain.txt hello.txt $ ls -la total 16 drwxr-xr-x 2 root root 4096 Feb 28 04:50 . drwxr-xr-x 3 root root 4096 Feb 28 03:55 .. -rw-r--r-- 1 root root 37 Feb 28 04:50 HiAgain.txt -rw-r--r-- 1 root root 32 Feb 28 03:58 hello.txt $ cat HiAgain.txt Hello again from K8s PV,PVC, and SC! $ cat hello.txt Testing PV, PVC, and SC in K8S! $ exit logout This confirms that, the storage is persistent again (even after Pod deletion). Networking KindNet Welcome to Kubernetes Networking. Let us setup a fresh minikube cluster with default settings and with 3 nodes in it. pradeep@learnk8s$ minikube start --nodes=3 \ud83d\ude04 minikube v1.25.2 on Darwin 12.2.1 \u2728 Automatically selected the hyperkit driver \ud83d\udcbe Downloading driver docker-machine-driver-hyperkit: > docker-machine-driver-hyper...: 65 B / 65 B [----------] 100.00% ? p/s 0s > docker-machine-driver-hyper...: 8.35 MiB / 8.35 MiB 100.00% 9.87 MiB p/s \ud83d\udd11 The 'hyperkit' driver requires elevated permissions. The following commands will be executed: $ sudo chown root:wheel /Users/pradeep/.minikube/bin/docker-machine-driver-hyperkit $ sudo chmod u+s /Users/pradeep/.minikube/bin/docker-machine-driver-hyperkit Password: \ud83d\udcbf Downloading VM boot image ... > minikube-v1.25.2.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s > minikube-v1.25.2.iso: 237.06 MiB / 237.06 MiB [] 100.00% 6.12 MiB p/s 39s \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udcbe Downloading Kubernetes v1.23.3 preload ... > preloaded-images-k8s-v17-v1...: 505.68 MiB / 505.68 MiB 100.00% 9.37 MiB \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa kubelet.cni-conf-dir=/etc/cni/net.mk \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd17 Configuring CNI (Container Networking Interface) ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: storage-provisioner, default-storageclass \ud83d\udc4d Starting worker node minikube-m02 in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=172.16.30.3 \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa env NO_PROXY=172.16.30.3 \ud83d\udd0e Verifying Kubernetes components... \ud83d\udc4d Starting worker node minikube-m03 in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=172.16.30.3,172.16.30.4 \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa env NO_PROXY=172.16.30.3 \u25aa env NO_PROXY=172.16.30.3,172.16.30.4 \ud83d\udd0e Verifying Kubernetes components... \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default pradeep@learnk8s$ Our cluster nodes are assinged the IPs: 172.16.30.3 , 172.16.30.4 , and 172.16.30.5 respectively. Also, note that the CNI config directory kubelet.cni-conf-dir=/etc/cni/net.mk and minikube is configuring CNI (Container Networking Interface) during the start. pradeep@learnk8s$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane,master 6m54s v1.23.3 172.16.30.3 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 minikube-m02 Ready <none> 4m43s v1.23.3 172.16.30.4 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 minikube-m03 Ready <none> 74s v1.23.3 172.16.30.5 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 pradeep@learnk8s$ Let us take a look at all the pods in this newly deployed cluster. Currently, all the pods are in the kube-system namespace. The pods that are of interest to us in this post are the ones starting with the name kindnet . So what is Kindnet? Kindnet is a simple CNI plugin for Kubernetes with IPv4 and IPv6 support that provides the Cluster Networking. pradeep@learnk8s$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-64897985d-llrjh 1/1 Running 0 7m25s kube-system etcd-minikube 1/1 Running 0 7m40s kube-system kindnet-4mrvw 1/1 Running 0 2m2s kube-system kindnet-cpv4s 1/1 Running 0 5m31s kube-system kindnet-xqjlm 1/1 Running 0 7m26s kube-system kube-apiserver-minikube 1/1 Running 0 7m37s kube-system kube-controller-manager-minikube 1/1 Running 0 7m37s kube-system kube-proxy-b97w8 1/1 Running 0 7m26s kube-system kube-proxy-gtlw7 1/1 Running 0 2m2s kube-system kube-proxy-rts4b 1/1 Running 0 5m31s kube-system kube-scheduler-minikube 1/1 Running 0 7m37s kube-system storage-provisioner 1/1 Running 1 7m35s There are three kindnet pods, and if we check the IP address of these pods, all of them are having the same IP address as that of their node. pradeep@learnk8s$ kubectl get pods -A -o wide | grep kindnet kube-system kindnet-4mrvw 1/1 Running 0 6m17s 172.16.30.5 minikube-m03 <none> <none> kube-system kindnet-cpv4s 1/1 Running 0 9m46s 172.16.30.4 minikube-m02 <none> <none> kube-system kindnet-xqjlm 1/1 Running 0 11m 172.16.30.3 minikube <none> <none> pradeep@learnk8s$ Let us login to the first node, and check its routing table and all the interfaces currently configured on this node. pradeep@learnk8s$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether d6:df:bb:d6:c7:bc brd ff:ff:ff:ff:ff:ff inet 172.16.30.3/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 85395sec preferred_lft 85395sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:df:e0:80:59 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: veth2b322de6@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 8e:09:87:56:8c:50 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.0.1/32 brd 10.244.0.1 scope global veth2b322de6 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.3 metric 1024 10.244.0.2 dev veth2b322de6 scope host 10.244.1.0/24 via 172.16.30.4 dev eth0 10.244.2.0/24 via 172.16.30.5 dev eth0 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.3 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.3 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown This minikube node has the IP address of 172.16.30.3/24 on the eth0 interface and there is a static route to 10.244.1.0/24 with next-hop as the minikube-m02 node with IP address of 172.16.30.4 and similarly another static route to 10.244.2.0/24 with next-hop as 172.16.30.5 which is the IP address of the the minikube-m03 node. There is another virtual interface ( veth ) called veth2b322de6@if4 with an IP address of 10.244.0.1/32 . So what are these subnets 10.244.X.0/24 used for? If we look at the kindnet CNI configuration file (located at the path given in the minikube start output shown above), on this minikube node, we can see that there is a subnet 10.244.0.0/24 range defined. $ cat /etc/cni/net.mk/10-kindnet.conflist { \"cniVersion\": \"0.3.1\", \"name\": \"kindnet\", \"plugins\": [ { \"type\": \"ptp\", \"ipMasq\": false, \"ipam\": { \"type\": \"host-local\", \"dataDir\": \"/run/cni-ipam-state\", \"routes\": [ { \"dst\": \"0.0.0.0/0\" } ], \"ranges\": [ [ { \"subnet\": \"10.244.0.0/24\" } ] ] } , \"mtu\": 1500 }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } Similarly, let us verify the same details in other two nodes of the cluster. First, login to the minikube-m02 node. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 2e:6c:52:09:b4:bb brd ff:ff:ff:ff:ff:ff inet 172.16.30.4/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 84780sec preferred_lft 84780sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:b9:25:87:9f brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.4 metric 1024 10.244.0.0/24 via 172.16.30.3 dev eth0 10.244.2.0/24 via 172.16.30.5 dev eth0 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.4 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.4 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ cat /etc/cni/net.mk/10-kindnet.conflist { \"cniVersion\": \"0.3.1\", \"name\": \"kindnet\", \"plugins\": [ { \"type\": \"ptp\", \"ipMasq\": false, \"ipam\": { \"type\": \"host-local\", \"dataDir\": \"/run/cni-ipam-state\", \"routes\": [ { \"dst\": \"0.0.0.0/0\" } ], \"ranges\": [ [ { \"subnet\": \"10.244.1.0/24\" } ] ] } , \"mtu\": 1500 }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } $ We can see a similar setup. Local subnet range defined in the CNI (kindnet) configuration file is the 10.244.1.0/24 subnet and two static routes ( 10.244.0.0/24 via 172.16.30.3 and another static route 10.244.2.0/24 via 172.16.30.5 ). Finally, let us check the same in the minikube-m03 node. pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 84738sec preferred_lft 84738sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.5 metric 1024 10.244.0.0/24 via 172.16.30.3 dev eth0 10.244.1.0/24 via 172.16.30.4 dev eth0 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.5 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.5 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ cat /etc/cni/net.mk/10-kindnet.conflist { \"cniVersion\": \"0.3.1\", \"name\": \"kindnet\", \"plugins\": [ { \"type\": \"ptp\", \"ipMasq\": false, \"ipam\": { \"type\": \"host-local\", \"dataDir\": \"/run/cni-ipam-state\", \"routes\": [ { \"dst\": \"0.0.0.0/0\" } ], \"ranges\": [ [ { \"subnet\": \"10.244.2.0/24\" } ] ] } , \"mtu\": 1500 }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } $ exit logout Here also, very similar setup. 10.244.2.0/24 is the local subnet and remote subnets are cofnigured via static routes ( 10.244.0.0/24 via 172.16.30.3 and 10.244.1.0/24 via 172.16.30.4 ). One thing to note is that, in both of the worker nodes, there isn't any veth interface present yet, unlike the controleplane node. To understand, why let us get the IP addresses of all the running pods. pradeep@learnk8s$ kubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-64897985d-llrjh 1/1 Running 0 32m 10.244.0.2 minikube <none> <none> kube-system etcd-minikube 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kindnet-4mrvw 1/1 Running 0 27m 172.16.30.5 minikube-m03 <none> <none> kube-system kindnet-cpv4s 1/1 Running 0 31m 172.16.30.4 minikube-m02 <none> <none> kube-system kindnet-xqjlm 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kube-apiserver-minikube 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kube-controller-manager-minikube 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kube-proxy-b97w8 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kube-proxy-gtlw7 1/1 Running 0 27m 172.16.30.5 minikube-m03 <none> <none> kube-system kube-proxy-rts4b 1/1 Running 0 31m 172.16.30.4 minikube-m02 <none> <none> kube-system kube-scheduler-minikube 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system storage-provisioner 1/1 Running 1 33m 172.16.30.3 minikube <none> <none> pradeep@learnk8s$ There is one Pod named coredns-64897985d-llrjh in the kube-system namespace with an IP address of 10.244.0.2 which is in the same subnet range defined in the CNI config on the minikube node. Let us create our first pod (we can use any image, it does not matter). For test purposes, let us create a new pod using the busybox image. pradeep@learnk8s$ kubectl run busybox --image=busybox pod/busybox created Verify the IP address assigned and the node on which it is running. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES busybox 0/1 Completed 0 12s 10.244.2.2 minikube-m03 <none> <none> pradeep@learnk8s$ We can see that the scheduler has assigned the minikube-m03 node for this new pod and this busybox pod obtained its IP address 10.244.2.2 which is from the CNI assigned subnet ( 10.244.2.0/24 ) for this node. Now, let us go back to the minikube-m03 node and check the list of interfaces and see if there is anything new! pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 83749sec preferred_lft 83749sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: veth07b7de9e@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 6a:9a:f7:af:ac:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.2.1/32 brd 10.244.2.1 scope global veth07b7de9e valid_lft forever preferred_lft forever $ Compared to the initial setup, there is one new interface (#5, with the name veth07b7de9e@if4 ). This is very similar to the controlplane node now. This veth interface has the first IP address ( 10.244.2.1/32 ) from the CNI assigned subnet. I tried to login to this container and check few things from inside the container, but before I do that the container crashed, so I have deleted it. pradeep@learnk8s$ kubectl exec -it busybox -- /bin/bash error: unable to upgrade connection: container not found (\"busybox\") pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE busybox 0/1 CrashLoopBackOff 6 (63s ago) 7m3s pradeep@learnk8s$ kubectl delete pod busybox pod \"busybox\" deleted Create another containter, this time using another image, nginx for test purposes. pradeep@learnk8s$ kubectl run nginx --image=nginx pod/nginx created Verify the IP address of this new pod and the node on which it is running. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 25s 10.244.2.3 minikube-m03 <none> <none> pradeep@learnk8s$ Ah!, this pod also got assigned to the same node, minikube-m03 and look at the IP address, the next IP address in the same range, 10.244.2.3 . pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 83069sec preferred_lft 83069sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 6: veth37e46d8f@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 7e:96:f9:98:5f:4a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.2.1/32 brd 10.244.2.1 scope global veth37e46d8f valid_lft forever preferred_lft forever $ From the routing table, we can see that the new Pod IP is routed via the veth interface 10.244.2.3 dev veth37e46d8f . $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.5 metric 1024 10.244.0.0/24 via 172.16.30.3 dev eth0 10.244.1.0/24 via 172.16.30.4 dev eth0 10.244.2.3 dev veth37e46d8f scope host 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.5 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.5 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ Let us manually schedule a Pod on the minikube-m02 node as well and observe. pradeep@learnk8s$ cat my-pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx-manual spec: nodeName: minikube-m02 containers: - image: nginx name: nginx pradeep@learnk8s$ kubectl create -f my-pod.yaml pod/nginx-manual created pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-manual 1/1 Running 0 73s 10.244.1.2 minikube-m02 <none> <none> pradeep@learnk8s$ We can see that, this pod has been given an IP ( 10.244.1.2 ) from the 10.244.1.0/24 subnet allocated to minikube-m02 node. Let us login to this node and verify the list of currently running pods with the docker ps command. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6397d143118 nginx \"/docker-entrypoint.\u2026\" 3 minutes ago Up 3 minutes k8s_nginx_nginx-manual_default_950195d8-bc1d-420f-aede-fcd4a273f0e0_0 d2fb47206b71 k8s.gcr.io/pause:3.6 \"/pause\" 4 minutes ago Up 4 minutes k8s_POD_nginx-manual_default_950195d8-bc1d-420f-aede-fcd4a273f0e0_0 ea4e0c180710 6de166512aa2 \"/bin/kindnetd\" 9 minutes ago Up 9 minutes k8s_kindnet-cni_kindnet-cpv4s_kube-system_5d3d3977-c5ff-4bea-917a-e0db52896da2_1 6ae7df286995 9b7cc9982109 \"/usr/local/bin/kube\u2026\" 9 minutes ago Up 9 minutes k8s_kube-proxy_kube-proxy-rts4b_kube-system_9773fb68-a469-4cb2-827b-546076677b3b_1 d77c3e7c06e4 k8s.gcr.io/pause:3.6 \"/pause\" 9 minutes ago Up 9 minutes k8s_POD_kindnet-cpv4s_kube-system_5d3d3977-c5ff-4bea-917a-e0db52896da2_1 5e18805db658 k8s.gcr.io/pause:3.6 \"/pause\" 9 minutes ago Up 9 minutes k8s_POD_kube-proxy-rts4b_kube-system_9773fb68-a469-4cb2-827b-546076677b3b_1 $ Obtain the container ID for the nginx-manual pod. In this case ,it is d6397d143118 . Use the docker inspect command to get the PID of this container. $ docker inspect d2fb47206b71 | grep Pid \"Pid\": 4200, \"PidMode\": \"\", \"PidsLimit\": null, $ Now, take the Pid and use the nsenter command to issue the ip a command from inside this Pod namespace. This requires root privileges, so use sudo command. $ sudo nsenter -t 4200 -n ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: eth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 52:a1:40:3b:54:18 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.1.2/24 brd 10.244.1.255 scope global eth0 valid_lft forever preferred_lft forever $ We can see that the Pod IP ( 10.244.1.2 ) is present on the interface `#4 named eth0@if5 . Look at all the interfaces from this minikube-m02 node. $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 2e:6c:52:09:b4:bb brd ff:ff:ff:ff:ff:ff inet 172.16.30.4/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 85770sec preferred_lft 85770sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:41:3c:16:fe brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: vethfa21a27c@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether b2:26:8c:88:bf:d3 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.1.1/32 brd 10.244.1.1 scope global vethfa21a27c valid_lft forever preferred_lft forever $ From the interface naming, we can see the link between the Pod namespace and the host. The 4: eth0@if5: interface with the IP address 10.244.1.2/24 is linked to the 5: vethfa21a27c@if4: interface with the IP address 10.244.1.1/32 . The name followed by the @ symbol tells us the interface number. For example, inside the Pod namespace, it is shown as @if5 for the interface number 4: , this corresponds to interface number 5: on the host, which is nothing but the 5: vethfa21a27c@if4: . From this 5: vethfa21a27c@if4: on the host, @if4 corresponds to the interface number 4 on the Pod, which is the eth0 interface indicated by 4: eth0@if5: . Now you can see the full picture of the internal connectivity. Just to confirm, this Pod hosted on minikube-m02 can communicate with a Pod with IP address 10.244.0.2 hosted on the minikube node. This can be confirmed by first looking at the Pod routing table, using the same nsenter command. There is a default route pointing to the gateway 10.244.1.1 . $ sudo nsenter -t 4200 -n ip route default via 10.244.1.1 dev eth0 10.244.1.0/24 via 10.244.1.1 dev eth0 src 10.244.1.2 10.244.1.1 dev eth0 scope link src 10.244.1.2 $ Ping to another Pod in the same cluster, but on another node. $ sudo nsenter -t 4200 -n ping 10.244.0.2 -c 3 PING 10.244.0.2 (10.244.0.2): 56 data bytes 64 bytes from 10.244.0.2: seq=0 ttl=62 time=1.313 ms 64 bytes from 10.244.0.2: seq=1 ttl=62 time=1.980 ms 64 bytes from 10.244.0.2: seq=2 ttl=62 time=1.139 ms --- 10.244.0.2 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 1.139/1.477/1.980 ms $ We can also see that using the same default route, this Pod can communicate with the outside cluster as well. $ sudo nsenter -t 4200 -n ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes 64 bytes from 8.8.8.8: seq=0 ttl=115 time=12.515 ms 64 bytes from 8.8.8.8: seq=1 ttl=115 time=12.238 ms 64 bytes from 8.8.8.8: seq=2 ttl=115 time=11.216 ms 64 bytes from 8.8.8.8: seq=3 ttl=115 time=11.303 ms ^C --- 8.8.8.8 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 11.216/11.818/12.515 ms $ The following diagram summarizes our discussion so far. This concludes our initial verification of the Kubernetes networking on the minikube with the default CNI (kindnet) which uses the simple static routes to facilitate communication across nodes. We will look at other CNIs later in other posts. Calico Kubernetes Networking with Calico CNI Let us build another minikube cluster, this time with the Calico CNI. Minikube supports many CNIs. From the minikube start -h output, we can see the supported CNI plug-ins list. --cni='': CNI plug-in to use. Valid options: auto, bridge, calico, cilium, flannel, kindnet, or path to a CNI manifest (default: auto) Start the 3-node minikube cluster with calico CNI plugin. pradeep@learnk8s$ minikube start --nodes=3 --cni=calico \ud83d\ude04 minikube v1.25.2 on Darwin 12.2.1 \u2728 Automatically selected the hyperkit driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd17 Configuring Calico (Container Networking Interface) ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \ud83d\udc4d Starting worker node minikube-m02 in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=172.16.30.6 \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa env NO_PROXY=172.16.30.6 \ud83d\udd0e Verifying Kubernetes components... \ud83d\udc4d Starting worker node minikube-m03 in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=172.16.30.6,172.16.30.7 \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa env NO_PROXY=172.16.30.6 \u25aa env NO_PROXY=172.16.30.6,172.16.30.7 \ud83d\udd0e Verifying Kubernetes components... \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default pradeep@learnk8s$ Verify the cluster node IPs: 172.16.30.6 , 172.16.30.7 , and 172.16.30.8 . pradeep@learnk8s$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane,master 22m v1.23.3 172.16.30.6 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 minikube-m02 Ready <none> 20m v1.23.3 172.16.30.7 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 minikube-m03 Ready <none> 17m v1.23.3 172.16.30.8 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 pradeep@learnk8s$ Verify the list of Pods in all namespaces. pradeep@learnk8s$ kubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system calico-kube-controllers-8594699699-dztlm 1/1 Running 0 6m50s 10.88.0.3 minikube <none> <none> kube-system calico-node-gqvw6 1/1 Running 1 (2m54s ago) 4m38s 172.16.30.7 minikube-m02 <none> <none> kube-system calico-node-qdbcf 1/1 Running 0 6m50s 172.16.30.6 minikube <none> <none> kube-system calico-node-sw74l 1/1 Running 0 114s 172.16.30.8 minikube-m03 <none> <none> kube-system coredns-64897985d-58btq 1/1 Running 0 6m50s 10.88.0.2 minikube <none> <none> kube-system etcd-minikube 1/1 Running 0 7m1s 172.16.30.6 minikube <none> <none> kube-system kube-apiserver-minikube 1/1 Running 0 7m1s 172.16.30.6 minikube <none> <none> kube-system kube-controller-manager-minikube 1/1 Running 0 7m1s 172.16.30.6 minikube <none> <none> kube-system kube-proxy-7k4lb 1/1 Running 0 114s 172.16.30.8 minikube-m03 <none> <none> kube-system kube-proxy-gm2dh 1/1 Running 0 4m38s 172.16.30.7 minikube-m02 <none> <none> kube-system kube-proxy-hvkqd 1/1 Running 0 6m50s 172.16.30.6 minikube <none> <none> kube-system kube-scheduler-minikube 1/1 Running 0 7m1s 172.16.30.6 minikube <none> <none> kube-system storage-provisioner 1/1 Running 1 (2m26s ago) 6m47s 172.16.30.6 minikube <none> <none> pradeep@learnk8s$ There are two Pods using the 10.88.0.X subnet ( calico-kube-controllers and coredns Pods ). Login to the controlplane node and check the CNI directory. pradeep@learnk8s$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ cat /etc/cni/net.d/ .keep 10-calico.conflist 87-podman-bridge.conflist calico-kubeconfig $ cat /etc/cni/net.d/10-calico.conflist { \"name\": \"k8s-pod-network\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"calico\", \"log_level\": \"info\", \"log_file_path\": \"/var/log/calico/cni/cni.log\", \"datastore_type\": \"kubernetes\", \"nodename\": \"minikube\", \"mtu\": 0, \"ipam\": { \"type\": \"calico-ipam\" }, \"policy\": { \"type\": \"k8s\" }, \"kubernetes\": { \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\" } }, { \"type\": \"portmap\", \"snat\": true, \"capabilities\": {\"portMappings\": true} }, { \"type\": \"bandwidth\", \"capabilities\": {\"bandwidth\": true} } ] }$ Apart from the Calico , there is a config file for Podman as well. Let us check that as well. $ cat /etc/cni/net.d/87-podman-bridge.conflist { \"cniVersion\": \"0.4.0\", \"name\": \"podman\", \"plugins\": [ { \"type\": \"bridge\", \"bridge\": \"cni-podman0\", \"isGateway\": true, \"ipMasq\": true, \"hairpinMode\": true, \"ipam\": { \"type\": \"host-local\", \"routes\": [{ \"dst\": \"0.0.0.0/0\" }], \"ranges\": [ [ { \"subnet\": \"10.88.0.0/16\", \"gateway\": \"10.88.0.1\" } ] ] } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } }, { \"type\": \"firewall\" }, { \"type\": \"tuning\" } ] } $ From this, it is clear that while starting the minikube cluster, those two Pods got the IP from the Podman CNI, which seems to compete with the Calico CNI. This issue is reported here: https://github.com/kubernetes/kubernetes/issues/107687 Let us create a new Pod in the default namespace and check which IP will be obtained. pradeep@learnk8s$ kubectl run nginx --image=nginx pod/nginx created Verify the IP of the new Pod and the node on which it got hosted. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 56s 10.244.205.193 minikube-m02 <none> <none> The new nginx pod got the IP address of 10.244.205.193 and is running on the minikube-m02 node. Let us log in to this minikube-m02 node and verify the routing table and list of interfaces. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 22:a7:1a:d9:be:42 brd ff:ff:ff:ff:ff:ff inet 172.16.30.7/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 84337sec preferred_lft 84337sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:d3:aa:60:ec brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 inet 10.244.205.192/32 scope global tunl0 valid_lft forever preferred_lft forever 8: calic440f455693@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0 $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.7 metric 1024 10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink 10.244.151.0/26 via 172.16.30.8 dev tunl0 proto bird onlink blackhole 10.244.205.192/26 proto bird 10.244.205.193 dev calic440f455693 scope link 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.7 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.7 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ There are two special interfaces, 5: tunl0@NONE: and 8: calic440f455693@if5: . We can see that it is an ipip tunnel and tunnel interface ( tunl0 ) has the IP address of 10.244.205.192/32 . From the routing table, there are two /26 routes pointing to the tunl0 interface. The 10.244.120.64/26 via 172.16.30.6 and 10.244.151.0/26 via 172.16.30.8 entries in the routing table, shows the Calico assigned subnets on the other two nodes, and the routes for those remote subnets via the IPIP tunnel interface. From within the minikube-m02 node, issue the docker ps command the retrieve the container ID of the nginx container. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0d1f5d390956 nginx \"/docker-entrypoint.\u2026\" 28 minutes ago Up 28 minutes k8s_nginx_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0 6b67d9586b86 k8s.gcr.io/pause:3.6 \"/pause\" 28 minutes ago Up 28 minutes k8s_POD_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0 6f65ec9ee321 5ef66b403f4f \"start_runit\" 39 minutes ago Up 39 minutes k8s_calico-node_calico-node-gqvw6_kube-system_6a835837-d36e-4366-aefe-cedc09d2f148_1 3d0be715b116 9b7cc9982109 \"/usr/local/bin/kube\u2026\" 41 minutes ago Up 41 minutes k8s_kube-proxy_kube-proxy-gm2dh_kube-system_770d601e-7b93-42fa-8019-5976ae95684e_0 9c077d1f8374 k8s.gcr.io/pause:3.6 \"/pause\" 41 minutes ago Up 41 minutes k8s_POD_calico-node-gqvw6_kube-system_6a835837-d36e-4366-aefe-cedc09d2f148_0 4b2360ad79a9 k8s.gcr.io/pause:3.6 \"/pause\" 41 minutes ago Up 41 minutes k8s_POD_kube-proxy-gm2dh_kube-system_770d601e-7b93-42fa-8019-5976ae95684e_0 $ Get the Pid of the nginx container with the docker inspect command. $ docker inspect 0d1f5d390956 | grep Pid \"Pid\": 11380, \"PidMode\": \"\", \"PidsLimit\": null, $ With the nsenter command, verify the list of interfaces inside the nginx container. $ sudo nsenter -t 11380 -n ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 3: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 5: eth0@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default link/ether e6:ea:af:be:ec:53 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.205.193/32 brd 10.244.205.193 scope global eth0 valid_lft forever preferred_lft forever $ We can see that the Pod IP is configured on the 5: eth0@if8: interface. From the @if8 , we can see the link to the 8: calic440f455693@if5: interface on the host (like the veth interfaces in the case of Kindnet CNI). Look at the routing table of the nginx pod. $ sudo nsenter -t 11380 -n ip route default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link $ Exit the minikube-m02 node and use the kubectl describe nodes command and filter for the PodCIDRs. pradeep@learnk8s$ kubectl describe nodes | grep -e Name -e PodCIDR Name: minikube PodCIDR: 10.244.0.0/24 PodCIDRs: 10.244.0.0/24 Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age Name: minikube-m02 PodCIDR: 10.244.1.0/24 PodCIDRs: 10.244.1.0/24 Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age Name: minikube-m03 PodCIDR: 10.244.2.0/24 PodCIDRs: 10.244.2.0/24 Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age pradeep@learnk8s$ Currently, there is only one pod ( nginx ) in the cluster. Similar to what we have verified on the minikube-m02 node, do the same on the other two nodes as well. From the minikube node: pradeep@learnk8s$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 9e:55:51:7f:e1:98 brd ff:ff:ff:ff:ff:ff inet 172.16.30.6/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 82845sec preferred_lft 82845sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:92:37:51:98 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: cni-podman0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 36:85:e8:76:d2:1c brd ff:ff:ff:ff:ff:ff inet 10.88.0.1/16 brd 10.88.255.255 scope global cni-podman0 valid_lft forever preferred_lft forever 6: veth5a561323@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master cni-podman0 state UP group default link/ether f6:67:49:87:51:36 brd ff:ff:ff:ff:ff:ff link-netnsid 0 7: vethb6ed82ba@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master cni-podman0 state UP group default link/ether 22:b5:fe:8a:37:59 brd ff:ff:ff:ff:ff:ff link-netnsid 1 8: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 inet 10.244.120.64/32 scope global tunl0 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.6 metric 1024 10.88.0.0/16 dev cni-podman0 proto kernel scope link src 10.88.0.1 blackhole 10.244.120.64/26 proto bird 10.244.151.0/26 via 172.16.30.8 dev tunl0 proto bird onlink 10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.6 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.6 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ exit logout pradeep@learnk8s$ On the controlplane node, there are extra interfaces coming from the Podman CNI . From the minikube-m03 node: pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether de:b8:1d:5e:d9:c0 brd ff:ff:ff:ff:ff:ff inet 172.16.30.8/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 83040sec preferred_lft 83040sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:66:0b:71:73 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 inet 10.244.151.0/32 scope global tunl0 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.8 metric 1024 10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink blackhole 10.244.151.0/26 proto bird 10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.8 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.8 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ On other nodes also, we do see similar tunl0 interface and routes for Calico subnets (/26s) via the IPIP tunnel. For the local subnet, there is a blackhole route: for example on the minikube-m03 node blackhole 10.244.151.0/26 proto bird , and on minikube-m02 node blackhole 10.244.205.192/26 proto bird , on the minikube node blackhole 10.244.120.64/26 proto bird . Let us manually schedule a Pod on the minikube-m03 node. pradeep@learnk8s$ cat my-pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx-manual spec: nodeName: minikube-m03 containers: - image: nginx name: nginx pradeep@learnk8s$ kubectl create -f my-pod.yaml pod/nginx-manual created pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 53m 10.244.205.193 minikube-m02 <none> <none> nginx-manual 1/1 Running 0 39s 10.244.151.1 minikube-m03 <none> <none> pradeep@learnk8s$ As expected, the nginx-manual pod got an IP from the 10.244.151.0/26 subnet. Log back to the minikube-m03 node and check the routing table and list of interfaces. pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether de:b8:1d:5e:d9:c0 brd ff:ff:ff:ff:ff:ff inet 172.16.30.8/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 82414sec preferred_lft 82414sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:66:0b:71:73 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 inet 10.244.151.0/32 scope global tunl0 valid_lft forever preferred_lft forever 8: califba6dd09590@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0 $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.8 metric 1024 10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink blackhole 10.244.151.0/26 proto bird 10.244.151.1 dev califba6dd09590 scope link 10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.8 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.8 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ Compared to the previous output (when there were no pods on this node), there is a new interface 8: califba6dd09590@if5: and new route entry for the Pod IP 10.244.151.1 dev califba6dd09590 scope link . Get the container ID and Pid of this nginx-manual container. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 244b50fad9d4 nginx \"/docker-entrypoint.\u2026\" 5 minutes ago Up 5 minutes k8s_nginx_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0 e7117e519923 k8s.gcr.io/pause:3.6 \"/pause\" 5 minutes ago Up 5 minutes k8s_POD_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0 c4c993bc1f2e calico/node \"start_runit\" About an hour ago Up About an hour k8s_calico-node_calico-node-sw74l_kube-system_0e30bbad-8370-4907-ab20-ce81450ad13c_0 a3482f106e3a 9b7cc9982109 \"/usr/local/bin/kube\u2026\" About an hour ago Up About an hour k8s_kube-proxy_kube-proxy-7k4lb_kube-system_f284e305-e71b-40b0-a715-796d0733bc03_0 25a914d158da k8s.gcr.io/pause:3.6 \"/pause\" About an hour ago Up About an hour k8s_POD_calico-node-sw74l_kube-system_0e30bbad-8370-4907-ab20-ce81450ad13c_0 abce35d2a0ae k8s.gcr.io/pause:3.6 \"/pause\" About an hour ago Up About an hour k8s_POD_kube-proxy-7k4lb_kube-system_f284e305-e71b-40b0-a715-796d0733bc03_0 $ $ docker inspect 244b50fad9d4 | grep Pid \"Pid\": 42125, \"PidMode\": \"\", \"PidsLimit\": null, $ Use the nsenter command and verify the Pod interfaces and route table. $ nsenter -t 42125 -n ip a nsenter: cannot open /proc/42125/ns/net: Permission denied $ sudo nsenter -t 42125 -n ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 3: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 5: eth0@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default link/ether 9a:93:7b:77:8f:c7 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.151.1/32 brd 10.244.151.1 scope global eth0 valid_lft forever preferred_lft forever $ The eth0 interface ( 5: eth0@if8 ) of the Pod has the IP address 10.244.151.1/32 and is mapped to the new cali interface 8: califba6dd09590@if5: on the host. $ sudo nsenter -t 42125 -n ip route default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link $ Verify communication between the two Pods. 10.244.205.193 is the IP address of the nginx pod running on the other node minikube-m02 . $ sudo nsenter -t 42125 -n ping 10.244.205.193 PING 10.244.205.193 (10.244.205.193): 56 data bytes 64 bytes from 10.244.205.193: seq=0 ttl=62 time=17.321 ms 64 bytes from 10.244.205.193: seq=1 ttl=62 time=0.963 ms 64 bytes from 10.244.205.193: seq=2 ttl=62 time=0.891 ms 64 bytes from 10.244.205.193: seq=3 ttl=62 time=1.529 ms ^C --- 10.244.205.193 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.891/5.176/17.321 ms $ Verify communication to the outside cluster hosts, like any internet host ( for example 8.8.8.8) $ sudo nsenter -t 42125 -n ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes 64 bytes from 8.8.8.8: seq=0 ttl=115 time=20.260 ms 64 bytes from 8.8.8.8: seq=1 ttl=115 time=15.457 ms ^C --- 8.8.8.8 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 15.457/17.858/20.260 ms $ Let us install calicoctl as a Pod itself. pradeep@learnk8s$ kubectl apply -f https://projectcalico.docs.tigera.io/manifests/calicoctl.yaml serviceaccount/calicoctl created pod/calicoctl created clusterrole.rbac.authorization.k8s.io/calicoctl created clusterrolebinding.rbac.authorization.k8s.io/calicoctl created pradeep@learnk8s$ Verify that the calicoctl pod is running. pradeep@learnk8s$ kubectl get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES calico-kube-controllers-8594699699-dztlm 1/1 Running 0 82m 10.88.0.3 minikube <none> <none> calico-node-gqvw6 1/1 Running 1 (78m ago) 80m 172.16.30.7 minikube-m02 <none> <none> calico-node-qdbcf 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> calico-node-sw74l 1/1 Running 0 77m 172.16.30.8 minikube-m03 <none> <none> calicoctl 1/1 Running 0 18s 172.16.30.7 minikube-m02 <none> <none> coredns-64897985d-58btq 1/1 Running 0 82m 10.88.0.2 minikube <none> <none> etcd-minikube 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> kube-apiserver-minikube 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> kube-controller-manager-minikube 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> kube-proxy-7k4lb 1/1 Running 0 77m 172.16.30.8 minikube-m03 <none> <none> kube-proxy-gm2dh 1/1 Running 0 80m 172.16.30.7 minikube-m02 <none> <none> kube-proxy-hvkqd 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> kube-scheduler-minikube 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> storage-provisioner 1/1 Running 1 (77m ago) 82m 172.16.30.6 minikube <none> <none> pradeep@learnk8s$ Verify the calicoctl pradeep@learnk8s$ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get nodes -o wide Failed to get resources: Version mismatch. Client Version: v3.22.1 Cluster Version: v3.20.0 Use --allow-version-mismatch to override. command terminated with exit code 1 pradeep@learnk8s$ Repeat it with --allow-version-mismatch . pradeep@learnk8s$ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get nodes -o wide --allow-version-mismatch NAME ASN IPV4 IPV6 minikube (64512) 172.16.30.6/24 minikube-m02 (64512) 172.16.30.7/24 minikube-m03 (64512) 172.16.30.8/24 pradeep@learnk8s$ We can confirm that the calicoctl is working and we can see some BGP Autonomous systems shown as well. Currently all three nodes are part of the same ASN ( 64512 ). Create an alias for the calicoctl pradeep@learnk8s$ alias calicoctl=\"kubectl exec -i -n kube-system calicoctl -- /calicoctl --allow-version-mismatch\" By default, calicoctl will attempt to read from the Kubernetes API using the default kubeconfig located at $(HOME)/.kube/config. If the default kubeconfig does not exist, or you would like to specify alternative API access information, you can do so using the following configuration options. pradeep@learnk8s$ DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes NAME minikube minikube-m02 minikube-m03 pradeep@learnk8s$ Verify all API-Resources installed by Calico CNI plugin. pradeep@learnk8s$ kubectl api-resources | grep calico bgpconfigurations crd.projectcalico.org/v1 false BGPConfiguration bgppeers crd.projectcalico.org/v1 false BGPPeer blockaffinities crd.projectcalico.org/v1 false BlockAffinity clusterinformations crd.projectcalico.org/v1 false ClusterInformation felixconfigurations crd.projectcalico.org/v1 false FelixConfiguration globalnetworkpolicies crd.projectcalico.org/v1 false GlobalNetworkPolicy globalnetworksets crd.projectcalico.org/v1 false GlobalNetworkSet hostendpoints crd.projectcalico.org/v1 false HostEndpoint ipamblocks crd.projectcalico.org/v1 false IPAMBlock ipamconfigs crd.projectcalico.org/v1 false IPAMConfig ipamhandles crd.projectcalico.org/v1 false IPAMHandle ippools crd.projectcalico.org/v1 false IPPool kubecontrollersconfigurations crd.projectcalico.org/v1 false KubeControllersConfiguration networkpolicies crd.projectcalico.org/v1 true NetworkPolicy networksets crd.projectcalico.org/v1 true NetworkSet pradeep@learnk8s$ Verify some of these resources with kubectl get and kubectl describe commands. pradeep@learnk8s$ kubectl get bgppeers -A No resources found pradeep@learnk8s$ kubectl get ippools NAME AGE default-ipv4-ippool 97m pradeep@learnk8s$ kubectl describe ippools Name: default-ipv4-ippool Namespace: Labels: <none> Annotations: projectcalico.org/metadata: {\"uid\":\"cf94cf43-8887-4528-a934-ca498f0422e1\",\"creationTimestamp\":\"2022-03-19T18:20:36Z\"} API Version: crd.projectcalico.org/v1 Kind: IPPool Metadata: Creation Timestamp: 2022-03-19T18:20:36Z Generation: 1 Managed Fields: API Version: crd.projectcalico.org/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:projectcalico.org/metadata: f:spec: .: f:blockSize: f:cidr: f:ipipMode: f:natOutgoing: f:nodeSelector: f:vxlanMode: Manager: Go-http-client Operation: Update Time: 2022-03-19T18:20:36Z Resource Version: 659 UID: 8d9e076a-dae6-4bbd-8f44-920da83472ba Spec: Block Size: 26 Cidr: 10.244.0.0/16 Ipip Mode: Always Nat Outgoing: true Node Selector: all() Vxlan Mode: Never Events: <none> pradeep@learnk8s$ pradeep@learnk8s$ kubectl get ipamblocks NAME AGE 10-244-120-64-26 98m 10-244-151-0-26 93m 10-244-205-192-26 95m pradeep@learnk8s$ We have seen these IPAM blocks already, when we verified the routing tables on each node. pradeep@learnk8s$ kubectl get networkpolicies No resources found in default namespace. Let us create a network policy now. pradeep@learnk8s$ cat network-policy.yaml --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: - Ingress pradeep@learnk8s$ pradeep@learnk8s$ kubectl create -f network-policy.yaml networkpolicy.networking.k8s.io/default-deny-ingress created Verify that the network policy is created. pradeep@learnk8s$ kubectl get netpol NAME POD-SELECTOR AGE default-deny-ingress <none> 2s Describe it for more details. pradeep@learnk8s$ kubectl describe netpol Name: default-deny-ingress Namespace: default Created on: 2022-03-20 01:45:29 +0530 IST Labels: <none> Annotations: <none> Spec: PodSelector: <none> (Allowing the specific traffic to all pods in this namespace) Allowing ingress traffic: <none> (Selected pods are isolated for ingress connectivity) Not affecting egress traffic Policy Types: Ingress pradeep@learnk8s$ cat network-policy.yaml --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: - Ingress pradeep@learnk8s$ If you recall, we have two pods in our cluster at the moment. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 105m 10.244.205.193 minikube-m02 <none> <none> nginx-manual 1/1 Running 0 52m 10.244.151.1 minikube-m03 <none> <none> Now that the network policy is applied, let us check the connectivity between these two pods again. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ docker ps | grep nginx 0d1f5d390956 nginx \"/docker-entrypoint.\u2026\" 2 hours ago Up 2 hours k8s_nginx_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0 6b67d9586b86 k8s.gcr.io/pause:3.6 \"/pause\" 2 hours ago Up 2 hours k8s_POD_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0 $ docker inspect 0d1f5d390956 | grep Pid \"Pid\": 11380, \"PidMode\": \"\", \"PidsLimit\": null, $ sudo nsenter -t 11380 -n ping 10.244.151.1 PING 10.244.151.1 (10.244.151.1): 56 data bytes ^C --- 10.244.151.1 ping statistics --- 3 packets transmitted, 0 packets received, 100% packet loss $ From nginx pod, we are not able to ping to nginx-manual pod. Similarly, verify the other way, ping from nginx-manual to nginx Pod (which was working earlier, that we tested already!). pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ docker ps | grep nginx 244b50fad9d4 nginx \"/docker-entrypoint.\u2026\" 55 minutes ago Up 55 minutes k8s_nginx_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0 e7117e519923 k8s.gcr.io/pause:3.6 \"/pause\" 55 minutes ago Up 55 minutes k8s_POD_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0 $ docker inspect 244b50fad9d4 | grep Pid \"Pid\": 42125, \"PidMode\": \"\", \"PidsLimit\": null, $ sudo nsenter -t 42125 -n ping 10.244.205.193 PING 10.244.205.193 (10.244.205.193): 56 data bytes ^C --- 10.244.205.193 ping statistics --- 2 packets transmitted, 0 packets received, 100% packet loss $ exit 0 logout pradeep@learnk8s$ With network policy applied, the communication is blocked. Let us create another network policy, this time to allow all ingress. pradeep@learnk8s$ cat allow-ingress.yaml --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-ingress spec: podSelector: {} ingress: - {} policyTypes: - Ingress pradeep@learnk8s$ kubectl create -f allow-ingress.yaml networkpolicy.networking.k8s.io/allow-all-ingress created pradeep@learnk8s$ kubectl get netpol NAME POD-SELECTOR AGE allow-all-ingress <none> 5s default-deny-ingress <none> 11m Describe the new policy. pradeep@learnk8s$ kubectl describe netpol allow-all-ingress Name: allow-all-ingress Namespace: default Created on: 2022-03-20 01:56:32 +0530 IST Labels: <none> Annotations: <none> Spec: PodSelector: <none> (Allowing the specific traffic to all pods in this namespace) Allowing ingress traffic: To Port: <any> (traffic allowed to all ports) From: <any> (traffic not restricted by source) Not affecting egress traffic Policy Types: Ingress pradeep@learnk8s$ Now that we have allowed the ingress traffic to all pods/all ports. Verify again. Now that we know the Pid of the containers, those steps are not needed. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ sudo nsenter -t 11380 -n ping 10.244.151.1 PING 10.244.151.1 (10.244.151.1): 56 data bytes 64 bytes from 10.244.151.1: seq=0 ttl=62 time=3.834 ms 64 bytes from 10.244.151.1: seq=1 ttl=62 time=1.409 ms 64 bytes from 10.244.151.1: seq=2 ttl=62 time=0.723 ms ^C --- 10.244.151.1 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.723/1.988/3.834 ms $ exit 0 logout Yay! Ping is working again. Similarly, verify in the other direction. pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ sudo nsenter -t 42125 -n ping 10.244.205.193 PING 10.244.205.193 (10.244.205.193): 56 data bytes 64 bytes from 10.244.205.193: seq=0 ttl=62 time=1.132 ms 64 bytes from 10.244.205.193: seq=1 ttl=62 time=1.672 ms 64 bytes from 10.244.205.193: seq=2 ttl=62 time=1.695 ms ^C --- 10.244.205.193 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 1.132/1.499/1.695 ms $ exit 0 logout This completes our initial discussion on the Calico CNI. We just scratched the surface of it, there are many more features, which will be discussed in other posts. Kubernetes Ingress Controller Enable Ingress Addon pradeep@learnk8s$ minikube addons list |-----------------------------|----------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|----------|--------------|--------------------------------| | ambassador | minikube | disabled | third-party (ambassador) | | auto-pause | minikube | disabled | google | | csi-hostpath-driver | minikube | disabled | kubernetes | | dashboard | minikube | disabled | kubernetes | | default-storageclass | minikube | enabled \u2705 | kubernetes | | efk | minikube | disabled | third-party (elastic) | | freshpod | minikube | disabled | google | | gcp-auth | minikube | disabled | google | | gvisor | minikube | disabled | google | | helm-tiller | minikube | disabled | third-party (helm) | | ingress | minikube | disabled | unknown (third-party) | | ingress-dns | minikube | disabled | google | | istio | minikube | disabled | third-party (istio) | | istio-provisioner | minikube | disabled | third-party (istio) | | kong | minikube | disabled | third-party (Kong HQ) | | kubevirt | minikube | disabled | third-party (kubevirt) | | logviewer | minikube | disabled | unknown (third-party) | | metallb | minikube | disabled | third-party (metallb) | | metrics-server | minikube | disabled | kubernetes | | nvidia-driver-installer | minikube | disabled | google | | nvidia-gpu-device-plugin | minikube | disabled | third-party (nvidia) | | olm | minikube | disabled | third-party (operator | | | | | framework) | | pod-security-policy | minikube | disabled | unknown (third-party) | | portainer | minikube | disabled | portainer.io | | registry | minikube | disabled | google | | registry-aliases | minikube | disabled | unknown (third-party) | | registry-creds | minikube | disabled | third-party (upmc enterprises) | | storage-provisioner | minikube | enabled \u2705 | google | | storage-provisioner-gluster | minikube | disabled | unknown (third-party) | | volumesnapshots | minikube | disabled | kubernetes | |-----------------------------|----------|--------------|--------------------------------| pradeep@learnk8s$ Enable the ingress addon which is currently disabled in this environment. pradeep@learnk8s$ minikube addons enable ingress \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 \u25aa Using image k8s.gcr.io/ingress-nginx/controller:v1.1.1 \ud83d\udd0e Verifying ingress addon... \ud83c\udf1f The 'ingress' addon is enabled pradeep@learnk8s$ Verify the addons list again, and confirm that the ingress is enabled. pradeep@learnk8s$ minikube addons list |-----------------------------|----------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|----------|--------------|--------------------------------| | ambassador | minikube | disabled | third-party (ambassador) | | auto-pause | minikube | disabled | google | | csi-hostpath-driver | minikube | disabled | kubernetes | | dashboard | minikube | disabled | kubernetes | | default-storageclass | minikube | enabled \u2705 | kubernetes | | efk | minikube | disabled | third-party (elastic) | | freshpod | minikube | disabled | google | | gcp-auth | minikube | disabled | google | | gvisor | minikube | disabled | google | | helm-tiller | minikube | disabled | third-party (helm) | | ingress | minikube | enabled \u2705 | unknown (third-party) | | ingress-dns | minikube | disabled | google | | istio | minikube | disabled | third-party (istio) | | istio-provisioner | minikube | disabled | third-party (istio) | | kong | minikube | disabled | third-party (Kong HQ) | | kubevirt | minikube | disabled | third-party (kubevirt) | | logviewer | minikube | disabled | unknown (third-party) | | metallb | minikube | disabled | third-party (metallb) | | metrics-server | minikube | disabled | kubernetes | | nvidia-driver-installer | minikube | disabled | google | | nvidia-gpu-device-plugin | minikube | disabled | third-party (nvidia) | | olm | minikube | disabled | third-party (operator | | | | | framework) | | pod-security-policy | minikube | disabled | unknown (third-party) | | portainer | minikube | disabled | portainer.io | | registry | minikube | disabled | google | | registry-aliases | minikube | disabled | unknown (third-party) | | registry-creds | minikube | disabled | third-party (upmc enterprises) | | storage-provisioner | minikube | enabled \u2705 | google | | storage-provisioner-gluster | minikube | disabled | unknown (third-party) | | volumesnapshots | minikube | disabled | kubernetes | |-----------------------------|----------|--------------|--------------------------------| pradeep@learnk8s$ Verify all resources that got created as part of this addon installation. pradeep@learnk8s$ kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-cvwv7 0/1 Completed 0 65s ingress-nginx-admission-patch-zsrq4 0/1 Completed 1 65s ingress-nginx-controller-cc8496874-6zr4h 1/1 Running 0 65s pradeep@learnk8s$ kubectl get all -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-admission-create-cvwv7 0/1 Completed 0 83s pod/ingress-nginx-admission-patch-zsrq4 0/1 Completed 1 83s pod/ingress-nginx-controller-cc8496874-6zr4h 1/1 Running 0 83s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller NodePort 10.101.85.4 <none> 80:30419/TCP,443:30336/TCP 83s service/ingress-nginx-controller-admission ClusterIP 10.104.163.136 <none> 443/TCP 83s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ingress-nginx-controller 1/1 1 1 83s NAME DESIRED CURRENT READY AGE replicaset.apps/ingress-nginx-controller-cc8496874 1 1 1 83s NAME COMPLETIONS DURATION AGE job.batch/ingress-nginx-admission-create 1/1 10s 83s job.batch/ingress-nginx-admission-patch 1/1 11s 83s pradeep@learnk8s$ Deploy a Hello, World App! pradeep@learnk8s$ kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0 deployment.apps/web created pradeep@learnk8s$ Expose the deployment as a NodePort Service on port number 8080. pradeep@learnk8s$ kubectl expose deployment web --type=NodePort - -port=8080 service/web exposed pradeep@learnk8s$ Verify that the service is created properly. pradeep@learnk8s$ kubectl get service web NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web NodePort 10.105.47.136 <none> 8080:31270/TCP 11s pradeep@learnk8s$ Get the URL for this newly created service. pradeep@learnk8s$ minikube service web --url http://172.16.30.6:31270 pradeep@learnk8s$ Access the URL pradeep@learnk8s$ curl http://172.16.30.6:31270 Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw pradeep@learnk8s$ Create an Ingress pradeep@learnk8s$ cat example-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: - host: hello-world.info http: paths: - path: / pathType: Prefix backend: service: name: web port: number: 8080 pradeep@learnk8s$ Create the Ingress using the above definition. pradeep@learnk8s$kubectl apply -f example-ingress.yaml ingress.networking.k8s.io/example-ingress created pradeep@learnk8s$ Verify that the Ingress is created. pradeep@learnk8s$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE example-ingress nginx hello-world.info 172.16.30.6 80 87s pradeep@learnk8s$ Add the 172.16.30.6 hello-world.info to the bottom of the /etc/hosts file on your computer (you will need administrator access): pradeep@learnk8s$ cat /etc/hosts ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127.0.0.1 localhost 255.255.255.255 broadcasthost ::1 localhost # Added by Docker Desktop # To allow the same kube context to work on the host and the container: 127.0.0.1 kubernetes.docker.internal # End of section 172.16.30.6 hello-world.info pradeep@learnk8s$ pradeep@learnk8s$ curl hello-world.info Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw pradeep@learnk8s$ Create a second Deployment pradeep@learnk8s$ kubectl create deployment web2 --image=gcr.io/google-samples/hello-app:2.0 deployment.apps/web2 created pradeep@learnk8s$ Expose the deployment. pradeep@learnk8s$ kubectl expose deployment web2 --port=8080 --type=NodePort service/web2 exposed pradeep@learnk8s$ Edit the existing example-ingress.yaml manifest, and add the following lines at the end: pradeep@learnk8s$ cat example-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: - host: hello-world.info http: paths: - path: / pathType: Prefix backend: service: name: web port: number: 8080 - path: /v2 pathType: Prefix backend: service: name: web2 port: number: 8080 pradeep@learnk8s$ Apply the changes. pradeep@learnk8s$ kubectl apply -f example-ingress.yaml ingress.networking.k8s.io/example-ingress configured pradeep@learnk8s$ Test your Ingress pradeep@learnk8s$ curl hello-world.info Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw pradeep@learnk8s$ Access the 2nd version of the Hello World app. pradeep@learnk8s$ curl hello-world.info/v2 Hello, world! Version: 2.0.0 Hostname: web2-5858b4c7c5-krt4l pradeep@learnk8s$ pradeep@learnk8s$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE example-ingress nginx hello-world.info 172.16.30.6 80 11m pradeep@learnk8s$ Let us describe the ingress to see how it looks like. pradeep@learnk8s$ kubectl describe ingress Name: example-ingress Namespace: default Address: 172.16.30.6 Default backend: default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>) Rules: Host Path Backends ---- ---- -------- hello-world.info / web:8080 (10.244.205.195:8080) /v2 web2:8080 (10.244.151.3:8080) Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 12m (x3 over 20m) nginx-ingress-controller Scheduled for sync pradeep@learnk8s$ We can see one host ( hello-world.info ) and two paths ( / and /v2 ), two backends ( web:8080 and web2:8080 ). These two services point to the respective Pods. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES web-746c8679d4-nfkcw 1/1 Running 0 75m 10.244.205.195 minikube-m02 <none> <none> web2-5858b4c7c5-krt4l 1/1 Running 0 66m 10.244.151.3 minikube-m03 <none> <none> pradeep@learnk8s$ What happens to the requests sent to a path different from both of these ( / and /v2 )? Those requests are handled by the default backend. From the above description, we can see there is a default backend default-http-backend:80 which is not defined yet, so there is an error. Let us test few undefined paths like ( /v3 , /v5 etc) pradeep@learnk8s$ curl hello-world.info/v3 Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw pradeep@learnk8s$ curl hello-world.info/v5 Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw It looks like all these undefined paths are handled by the web:8080 backend only. According to Kubernetes documentation, If defaultBackend is not set, the handling of requests that do not match any of the rules will be up to the ingress controller (consult the documentation for your ingress controller to find out how it handles this case). If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend. Let us edit the ingress, to add a defaultBackend, in this case , change it to web2 . pradeep@learnk8s$ kubectl edit ingress example-ingress ingress.networking.k8s.io/example-ingress edited Describe it again, to see the changes. pradeep@learnk8s$ kubectl describe ingress Name: example-ingress Namespace: default Address: 172.16.30.6 Default backend: web2:8080 (10.244.151.3:8080) Rules: Host Path Backends ---- ---- -------- hello-world.info / web:8080 (10.244.205.195:8080) /v2 web2:8080 (10.244.151.3:8080) Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 11s (x4 over 78m) nginx-ingress-controller Scheduled for sync pradeep@learnk8s$ This line Default backend: web2:8080 (10.244.151.3:8080) confirms our change. List out all the endpoints. pradeep@learnk8s$ kubectl get ep NAME ENDPOINTS AGE kubernetes 172.16.30.6:8443 47h web 10.244.205.195:8080 87m web2 10.244.151.3:8080 78m pradeep@learnk8s$ Let us look at the ingress controller logs. pradeep@learnk8s$ kubectl -n ingress-nginx logs ingress-nginx-controller-cc8496874-6zr4h ------------------------------------------------------------------------------- NGINX Ingress controller Release: v1.1.1 Build: a17181e43ec85534a6fea968d95d019c5a4bc8cf Repository: https://github.com/kubernetes/ingress-nginx nginx version: nginx/1.19.9 ------------------------------------------------------------------------------- W0321 15:42:29.028539 9 client_config.go:615] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. I0321 15:42:29.028983 9 main.go:223] \"Creating API client\" host=\"https://10.96.0.1:443\" I0321 15:42:29.057278 9 main.go:267] \"Running in Kubernetes cluster\" major=\"1\" minor=\"23\" git=\"v1.23.3\" state=\"clean\" commit=\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\" platform=\"linux/amd64\" I0321 15:42:29.317187 9 main.go:104] \"SSL fake certificate created\" file=\"/etc/ingress-controller/ssl/default-fake-certificate.pem\" I0321 15:42:29.374201 9 ssl.go:531] \"loading tls certificate\" path=\"/usr/local/certificates/cert\" key=\"/usr/local/certificates/key\" I0321 15:42:29.419869 9 nginx.go:255] \"Starting NGINX Ingress controller\" I0321 15:42:29.437178 9 event.go:282] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller\", UID:\"d14f8541-76a0-4357-b9ad-38c7ce8f6586\", APIVersion:\"v1\", ResourceVersion:\"12223\", FieldPath:\"\"}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller I0321 15:42:29.442938 9 event.go:282] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"tcp-services\", UID:\"9cffa0f1-6754-4bfd-bf78-6cae4febc393\", APIVersion:\"v1\", ResourceVersion:\"12224\", FieldPath:\"\"}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services I0321 15:42:29.443183 9 event.go:282] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"udp-services\", UID:\"3ac9b32a-17b9-4824-bc0b-51a609648e9c\", APIVersion:\"v1\", ResourceVersion:\"12225\", FieldPath:\"\"}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services I0321 15:42:30.622870 9 nginx.go:297] \"Starting NGINX process\" I0321 15:42:30.623750 9 leaderelection.go:248] attempting to acquire leader lease ingress-nginx/ingress-controller-leader... I0321 15:42:30.624864 9 controller.go:155] \"Configuration changes detected, backend reload required\" I0321 15:42:30.623967 9 nginx.go:317] \"Starting validation webhook\" address=\":8443\" certPath=\"/usr/local/certificates/cert\" keyPath=\"/usr/local/certificates/key\" I0321 15:42:30.666454 9 leaderelection.go:258] successfully acquired lease ingress-nginx/ingress-controller-leader I0321 15:42:30.667675 9 status.go:84] \"New leader elected\" identity=\"ingress-nginx-controller-cc8496874-6zr4h\" I0321 15:42:30.700891 9 status.go:214] \"POD is not ready\" pod=\"ingress-nginx/ingress-nginx-controller-cc8496874-6zr4h\" node=\"minikube\" I0321 15:42:30.884747 9 controller.go:172] \"Backend successfully reloaded\" I0321 15:42:30.885032 9 controller.go:183] \"Initial sync, sleeping for 1 second\" I0321 15:42:30.885722 9 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-cc8496874-6zr4h\", UID:\"de6796ef-94a6-499c-b9b5-418c39cc62c0\", APIVersion:\"v1\", ResourceVersion:\"12337\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration I0321 15:56:40.753346 9 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.412s renderingIngressLength:1 renderingIngressTime:0.009s admissionTime:18.0kBs testedConfigurationSize:0.421} I0321 15:56:40.755944 9 main.go:101] \"successfully validated configuration, accepting\" ingress=\"default/example-ingress\" I0321 15:56:40.794396 9 store.go:424] \"Found valid IngressClass\" ingress=\"default/example-ingress\" ingressclass=\"nginx\" I0321 15:56:40.796827 9 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"example-ingress\", UID:\"51471ff6-1982-47da-85ee-a9356165df9f\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"13112\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync I0321 15:56:40.802072 9 controller.go:155] \"Configuration changes detected, backend reload required\" I0321 15:56:40.979390 9 controller.go:172] \"Backend successfully reloaded\" I0321 15:56:40.988286 9 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-cc8496874-6zr4h\", UID:\"de6796ef-94a6-499c-b9b5-418c39cc62c0\", APIVersion:\"v1\", ResourceVersion:\"12337\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration I0321 15:57:28.297662 9 status.go:299] \"updating Ingress status\" namespace=\"default\" ingress=\"example-ingress\" currentValue=[] newValue=[{IP:172.16.30.6 Hostname: Ports:[]}] I0321 15:57:28.322904 9 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"example-ingress\", UID:\"51471ff6-1982-47da-85ee-a9356165df9f\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"13164\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync 172.16.30.1 - - [21/Mar/2022:16:00:09 +0000] \"GET / HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 80 0.009 [default-web-8080] [] 10.244.205.195:8080 60 0.009 200 766ea45be48fc8dcb53c425eb3800ab2 I0321 16:04:49.720933 9 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.161s renderingIngressLength:1 renderingIngressTime:0s admissionTime:21.7kBs testedConfigurationSize:0.161} I0321 16:04:49.721038 9 main.go:101] \"successfully validated configuration, accepting\" ingress=\"default/example-ingress\" I0321 16:04:49.733433 9 controller.go:155] \"Configuration changes detected, backend reload required\" I0321 16:04:49.734529 9 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"example-ingress\", UID:\"51471ff6-1982-47da-85ee-a9356165df9f\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"13649\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync I0321 16:04:49.911585 9 controller.go:172] \"Backend successfully reloaded\" I0321 16:04:49.914092 9 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-cc8496874-6zr4h\", UID:\"de6796ef-94a6-499c-b9b5-418c39cc62c0\", APIVersion:\"v1\", ResourceVersion:\"12337\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration 172.16.30.1 - - [21/Mar/2022:16:05:30 +0000] \"GET / HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 80 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 c3d48047b7ef7ac70326098b25f1e6c1 172.16.30.1 - - [21/Mar/2022:16:05:33 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.013 [default-web2-8080] [] 10.244.151.3:8080 61 0.013 200 e90cf46a8385a64ac01c834734768488 172.16.30.1 - - [21/Mar/2022:16:32:44 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.114 [default-web2-8080] [] 10.244.151.3:8080 61 0.114 200 cb87a3a8abd12099f7db64a2ec83c681 172.16.30.1 - - [21/Mar/2022:17:03:17 +0000] \"GET /v3 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.040 [default-web-8080] [] 10.244.205.195:8080 60 0.041 200 f794e2a783715ac73ff7177f60e9dd68 172.16.30.1 - - [21/Mar/2022:17:03:21 +0000] \"GET /v5 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.016 [default-web-8080] [] 10.244.205.195:8080 60 0.017 200 095cbb53ca04bdb4359fa7b745e5a809 172.16.30.1 - - [21/Mar/2022:17:03:29 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.002 [default-web2-8080] [] 10.244.151.3:8080 61 0.002 200 cbc0c654a89c5f98f18a5102ddf61b0e 172.16.30.1 - - [21/Mar/2022:17:03:33 +0000] \"GET /1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 81 0.008 [default-web-8080] [] 10.244.205.195:8080 60 0.008 200 767955b2b4eed7791912b2a843363b28 172.16.30.1 - - [21/Mar/2022:17:05:26 +0000] \"GET /1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 81 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 0ec584df780661bccfa10731bcc70524 172.16.30.1 - - [21/Mar/2022:17:05:29 +0000] \"GET / HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 80 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 5ecb91527d77741daf16c17a8d79a462 172.16.30.1 - - [21/Mar/2022:17:05:31 +0000] \"GET /as HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.006 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 034643529f3104fef19915593168b200 I0321 17:14:53.079003 9 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.41s renderingIngressLength:1 renderingIngressTime:0.007s admissionTime:21.7kBs testedConfigurationSize:0.417} I0321 17:14:53.079746 9 main.go:101] \"successfully validated configuration, accepting\" ingress=\"default/example-ingress\" I0321 17:14:53.099157 9 controller.go:155] \"Configuration changes detected, backend reload required\" I0321 17:14:53.099689 9 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"example-ingress\", UID:\"51471ff6-1982-47da-85ee-a9356165df9f\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"15050\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync I0321 17:14:53.225350 9 controller.go:172] \"Backend successfully reloaded\" I0321 17:14:53.227604 9 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-cc8496874-6zr4h\", UID:\"de6796ef-94a6-499c-b9b5-418c39cc62c0\", APIVersion:\"v1\", ResourceVersion:\"12337\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration 172.16.30.1 - - [21/Mar/2022:17:16:53 +0000] \"GET /v5 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.009 [default-web-8080] [] 10.244.205.195:8080 60 0.009 200 2e8ee25ad012ea3d5ac6fb4646f37d61 172.16.30.1 - - [21/Mar/2022:17:16:57 +0000] \"GET /v3 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 70d9d27891c2b97a6bcda5e315fa3691 172.16.30.1 - - [21/Mar/2022:17:17:01 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.010 [default-web2-8080] [] 10.244.151.3:8080 61 0.009 200 297effa7825bef53d93e6dce4e5d9d62 172.16.30.1 - - [21/Mar/2022:17:17:02 +0000] \"GET /v1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 421c9cb655e07183d9a7d1a89562db97 172.16.30.1 - - [21/Mar/2022:17:17:29 +0000] \"GET /v1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 3bff305b8c1b782d08acf2de386076d9 172.16.30.1 - - [21/Mar/2022:17:17:31 +0000] \"GET /v3 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.001 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 e3dca522b7470345325f5dcda8adfc8e 172.16.30.1 - - [21/Mar/2022:17:17:34 +0000] \"GET /v5 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 207b1c9f4b52683061bb0807857c32f6 172.16.30.1 - - [21/Mar/2022:17:17:36 +0000] \"GET /v6 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 f2d0ae4264c26168246099ee90cc9f27 172.16.30.1 - - [21/Mar/2022:17:17:39 +0000] \"GET /vdfs HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.001 200 b08773ef6c074b4b10fee5626457df47 172.16.30.1 - - [21/Mar/2022:17:17:40 +0000] \"GET /vdfs HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 cd5a39f9d32b212bf97b41c57f7b429b 172.16.30.1 - - [21/Mar/2022:17:18:58 +0000] \"GET /vdfs HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.009 [default-web-8080] [] 10.244.205.195:8080 60 0.008 200 f413dd82a31c72df36114843c5054aad 172.16.30.1 - - [21/Mar/2022:17:18:59 +0000] \"GET /vdfs HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 09a92db2773acffcd23b31c856673a9b 172.16.30.1 - - [21/Mar/2022:17:19:12 +0000] \"GET /v1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.001 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 c21e9af32944af9d9d70b39eaedbe11c 172.16.30.1 - - [21/Mar/2022:17:19:14 +0000] \"GET /v HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 81 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 5daf84fe1f8992ff8d9d0cdec4b751c1 172.16.30.1 - - [21/Mar/2022:17:19:17 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.007 [default-web2-8080] [] 10.244.151.3:8080 61 0.008 200 52df2d3780d65d11e34a545583958a17 172.16.30.1 - - [21/Mar/2022:17:20:32 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.003 [default-web2-8080] [] 10.244.151.3:8080 61 0.003 200 d05940e016c2b36ce79996c04065b5a8 172.16.30.1 - - [21/Mar/2022:17:20:34 +0000] \"GET /v2f HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 83 0.003 [default-web2-8080] [] 10.244.151.3:8080 61 0.002 200 c59de52e577935709573bf0515e999e2 172.16.30.1 - - [21/Mar/2022:17:20:36 +0000] \"GET /v3 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 3354dc569894182a6ee5a5f69892ca66 172.16.30.1 - - [21/Mar/2022:17:20:40 +0000] \"GET /v35 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 83 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 75cccfe798f3a6f2ee072c9233c8e3b7 172.16.30.1 - - [21/Mar/2022:17:20:44 +0000] \"GET /v2a HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 83 0.004 [default-web2-8080] [] 10.244.151.3:8080 61 0.004 200 8ae6170f4a95446d0615bfc5d4a16403 172.16.30.1 - - [21/Mar/2022:17:20:48 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 c4cea1304f930170d7a7148fc2ac04a7 172.16.30.1 - - [21/Mar/2022:17:21:49 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 b860ca473d1b622514dd4ebf194311f2 172.16.30.1 - - [21/Mar/2022:17:21:50 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 1f8bf150a942fb5272d01d9a82c6fd77 172.16.30.1 - - [21/Mar/2022:17:21:51 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 2a2c3eec3ebe85dd546ea35cf92f1357 172.16.30.1 - - [21/Mar/2022:17:21:52 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.005 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 fca07770b107dfb25126f60a1f4dabcc 172.16.30.1 - - [21/Mar/2022:17:21:52 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 421ddaffd339aafd3ac912c195035b7e 172.16.30.1 - - [21/Mar/2022:17:22:41 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 58a2dc6300d892b640bd26ab007daba8 172.16.30.1 - - [21/Mar/2022:17:22:42 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 402a0a5fd8037182ea8671d3d9347ee7 172.16.30.1 - - [21/Mar/2022:17:22:44 +0000] \"GET /df HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 4cb6f78cb734373510f98f669cdf253d 172.16.30.1 - - [21/Mar/2022:17:22:46 +0000] \"GET /dfdg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.001 200 2fd8c843e0fac657bc80e2a6b23dcaf6 172.16.30.1 - - [21/Mar/2022:17:23:52 +0000] \"GET /v7 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 5aaabd3963132e95499b5be8b57368e6 172.16.30.1 - - [21/Mar/2022:17:23:54 +0000] \"GET /v9 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 452ce52c0ab19f2fdc37152f925887f7 172.16.30.1 - - [21/Mar/2022:17:24:28 +0000] \"GET /v9 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 faf0e7f8be1b66b38f6747c7df815525 172.16.30.1 - - [21/Mar/2022:17:24:30 +0000] \"GET /v9 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 c4526f5d5a9fab7245b1d77becf24bb7 172.16.30.1 - - [21/Mar/2022:17:24:31 +0000] \"GET /v9 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 47fd2dd3ed81e5fa3762687343ed06c0 pradeep@learnk8s$ Kubernetes DNS When we setup our minikube cluster, it is configured to use the CoreDNS addon or its precursor, kube-dns. We can verify the same. pradeep@learnk8s$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-8594699699-dztlm 1/1 Running 2 (120m ago) 47h calico-node-gqvw6 1/1 Running 2 (119m ago) 47h calico-node-qdbcf 1/1 Running 1 (120m ago) 47h calico-node-sw74l 1/1 Running 1 (118m ago) 47h coredns-64897985d-58btq 1/1 Running 2 (120m ago) 47h etcd-minikube 1/1 Running 1 (120m ago) 47h kube-apiserver-minikube 1/1 Running 3 (120m ago) 47h kube-controller-manager-minikube 1/1 Running 3 (120m ago) 47h kube-proxy-7k4lb 1/1 Running 1 (118m ago) 47h kube-proxy-gm2dh 1/1 Running 1 (119m ago) 47h kube-proxy-hvkqd 1/1 Running 1 (120m ago) 47h kube-scheduler-minikube 1/1 Running 2 (120m ago) 47h storage-provisioner 1/1 Running 10 (107m ago) 47h Describe the coredns pod, to get additional information. pradeep@learnk8s$ kubectl describe pods coredns-64897985d-58btq -n kube-system Name: coredns-64897985d-58btq Namespace: kube-system Priority: 2000000000 Priority Class Name: system-cluster-critical Node: minikube/172.16.30.6 Start Time: Sat, 19 Mar 2022 23:48:56 +0530 Labels: k8s-app=kube-dns pod-template-hash=64897985d Annotations: <none> Status: Running IP: 10.88.0.4 IPs: IP: 10.88.0.4 Controlled By: ReplicaSet/coredns-64897985d Containers: coredns: Container ID: docker://c866d7b12f5f086407e04f3fd4e500889f860bfd55790e1e2a4870fad1052330 Image: k8s.gcr.io/coredns/coredns:v1.8.6 Image ID: docker-pullable://k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e Ports: 53/UDP, 53/TCP, 9153/TCP Host Ports: 0/UDP, 0/TCP, 0/TCP Args: -conf /etc/coredns/Corefile State: Running Started: Mon, 21 Mar 2022 21:07:03 +0530 Last State: Terminated Reason: Error Exit Code: 255 Started: Sun, 20 Mar 2022 19:25:02 +0530 Finished: Mon, 21 Mar 2022 21:06:40 +0530 Ready: True Restart Count: 2 Limits: memory: 170Mi Requests: cpu: 100m memory: 70Mi Liveness: http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5 Readiness: http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3 Environment: <none> Mounts: /etc/coredns from config-volume (ro) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rhcln (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: config-volume: Type: ConfigMap (a volume populated by a ConfigMap) Name: coredns Optional: false kube-api-access-rhcln: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: Burstable Node-Selectors: kubernetes.io/os=linux Tolerations: CriticalAddonsOnly op=Exists node-role.kubernetes.io/control-plane:NoSchedule node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 107m kubelet Liveness probe failed: Get \"http://10.88.0.4:8080/health\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 107m (x2 over 107m) kubelet Readiness probe failed: Get \"http://10.88.0.4:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) pradeep@learnk8s$ Create a simple Pod to use as a test environment pradeep@learnk8s$ cat dnsutils.yaml apiVersion: v1 kind: Pod metadata: name: dnsutils namespace: default spec: containers: - name: dnsutils image: k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always pradeep@learnk8s$ pradeep@learnk8s$ kubectl create -f dnsutils.yaml pod/dnsutils created pradeep@learnk8s$ Once that Pod is running, you can exec nslookup in that environment. pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup kubernetes.default ;; connection timed out; no servers could be reached command terminated with exit code 1 pradeep@learnk8s$ As the nslookup command failed, check the following: pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 47h web NodePort 10.105.47.136 <none> 8080:31270/TCP 111m web2 NodePort 10.106.73.151 <none> 8080:31691/TCP 102m kubernetes service is running the default namespace. Check the kube-dns service in the kube-system namespace. pradeep@learnk8s$ kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 47h pradeep@learnk8s$ It is running fine. Let us check what nameserver is being used by this Pod. pradeep@learnk8s$ kubectl exec -ti dnsutils -- cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 This confirms that the dnsutils pod is using the correct nameserver. Are DNS endpoints exposed? You can verify that DNS endpoints are exposed by using the kubectl get endpoints command. pradeep@learnk8s$ kubectl get ep kube-dns -n kube-system NAME ENDPOINTS AGE kube-dns 10.88.0.4:53,10.88.0.4:53,10.88.0.4:9153 47h pradeep@learnk8s$ Hmm, the endpoints are coming from the Podman CNI but not from the Calico CNI that we are using in this cluster. You remember the race condition issue between Calico and Podman CNIs?! Let us get the replicaset ID for the coredns service and delete it. pradeep@learnk8s$ kubectl get rs -n kube-system NAME DESIRED CURRENT READY AGE calico-kube-controllers-8594699699 1 1 1 47h coredns-64897985d 1 1 1 47h Deleting the replicaset pradeep@learnk8s$ kubectl delete rs coredns-64897985d -n kube-system replicaset.apps \"coredns-64897985d\" deleted pradeep@learnk8s$ Immediately, the replicaset got re-created. Check the AGE! pradeep@learnk8s$ kubectl get rs -n kube-system NAME DESIRED CURRENT READY AGE calico-kube-controllers-8594699699 1 1 1 47h coredns-64897985d 1 1 0 2s pradeep@learnk8s$ Let us check the endpoints again. pradeep@learnk8s$ kubectl get ep kube-dns -n kube-system NAME ENDPOINTS AGE kube-dns 10.244.205.196:53,10.244.205.196:53,10.244.205.196:9153 47h pradeep@learnk8s$ Now that the endpoints are correctly pointing to the Calico CNI IP addresses, verify the nslookup again. pradeep@learnk8s$ kubectl exec -ti dnsutils -- cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup kubernetes.default Server: 10.96.0.10 Address: 10.96.0.10#53 Name: kubernetes.default.svc.cluster.local Address: 10.96.0.1 pradeep@learnk8s$ Just to verify the coredns status. pradeep@learnk8s$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-8594699699-dztlm 1/1 Running 2 (132m ago) 47h calico-node-gqvw6 1/1 Running 2 (131m ago) 47h calico-node-qdbcf 1/1 Running 1 (132m ago) 47h calico-node-sw74l 1/1 Running 1 (130m ago) 47h coredns-64897985d-kwp95 1/1 Running 0 74s etcd-minikube 1/1 Running 1 (132m ago) 47h kube-apiserver-minikube 1/1 Running 3 (132m ago) 47h kube-controller-manager-minikube 1/1 Running 3 (132m ago) 47h kube-proxy-7k4lb 1/1 Running 1 (130m ago) 47h kube-proxy-gm2dh 1/1 Running 1 (131m ago) 47h kube-proxy-hvkqd 1/1 Running 1 (132m ago) 47h kube-scheduler-minikube 1/1 Running 2 (132m ago) 47h storage-provisioner 1/1 Running 10 (118m ago) 47h pradeep@learnk8s$ Are DNS queries being received/processed? You can verify if queries are being received by CoreDNS by adding the log plugin to the CoreDNS configuration (aka Corefile). The CoreDNS Corefile is held in a ConfigMap named coredns . pradeep@learnk8s$ kubectl get cm -n kube-system NAME DATA AGE calico-config 4 47h coredns 1 47h extension-apiserver-authentication 6 47h kube-proxy 2 47h kube-root-ca.crt 1 47h kubeadm-config 1 47h kubelet-config-1.23 1 47h pradeep@learnk8s$ Describe the config map. pradeep@learnk8s$ kubectl describe cm coredns -n kube-system Name: coredns Namespace: kube-system Labels: <none> Annotations: <none> Data ==== Corefile: ---- .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 hosts { 192.168.64.1 host.minikube.internal fallthrough } forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } BinaryData ==== Events: <none> pradeep@learnk8s$ Edit this configmap and add the log plugin. pradeep@learnk8s$ kubectl edit cm coredns -n kube-system configmap/coredns edited pradeep@learnk8s$ Describe it again and verify the change pradeep@learnk8s$ kubectl describe cm coredns -n kube-system Name: coredns Namespace: kube-system Labels: <none> Annotations: <none> Data ==== Corefile: ---- .:53 { log errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 hosts { 192.168.64.1 host.minikube.internal fallthrough } forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } BinaryData ==== Events: <none> pradeep@learnk8s$ After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods. Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs. pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95 .:53 [INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34 CoreDNS-1.8.6 linux/amd64, go1.17.1, 13a9191 [INFO] Reloading [INFO] plugin/health: Going into lameduck mode for 5s pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup kubernetes.default Server: 10.96.0.10 Address: 10.96.0.10#53 Name: kubernetes.default.svc.cluster.local Address: 10.96.0.1 pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95 .:53 [INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34 CoreDNS-1.8.6 linux/amd64, go1.17.1, 13a9191 [INFO] Reloading [INFO] plugin/health: Going into lameduck mode for 5s [INFO] plugin/reload: Running configuration MD5 = 07d7c5ad4525bf2c472eaef020d0184d [INFO] Reloading complete [INFO] 127.0.0.1:39552 - 1628 \"HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512\" NXDOMAIN qr,rd,ra 132 1.058936054s [INFO] 10.244.151.4:41947 - 42256 \"A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512\" NXDOMAIN qr,aa,rd 155 0.000469847s [INFO] 10.244.151.4:55247 - 43436 \"A IN kubernetes.default.svc.cluster.local. udp 54 false 512\" NOERROR qr,aa,rd 106 0.000357148s pradeep@learnk8s$ Let us make some more queries, pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup web.default Server: 10.96.0.10 Address: 10.96.0.10#53 Name: web.default.svc.cluster.local Address: 10.105.47.136 pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup web2.default Server: 10.96.0.10 Address: 10.96.0.10#53 Name: web2.default.svc.cluster.local Address: 10.106.73.151 pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup web3.default Server: 10.96.0.10 Address: 10.96.0.10#53 ** server can't find web3.default: NXDOMAIN command terminated with exit code 1 pradeep@learnk8s$ All the queries worked as expected. We did not have any service named web3 in the default namespace, so it failed. Let us review the coredns logs again. pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95 .:53 [INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34 CoreDNS-1.8.6 linux/amd64, go1.17.1, 13a9191 [INFO] Reloading [INFO] plugin/health: Going into lameduck mode for 5s [INFO] plugin/reload: Running configuration MD5 = 07d7c5ad4525bf2c472eaef020d0184d [INFO] Reloading complete [INFO] 127.0.0.1:39552 - 1628 \"HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512\" NXDOMAIN qr,rd,ra 132 1.058936054s [INFO] 10.244.151.4:41947 - 42256 \"A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512\" NXDOMAIN qr,aa,rd 155 0.000469847s [INFO] 10.244.151.4:55247 - 43436 \"A IN kubernetes.default.svc.cluster.local. udp 54 false 512\" NOERROR qr,aa,rd 106 0.000357148s [INFO] 10.244.151.4:46146 - 25028 \"A IN web.default.default.svc.cluster.local. udp 55 false 512\" NXDOMAIN qr,aa,rd 148 0.000302078s [INFO] 10.244.151.4:43513 - 15523 \"A IN web.default.svc.cluster.local. udp 47 false 512\" NOERROR qr,aa,rd 92 0.000412597s [INFO] 10.244.151.4:41650 - 22470 \"A IN web2.default.default.svc.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000398918s [INFO] 10.244.151.4:33214 - 54758 \"A IN web2.default.svc.cluster.local. udp 48 false 512\" NOERROR qr,aa,rd 94 0.00031088s [INFO] 10.244.151.4:34263 - 58079 \"A IN web3.default.default.svc.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000433005s [INFO] 10.244.151.4:52587 - 56008 \"A IN web3.default.svc.cluster.local. udp 48 false 512\" NXDOMAIN qr,aa,rd 141 0.000420513s [INFO] 10.244.151.4:49172 - 35844 \"A IN web3.default.cluster.local. udp 44 false 512\" NXDOMAIN qr,aa,rd 137 0.000302094s [INFO] 10.244.151.4:44833 - 3203 \"A IN web3.default. udp 30 false 512\" NXDOMAIN qr,rd,ra 105 0.18944251s pradeep@learnk8s$ Are you in the right namespace for the service? DNS queries that don't specify a namespace are limited to the pod's namespace. If the namespace of the pod and service differ, the DNS query must include the namespace of the service. This query is limited to the pod's namespace: pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup ingress-nginx-controller Server: 10.96.0.10 Address: 10.96.0.10#53 ** server can't find ingress-nginx-controller: NXDOMAIN command terminated with exit code 1 pradeep@learnk8s$ This failed because, ingress-nginx-controller svc is not present in the default namespace. pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95 .:53 [INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34 CoreDNS-1.8.6 linux/amd64, go1.17.1, 13a9191 [INFO] Reloading [INFO] plugin/health: Going into lameduck mode for 5s [INFO] plugin/reload: Running configuration MD5 = 07d7c5ad4525bf2c472eaef020d0184d [INFO] Reloading complete [INFO] 127.0.0.1:39552 - 1628 \"HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512\" NXDOMAIN qr,rd,ra 132 1.058936054s [INFO] 10.244.151.4:41947 - 42256 \"A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512\" NXDOMAIN qr,aa,rd 155 0.000469847s [INFO] 10.244.151.4:55247 - 43436 \"A IN kubernetes.default.svc.cluster.local. udp 54 false 512\" NOERROR qr,aa,rd 106 0.000357148s [INFO] 10.244.151.4:46146 - 25028 \"A IN web.default.default.svc.cluster.local. udp 55 false 512\" NXDOMAIN qr,aa,rd 148 0.000302078s [INFO] 10.244.151.4:43513 - 15523 \"A IN web.default.svc.cluster.local. udp 47 false 512\" NOERROR qr,aa,rd 92 0.000412597s [INFO] 10.244.151.4:41650 - 22470 \"A IN web2.default.default.svc.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000398918s [INFO] 10.244.151.4:33214 - 54758 \"A IN web2.default.svc.cluster.local. udp 48 false 512\" NOERROR qr,aa,rd 94 0.00031088s [INFO] 10.244.151.4:34263 - 58079 \"A IN web3.default.default.svc.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000433005s [INFO] 10.244.151.4:52587 - 56008 \"A IN web3.default.svc.cluster.local. udp 48 false 512\" NXDOMAIN qr,aa,rd 141 0.000420513s [INFO] 10.244.151.4:49172 - 35844 \"A IN web3.default.cluster.local. udp 44 false 512\" NXDOMAIN qr,aa,rd 137 0.000302094s [INFO] 10.244.151.4:44833 - 3203 \"A IN web3.default. udp 30 false 512\" NXDOMAIN qr,rd,ra 105 0.18944251s [INFO] 10.244.151.4:50251 - 50181 \"A IN ingress-nginx-controller.default.svc.cluster.local. udp 68 false 512\" NXDOMAIN qr,aa,rd 161 0.000381672s [INFO] 10.244.151.4:52033 - 12007 \"A IN ingress-nginx-controller.svc.cluster.local. udp 60 false 512\" NXDOMAIN qr,aa,rd 153 0.000844354s [INFO] 10.244.151.4:50003 - 30556 \"A IN ingress-nginx-controller.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000453492s [INFO] 10.244.151.4:41810 - 19431 \"A IN ingress-nginx-controller. udp 42 false 512\" NXDOMAIN qr,rd,ra 117 0.057777871s pradeep@learnk8s$ We can see that the query is done with the default namespace: ingress-nginx-controller.default.svc.cluster.local. Let us list all active services in all namespaces. pradeep@learnk8s$ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 47h default web NodePort 10.105.47.136 <none> 8080:31270/TCP 137m default web2 NodePort 10.106.73.151 <none> 8080:31691/TCP 127m ingress-nginx ingress-nginx-controller NodePort 10.101.85.4 <none> 80:30419/TCP,443:30336/TCP 147m ingress-nginx ingress-nginx-controller-admission ClusterIP 10.104.163.136 <none> 443/TCP 147m kube-system kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 47h pradeep@learnk8s$ Now that we see ingress-nginx-controller in the ingress-nginx namespace, let us query for the correct name. pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup ingress-nginx-controller.ingress-nginx Server: 10.96.0.10 Address: 10.96.0.10#53 Name: ingress-nginx-controller.ingress-nginx.svc.cluster.local Address: 10.101.85.4 We can see the name resolution is successful this time. :end: Thank you for reading my :book: :warning: Warning: Do not push the big red button. :memo: Note: Sunrises are beautiful. :bulb: Tip: Remember to appreciate the little things in life. :building_construction: :building_construction::building_construction: Learning continues and new topics will be updated here regularly. Keep learning :rocket: Best regards, Pradeep","title":"Kubernetes"},{"location":"Kubernetes/#kubernetes","text":"","title":"Kubernetes"},{"location":"Kubernetes/#introduction","text":"Google has created a resource in the form of a comic to understand why Kubernetes and what problems can be addressed by Kubernetes. Here is the link to the same. https://cloud.google.com/kubernetes-engine/kubernetes-comic And here is an extract:","title":"Introduction"},{"location":"Kubernetes/#learning-resources","text":"While there are many resources out there, personally I have used these resources: Introduction to Kubernetes by Linux Foundation then, the most popular Udemy course on this subject: Certified Kubernetes Administrator (CKA) with Practice Tests by Mumshad Mannambeth , and the official Linux Foundation course LFS258 Also, I got the opportunity to attend some of the O'Reilly Live Classes\u2013 Certified Kubernetes Administrator (CKA) Exam Prep, hosted by Benjamin Muschko , and Certified Kubernetes Administrator (CKA) Crash Course, presented by Sander van Vugt . Of course, the official Kubernetes Documentation","title":"Learning Resources"},{"location":"Kubernetes/#lab-setup","text":"In this post, let's explore the world of Kubernetes right from our workstation. All the examples shown here are executed on a machine running macOS. To get started, we need to install three packages: minikube , hyperkit , and kubectl . If you already have some kind of virtualization software like Vmware Fusion or Virtualbox, hyperkit is not mandatory, but I prefer to use hyperkit (which seems to be the case with minikube also).","title":"Lab Setup"},{"location":"Kubernetes/#minikube","text":"","title":"Minikube"},{"location":"Kubernetes/#download","text":"pradeep@learnk8s$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64","title":"Download"},{"location":"Kubernetes/#install","text":"pradeep@learnk8s$ sudo install minikube-darwin-amd64 /usr/local/bin/minikube pradeep@learnk8s$ minikube version minikube version: v1.25.1 commit: 3e64b11ed75e56e4898ea85f96b2e4af0301f43d","title":"Install"},{"location":"Kubernetes/#hyperkit","text":"","title":"Hyperkit"},{"location":"Kubernetes/#download_1","text":"If not, install Brew using, https://brew.sh/","title":"Download"},{"location":"Kubernetes/#install_1","text":"if you have Brew package manager, run: pradeep@learnk8s$ brew install hyperkit pradeep@learnk8s$ hyperkit -v hyperkit: 0.20200908 Homepage: https://github.com/docker/hyperkit License: BSD","title":"Install"},{"location":"Kubernetes/#kubectl","text":"The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.","title":"Kubectl"},{"location":"Kubernetes/#download_2","text":"https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/","title":"Download"},{"location":"Kubernetes/#install_2","text":"pradeep@learnk8s$ brew install kubectl pradeep@learnk8s$ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.2\", GitCommit:\"9d142434e3af351a628bffee3939e64c681afa4d\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:27:51Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.1\", GitCommit:\"86ec240af8cbd1b60bcc4c03c20da9b98005b92e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T11:34:54Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}","title":"Install"},{"location":"Kubernetes/#all-in-one-minikube-cluster-macos","text":"pradeep@learnk8s$ minikube start \ud83d\ude04 minikube v1.25.1 on Darwin 11.6.2 \u2728 Automatically selected the hyperkit driver. Other choices: vmware, virtualbox, ssh \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ... \u2757 This VM is having trouble accessing https://k8s.gcr.io \ud83d\udca1 To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/ \ud83d\udc33 Preparing Kubernetes v1.23.1 on Docker 20.10.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \u2757 /usr/local/bin/kubectl is version 1.21.2, which may have incompatibilites with Kubernetes 1.23.1. \u25aa Want kubectl v1.23.1? Try 'minikube kubectl -- get pods -A' \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default","title":"All in one Minikube Cluster (macOS)"},{"location":"Kubernetes/#verify","text":"pradeep@learnk8s$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured pradeep@learnk8s$ kubectl get pods No resources found in default namespace. pradeep@learnk8s$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-64897985d-nvzjq 1/1 Running 0 5m7s kube-system etcd-minikube 1/1 Running 0 5m7s kube-system kube-apiserver-minikube 1/1 Running 0 5m7s kube-system kube-controller-manager-minikube 1/1 Running 0 5m7s kube-system kube-proxy-2ch8q 1/1 Running 0 5m7s kube-system kube-scheduler-minikube 1/1 Running 0 5m7s kube-system storage-provisioner 1/1 Running 0 5m7s","title":"Verify"},{"location":"Kubernetes/#multi-node-minikube-cluster-macos","text":"pradeep@learnk8s$ minikube start --nodes 2 -p k8s \ud83d\ude04 [k8s] minikube v1.25.1 on Darwin 11.6.2 \u2728 Automatically selected the hyperkit driver. Other choices: vmware, virtualbox, ssh \ud83d\udc4d Starting control plane node k8s in cluster k8s \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \u2757 This VM is having trouble accessing https://k8s.gcr.io \ud83d\udca1 To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/ \ud83d\udc33 Preparing Kubernetes v1.23.1 on Docker 20.10.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa kubelet.cni-conf-dir=/etc/cni/net.mk \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd17 Configuring CNI (Container Networking Interface) ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: storage-provisioner, default-storageclass \ud83d\udc4d Starting worker node k8s-m02 in cluster k8s \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=192.168.177.17 \u2757 This VM is having trouble accessing https://k8s.gcr.io \ud83d\udca1 To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/ \ud83d\udc33 Preparing Kubernetes v1.23.1 on Docker 20.10.12 ... \u25aa env NO_PROXY=192.168.177.17 \ud83d\udd0e Verifying Kubernetes components... \u2757 /usr/local/bin/kubectl is version 1.21.2, which may have incompatibilites with Kubernetes 1.23.1. \u25aa Want kubectl v1.23.1? Try 'minikube kubectl -- get pods -A' \ud83c\udfc4 Done! kubectl is now configured to use \"k8s\" cluster and \"default\" namespace by default","title":"Multi-node Minikube Cluster (macOS)"},{"location":"Kubernetes/#verify_1","text":"pradeep@learnk8s$ minikube status -p k8s k8s type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured k8s-m02 type: Worker host: Running kubelet: Running","title":"Verify"},{"location":"Kubernetes/#minikube-cluster-with-a-different-runtime-containerd","text":"Read this post on Kubernetes Blog https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ pradeep@learnk8s$ minikube start --container-runtime=containerd \ud83d\ude04 minikube v1.25.1 on Darwin 11.6.2 \u2728 Automatically selected the hyperkit driver. Other choices: vmware, virtualbox, ssh \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udcbe Downloading Kubernetes v1.23.1 preload ... > preloaded-images-k8s-v16-v1...: 571.57 MiB / 571.57 MiB 100.00% 3.17 MiB \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ... \u2757 This VM is having trouble accessing https://k8s.gcr.io \ud83d\udca1 To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/ \ud83d\udce6 Preparing Kubernetes v1.23.1 on containerd 1.4.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd17 Configuring bridge CNI (Container Networking Interface) ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: storage-provisioner, default-storageclass \u2757 /usr/local/bin/kubectl is version 1.21.2, which may have incompatibilites with Kubernetes 1.23.1. \u25aa Want kubectl v1.23.1? Try 'minikube kubectl -- get pods -A' \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default","title":"Minikube Cluster with a different runtime (containerd)"},{"location":"Kubernetes/#kubernetes-architecture","text":"https://kubernetes.io/docs/concepts/overview/components/","title":"Kubernetes Architecture"},{"location":"Kubernetes/#control-plane","text":"Etcd\u2014A distributed data store which persists cluster configuration API Server\u2014Central communication hub for all Kubernetes components This is also the endpoint used by Kubernetes clients, such as the kubectl CLI Scheduler\u2014The component responsible for assigning application components to worker nodes Controller Manager\u2014Handles cluster-level operations, such as tracking worker node status, handling node failures, and replicating components For our examples we will use a single master configuration, but the master functions can be replicated and distributed for reliability","title":"Control Plane"},{"location":"Kubernetes/#worker-nodes","text":"Kubelet\u2014Deploys and manages application components assigned to the local node by the scheduler. Kube-proxy\u2014The local networking component on the worker. Handles load-balancing between application components. Container engine \u2013 The container runtime used by the local worker node. For our examples we will use the most common runtime, Docker.","title":"Worker Nodes"},{"location":"Kubernetes/#kubernetes-api-resources","text":"pradeep@learnk8s$ kubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND bindings v1 true Binding componentstatuses cs v1 false ComponentStatus configmaps cm v1 true ConfigMap endpoints ep v1 true Endpoints events ev v1 true Event limitranges limits v1 true LimitRange namespaces ns v1 false Namespace nodes no v1 false Node persistentvolumeclaims pvc v1 true PersistentVolumeClaim persistentvolumes pv v1 false PersistentVolume pods po v1 true Pod podtemplates v1 true PodTemplate replicationcontrollers rc v1 true ReplicationController resourcequotas quota v1 true ResourceQuota secrets v1 true Secret serviceaccounts sa v1 true ServiceAccount services svc v1 true Service mutatingwebhookconfigurations admissionregistration.k8s.io/v1 false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io/v1 false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io/v1 false CustomResourceDefinition apiservices apiregistration.k8s.io/v1 false APIService controllerrevisions apps/v1 true ControllerRevision daemonsets ds apps/v1 true DaemonSet deployments deploy apps/v1 true Deployment replicasets rs apps/v1 true ReplicaSet statefulsets sts apps/v1 true StatefulSet tokenreviews authentication.k8s.io/v1 false TokenReview localsubjectaccessreviews authorization.k8s.io/v1 true LocalSubjectAccessReview selfsubjectaccessreviews authorization.k8s.io/v1 false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io/v1 false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io/v1 false SubjectAccessReview horizontalpodautoscalers hpa autoscaling/v2 true HorizontalPodAutoscaler cronjobs cj batch/v1 true CronJob jobs batch/v1 true Job certificatesigningrequests csr certificates.k8s.io/v1 false CertificateSigningRequest leases coordination.k8s.io/v1 true Lease endpointslices discovery.k8s.io/v1 true EndpointSlice events ev events.k8s.io/v1 true Event flowschemas flowcontrol.apiserver.k8s.io/v1beta2 false FlowSchema prioritylevelconfigurations flowcontrol.apiserver.k8s.io/v1beta2 false PriorityLevelConfiguration ingressclasses networking.k8s.io/v1 false IngressClass ingresses ing networking.k8s.io/v1 true Ingress networkpolicies netpol networking.k8s.io/v1 true NetworkPolicy runtimeclasses node.k8s.io/v1 false RuntimeClass poddisruptionbudgets pdb policy/v1 true PodDisruptionBudget podsecuritypolicies psp policy/v1beta1 false PodSecurityPolicy clusterrolebindings rbac.authorization.k8s.io/v1 false ClusterRoleBinding clusterroles rbac.authorization.k8s.io/v1 false ClusterRole rolebindings rbac.authorization.k8s.io/v1 true RoleBinding roles rbac.authorization.k8s.io/v1 true Role priorityclasses pc scheduling.k8s.io/v1 false PriorityClass csidrivers storage.k8s.io/v1 false CSIDriver csinodes storage.k8s.io/v1 false CSINode csistoragecapacities storage.k8s.io/v1beta1 true CSIStorageCapacity storageclasses sc storage.k8s.io/v1 false StorageClass volumeattachments storage.k8s.io/v1 false VolumeAttachment","title":"Kubernetes API Resources"},{"location":"Kubernetes/#kubernetes-resources-of-a-standard-application","text":"As shown here, to deploy a standard application in Kubernetes, we might have to deploy a lot of these API resources. https://github.com/kubernetes/community/blob/master/icons/examples/schemas/std-app.png Reference https://kubernetes.io/docs Pods Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. ReplicaSet A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. Deployments A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. Service An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you don't need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.","title":"Kubernetes resources of a standard application"},{"location":"Kubernetes/#namespace","text":"","title":"Namespace"},{"location":"Kubernetes/#defaults","text":"pradeep@learnk8s$ kubectl get namespaces NAME STATUS AGE default Active 2m7s kube-node-lease Active 2m10s kube-public Active 2m10s kube-system Active 2m10s As we saw at the beginning, when we setup a kubernetes cluster, all control plane and worker node compononets gets installed as pods in the system defined namespace kube-system .","title":"Defaults"},{"location":"Kubernetes/#create-a-new-namespace","text":"pradeep@learnk8s$ kubectl create namespace prod namespace/prod created pradeep@learnk8s$ kubectl get ns NAME STATUS AGE default Active 5m4s prod Active 33s kube-node-lease Active 5m7s kube-public Active 5m7s kube-system Active 5m7s pradeep@learnk8s$ kubectl run nginx --image=nginx -n prod pod/nginx created pradeep@learnk8s$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 79s","title":"Create a new namespace"},{"location":"Kubernetes/#life-cycle-of-a-pod","text":"kubectl client sends new pod request Scheduler assigns new pod to worker node API server instructs kubelet to provision new Pod kubelet requests that Docker create container Docker retrieves nginx image Docker creates container Kubelet passes container details to K8s API Pod configs stored in etcd Network configs are created on kube-proxy","title":"Life cycle of a Pod"},{"location":"Kubernetes/#create-a-deployment","text":"pradeep@learnk8s$ kubectl create deployment demo --image=k8s.gcr.io/echoserver:1.4 deployment.apps/demo created pradeep@learnk8s$ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE demo 1/1 1 1 36s pradeep@learnk8s$ kubectl get replicasets NAME DESIRED CURRENT READY AGE demo-75f9c7566f 1 1 1 31s pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-75f9c7566f-nmjw2 1/1 Running 0 26s","title":"Create a Deployment"},{"location":"Kubernetes/#view-cluster-events","text":"pradeep@learnk8s$ kubectl get events LAST SEEN TYPE REASON OBJECT MESSAGE 5m15s Normal Scheduled pod/demo-75f9c7566f-nmjw2 Successfully assigned default/demo-75f9c7566f-nmjw2 to k8s-m02 5m14s Normal Pulling pod/demo-75f9c7566f-nmjw2 Pulling image \"k8s.gcr.io/echoserver:1.4\" 4m57s Normal Pulled pod/demo-75f9c7566f-nmjw2 Successfully pulled image \"k8s.gcr.io/echoserver:1.4\" in 16.898815167s 4m57s Normal Created pod/demo-75f9c7566f-nmjw2 Created container echoserver 4m57s Normal Started pod/demo-75f9c7566f-nmjw2 Started container echoserver 5m15s Normal SuccessfulCreate replicaset/demo-75f9c7566f Created pod: demo-75f9c7566f-nmjw2 5m15s Normal ScalingReplicaSet deployment/demo Scaled up replica set demo-75f9c7566f to 1 14m Normal Starting node/k8s-m02 Starting kubelet. 14m Normal NodeHasSufficientMemory node/k8s-m02 Node k8s-m02 status is now: NodeHasSufficientMemory 14m Normal NodeHasNoDiskPressure node/k8s-m02 Node k8s-m02 status is now: NodeHasNoDiskPressure 14m Normal NodeHasSufficientPID node/k8s-m02 Node k8s-m02 status is now: NodeHasSufficientPID 14m Normal NodeAllocatableEnforced node/k8s-m02 Updated Node Allocatable limit across pods 14m Normal Starting node/k8s-m02 14m Normal RegisteredNode node/k8s-m02 Node k8s-m02 event: Registered Node k8s-m02 in Controller 13m Normal NodeReady node/k8s-m02 Node k8s-m02 status is now: NodeReady 15m Normal NodeHasSufficientMemory node/k8s Node k8s status is now: NodeHasSufficientMemory 15m Normal NodeHasNoDiskPressure node/k8s Node k8s status is now: NodeHasNoDiskPressure 15m Normal NodeHasSufficientPID node/k8s Node k8s status is now: NodeHasSufficientPID 14m Normal Starting node/k8s Starting kubelet. 14m Normal NodeHasSufficientMemory node/k8s Node k8s status is now: NodeHasSufficientMemory 14m Normal NodeHasNoDiskPressure node/k8s Node k8s status is now: NodeHasNoDiskPressure 14m Normal NodeHasSufficientPID node/k8s Node k8s status is now: NodeHasSufficientPID 14m Normal NodeAllocatableEnforced node/k8s Updated Node Allocatable limit across pods 14m Normal RegisteredNode node/k8s Node k8s event: Registered Node k8s in Controller 14m Normal Starting node/k8s 14m Normal NodeReady node/k8s Node k8s status is now: NodeReady","title":"View Cluster Events"},{"location":"Kubernetes/#scaling-a-deployment","text":"pradeep@learnk8s$ kubectl scale deployment demo --replicas=3 deployment.apps/demo scaled pradeep@learnk8s$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE demo 3/3 3 3 11m pradeep@learnk8s$ kubectl get rs NAME DESIRED CURRENT READY AGE demo-75f9c7566f 3 3 3 12m pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-75f9c7566f-fct76 1/1 Running 0 2m2s demo-75f9c7566f-kz9kb 1/1 Running 0 2m3s demo-75f9c7566f-nmjw2 1/1 Running 0 12m pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-75f9c7566f-fct76 1/1 Running 0 2m10s 10.244.1.4 k8s-m02 <none> <none> demo-75f9c7566f-kz9kb 1/1 Running 0 2m11s 10.244.0.3 k8s <none> <none> demo-75f9c7566f-nmjw2 1/1 Running 0 12m 10.244.1.3 k8s-m02 <none> <none>","title":"Scaling a Deployment"},{"location":"Kubernetes/#minikube-ssh-and-access-pods","text":"pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.4:8080 CLIENT VALUES: client_address=192.168.177.27 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.244.1.4:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.244.1.4:8080 user-agent=curl/7.78.0 BODY: -no body in request-$ $ curl 10.244.0.3:8080 CLIENT VALUES: client_address=10.244.0.1 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.244.0.3:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.244.0.3:8080 user-agent=curl/7.78.0 BODY: -no body in request-$ $ curl 10.244.1.3:8080 CLIENT VALUES: client_address=192.168.177.27 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.244.1.3:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.244.1.3:8080 user-agent=curl/7.78.0 BODY: -no body in request-$","title":"Minikube SSH and Access Pods"},{"location":"Kubernetes/#services","text":"ClusterIP: Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType. NodePort: Exposes the Service on each Node's IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You'll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort> . LoadBalancer: Exposes the Service externally using a cloud provider's load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.","title":"Services"},{"location":"Kubernetes/#default-services","text":"From the default namespace pradeep@learnk8s$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 33m From all namespaces pradeep@learnk8s$ kubectl get services -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 33m kube-system kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 33m","title":"Default Services"},{"location":"Kubernetes/#nodeport-type-service","text":"Expose the deployment as a NodePort type Service pradeep@learnk8s$ kubectl expose deployment demo --type=NodePort --port=8080 service/demo exposed pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo NodePort 10.101.87.60 <none> 8080:32298/TCP 4s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 35m Note the port number 32298 assigned to the service","title":"NodePort type service"},{"location":"Kubernetes/#access-nodeport-type-service","text":"pradeep@learnk8s$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s Ready control-plane,master 38m v1.23.1 192.168.177.27 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 k8s-m02 Ready <none> 37m v1.23.1 192.168.177.28 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 From Minikube SSH session, with k8s-m02 node IP, and NodePort 32298 $ curl 192.168.177.28:32298 CLIENT VALUES: client_address=10.244.1.1 command=GET real path=/ query=nil request_version=1.1 request_uri=http://192.168.177.28:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=192.168.177.28:32298 user-agent=curl/7.78.0 BODY: -no body in request-$ with k8s node IP and NodePort 32298 $ curl 192.168.177.27:32298 CLIENT VALUES: client_address=10.244.0.1 command=GET real path=/ query=nil request_version=1.1 request_uri=http://192.168.177.27:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=192.168.177.27:32298 user-agent=curl/7.78.0 BODY: -no body in request-$","title":"Access NodePort type Service"},{"location":"Kubernetes/#clusterip-service","text":"Let's delete the service pradeep@learnk8s$ kubectl delete svc demo service \"demo\" deleted and expose the same deployment as a ClusterIP (the default ServiceType). If we don't specify any type, its the ClusterIP. pradeep@learnk8s$ kubectl expose deployment demo --port=8080 service/demo exposed pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo ClusterIP 10.103.185.242 <none> 8080/TCP 5s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 45m Note the ClusterIP assigned to this service 10.103.185.242 .","title":"ClusterIP Service"},{"location":"Kubernetes/#access-pods-with-clusterip","text":"pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.103.185.242:8080 CLIENT VALUES: client_address=192.168.177.27 command=GET real path=/ query=nil request_version=1.1 request_uri=http://10.103.185.242:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=10.103.185.242:8080 user-agent=curl/7.78.0 BODY: -no body in request-$","title":"Access Pods with ClusterIP"},{"location":"Kubernetes/#loadbalancer-service","text":"pradeep@learnk8s$ kubectl delete service demo service \"demo\" deleted pradeep@learnk8s$ kubectl expose deployment demo \u2013type=LoadBalancer --port=8080 service/demo exposed pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo LoadBalancer 10.98.252.85 <pending> 8080:30646/TCP 3s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 50m Open another terminal and initiate minikube tunnel pradeep@learnk8s$ minikube tunnel -p k8s Password: Status: machine: k8s pid: 95721 route: 10.96.0.0/12 -> 192.168.177.27 minikube: Running services: [demo] errors: minikube: no errors router: no errors loadbalancer emulator: no errors Return to the first terminal, Verify that EXTERNAL-IP is populated now pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo LoadBalancer 10.98.252.85 10.98.252.85 8080:30646/TCP 3m27s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 53m NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.","title":"LoadBalancer Service"},{"location":"Kubernetes/#access-pods-with-loadbalancer-service","text":"pradeep@learnk8s$ minikube service demo -p k8s |-----------|------|-------------|-----------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|------|-------------|-----------------------------| | default | demo | 8080 | http://192.168.177.27:30646 | |-----------|------|-------------|-----------------------------| \ud83c\udf89 Opening service default/demo in default browser..","title":"Access Pods with LoadBalancer Service"},{"location":"Kubernetes/#deleting-resources","text":"pradeep@learnk8s$ kubectl delete pod demo-75f9c7566f-fct76 pod \"demo-75f9c7566f-fct76\" deleted pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-75f9c7566f-b45x7 0/1 ContainerCreating 0 3s demo-75f9c7566f-kz9kb 1/1 Running 0 55m demo-75f9c7566f-nmjw2 1/1 Running 0 65m pradeep@learnk8s$ kubectl get all NAME READY STATUS RESTARTS AGE pod/demo-75f9c7566f-b45x7 1/1 Running 0 2m29s pod/demo-75f9c7566f-kz9kb 1/1 Running 0 57m pod/demo-75f9c7566f-nmjw2 1/1 Running 0 67m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 77m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/demo 3/3 3 3 67m NAME DESIRED CURRENT READY AGE replicaset.apps/demo-75f9c7566f 3 3 3 67m pradeep@learnk8s$ kubectl delete deployment demo deployment.apps \"demo\" deleted Deleting a deployment, deletes all the associated pods, replicaset as well. pradeep@learnk8s$ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 78m","title":"Deleting Resources"},{"location":"Kubernetes/#creating-resources-in-a-declarative-wayyaml","text":"pradeep@learnk8s$ kubectl run nginx --image=nginx --dry-run=client -o yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: nginx name: nginx spec: containers: - image: nginx name: nginx resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} pradeep@learnk8s$ kubectl run nginx --image=nginx --dry-run=client -o yaml > demopod.yaml pradeep@learnk8s$ kubectl create -f demopod.yaml pod/nginx created pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 50s pradeep@learnk8s$ kubectl create deployment demo --image=nginx --replicas=3 --dry-run=client -o yaml > demodeploy.yaml pradeep@learnk8s$ cat demodeploy.yaml apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: demo name: demo spec: replicas: 3 selector: matchLabels: app: demo strategy: {} template: metadata: creationTimestamp: null labels: app: demo spec: containers: - image: nginx name: nginx resources: {} status: {} pradeep@learnk8s$ kubectl create -f demodeploy.yaml deployment.apps/demo created pradeep@learnk8s$ kubectl get all NAME READY STATUS RESTARTS AGE pod/demo-6c54f77c95-6g7zq 1/1 Running 0 71s pod/demo-6c54f77c95-sb4c9 1/1 Running 0 71s pod/demo-6c54f77c95-w2bsw 1/1 Running 0 71s pod/nginx 1/1 Running 0 5m45s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 90m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/demo 3/3 3 3 71s NAME DESIRED CURRENT READY AGE replicaset.apps/demo-6c54f77c95 3 3 3 71s","title":"Creating Resources in a Declarative Way\u2014YAML"},{"location":"Kubernetes/#show-labels","text":"pradeep@learnk8s$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS demo-6c54f77c95-6g7zq 1/1 Running 0 3m32s app=demo,pod-template-hash=6c54f77c95 demo-6c54f77c95-sb4c9 1/1 Running 0 3m32s app=demo,pod-template-hash=6c54f77c95 demo-6c54f77c95-w2bsw 1/1 Running 0 3m32s app=demo,pod-template-hash=6c54f77c95 nginx 1/1 Running 0 8m6s run=nginx pradeep@learnk8s$ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s Ready control-plane,master 92m v1.23.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s,kubernetes.io/os=linux,minikube.k8s.io/commit=3e64b11ed75e56e4898ea85f96b2e4af0301f43d,minikube.k8s.io/name=k8s,minikube.k8s.io/updated_at=2022_02_04T05_49_32_0700,minikube.k8s.io/version=v1.25.1,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-m02 Ready <none> 91m v1.23.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m02,kubernetes.io/os=linux","title":"Show Labels"},{"location":"Kubernetes/#apply-labels","text":"pradeep@learnk8s$ kubectl label pods nginx new-label=awesome pod/nginx labeled pradeep@learnk8s$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS demo-6c54f77c95-6g7zq 1/1 Running 0 8m47s app=demo,pod-template-hash=6c54f77c95 demo-6c54f77c95-sb4c9 1/1 Running 0 8m47s app=demo,pod-template-hash=6c54f77c95 demo-6c54f77c95-w2bsw 1/1 Running 0 8m47s app=demo,pod-template-hash=6c54f77c95 nginx 1/1 Running 0 13m new-label=awesome,run=nginx","title":"Apply Labels"},{"location":"Kubernetes/#label-selector","text":"pradeep@learnk8s$ kubectl get pods --selector=app=demo NAME READY STATUS RESTARTS AGE demo-6c54f77c95-6g7zq 1/1 Running 0 10m demo-6c54f77c95-sb4c9 1/1 Running 0 10m demo-6c54f77c95-w2bsw 1/1 Running 0 10m pradeep@learnk8s$ kubectl get pods --selector=run=nginx NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 15m pradeep@learnk8s$ kubectl get pods --selector=new-label=awesome NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 15m","title":"Label Selector"},{"location":"Kubernetes/#describe-resources","text":"pradeep@learnk8s$ kubectl describe pods nginx Name: nginx Namespace: default Priority: 0 Node: k8s-m02/192.168.177.28 Start Time: Fri, 04 Feb 2022 07:13:50 +0530 Labels: new-label=awesome run=nginx Annotations: <none> Status: Running IP: 10.244.1.6 IPs: IP: 10.244.1.6 Containers: nginx: Container ID: docker://32bfdccc6c984f51a4c4092e4027ff7f1e53dd518938196c95d5427a44a90e40 Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: <none> Host Port: <none> State: Running Started: Fri, 04 Feb 2022 07:13:56 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9sstv (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-9sstv: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18m default-scheduler Successfully assigned default/nginx to k8s-m02 Normal Pulling 18m kubelet Pulling image \"nginx\" Normal Pulled 18m kubelet Successfully pulled image \"nginx\" in 4.864287771s Normal Created 18m kubelet Created container nginx Normal Started 18m kubelet Started container nginx","title":"Describe Resources"},{"location":"Kubernetes/#kubectl-explain","text":"pradeep@learnk8s$ kubectl explain pod.spec KIND: Pod VERSION: v1 RESOURCE: spec <Object> DESCRIPTION: Specification of the desired behavior of the pod. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status PodSpec is a description of a pod. FIELDS: <SNIP> containers <[]Object> -required- List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. <SNIP> nodeName <string> NodeName is a request to schedule this pod onto a specific node. If it is non-empty, the scheduler simply schedules this pod onto that node, assuming that it fits resource requirements. <SNIP>","title":"Kubectl Explain!"},{"location":"Kubernetes/#scheduling","text":"","title":"Scheduling"},{"location":"Kubernetes/#manual-scheduling","text":"Modify the Pod definition file, to includ the nodeName in the spec. pradeep@learnk8s$ cat manual-sched-pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx-manual spec: nodeName: k8s containers: - image: nginx name: nginx pradeep@learnk8s$ kubectl create -f manual-sched-pod.yaml pod/nginx-manual created pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-6c54f77c95-6g7zq 1/1 Running 0 20m demo-6c54f77c95-sb4c9 1/1 Running 0 20m demo-6c54f77c95-w2bsw 1/1 Running 0 20m nginx 1/1 Running 0 25m nginx-manual 1/1 Running 0 48s pradeep@learnk8s$ kubectl get pods -o wide | grep manual nginx-manual 1/1 Running 0 2m9s 10.244.0.5 k8s <none> <none>","title":"Manual Scheduling"},{"location":"Kubernetes/#node-selector","text":"You can constrain a Pod so that it can only run on particular set of Node(s). nodeSelector provides a very simple way to constrain pods to nodes with particular labels. First, let\u2019s label one of the nodes with disktype=ssd . pradeep@learnk8s$ kubectl label nodes k8s-m02 disktype=ssd node/k8s-m02 labeled pradeep@learnk8s$ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s Ready control-plane,master 3d11h v1.23.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s,kubernetes.io/os=linux,minikube.k8s.io/commit=3e64b11ed75e56e4898ea85f96b2e4af0301f43d,minikube.k8s.io/name=k8s,minikube.k8s.io/updated_at=2022_02_07T17_03_56_0700,minikube.k8s.io/version=v1.25.1,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-m02 Ready <none> 4m3s v1.23.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m02,kubernetes.io/os=linux pradeep@learnk8s$ Now, let's create a pod with nodeSelector spec set to the label that we assigned, so that the pod gets scheduled on this node. pradeep@learnk8s$ cat pod-node-selector.yaml apiVersion: v1 kind: Pod metadata: name: nginx-node-selector labels: env: test spec: containers: - name: nginx-node-selector image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd Finally, verify the node on which this pod is running. pradeep@learnk8s$ kubectl get pods -o wide | grep selector nginx-node-selector 1/1 Running 0 17s 10.244.1.2 k8s-m02 <none> <none> As expected, this new pod is running on the node( k8s-m02 ) with the label specified by the nodeSelector pod spec. If you describe this pod, you will see the configured Node-Selectors. pradeep@learnk8s$ kubectl describe pod nginx-node-selector | grep Node-Selectors Node-Selectors: disktype=ssd","title":"Node Selector"},{"location":"Kubernetes/#node-affinity","text":"There are currently two types of node affinity, called requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution . You can think of them as \"hard\" and \"soft\" respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (similar to nodeSelector but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee. The IgnoredDuringExecution part of the names means that, similar to how nodeSelector works, if labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod continues to run on the node. pradeep@learnk8s$ cat pod-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: disktype operator: In values: - ssd containers: - name: with-node-affinity image: nginx pradeep@learnk8s$ kubectl create -f pod-node-affinity.yaml pod/with-node-affinity created pradeep@learnk8s$ kubectl get pods -o wide | grep affinity with-node-affinity 1/1 Running 0 9s 10.244.1.3 k8s-m02 <none> <none>","title":"Node Affinity"},{"location":"Kubernetes/#taints-and-tolerations","text":"Taints allow a node to repel a set of pods. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints. Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. pradeep@learnk8s$ kubectl describe nodes | grep Taint Taints: <none> Taints: <none> pradeep@learnk8s$ kubectl taint nodes k8s key1=value1:NoSchedule node/k8s tainted pradeep@learnk8s$ kubectl describe nodes | grep Taint Taints: key1=value1:NoSchedule Taints: <none> pradeep@learnk8s$ kubectl taint nodes k8s-m02 key2=value2:NoSchedule node/k8s-m02 tainted pradeep@learnk8s$ kubectl describe nodes | grep Taint Taints: key1=value1:NoSchedule Taints: key2=value2:NoSchedule pradeep@learnk8s$ cat pod-toleration.yaml apiVersion: v1 kind: Pod metadata: name: nginx-taint-demo labels: env: test spec: containers: - name: nginx-taint-demo image: nginx imagePullPolicy: IfNotPresent tolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoSchedule\" pradeep@learnk8s$ kubectl create -f pod-toleration.yaml pod/nginx-taint-demo created pradeep@learnk8s$ kubectl get pods -o wide | grep taint-demo nginx-taint-demo 1/1 Running 0 9m3s 10.244.0.15 k8s <none> <none> pradeep@learnk8s$ cat pod-toleration-2.yaml apiVersion: v1 kind: Pod metadata: name: nginx-taint-demo-2 labels: env: test spec: containers: - name: nginx-taint-demo-2 image: nginx imagePullPolicy: IfNotPresent tolerations: - key: \"key2\" operator: \"Equal\" value: \"value2\" effect: \"NoSchedule\" pradeep@learnk8s$ kubectl create -f pod-toleration-2.yaml pod/nginx-taint-demo-2 created pradeep@learnk8s$ kubectl get pods -o wide | grep taint-demo nginx-taint-demo 1/1 Running 0 11m 10.244.0.15 k8s <none> <none> nginx-taint-demo-2 1/1 Running 0 9m35s 10.244.2.5 k8s-m02 <none> <none> pradeep@learnk8s$ cat pod-no-toleration.yaml apiVersion: v1 kind: Pod metadata: name: nginx-no-tolerate labels: env: test spec: containers: - name: nginx-no-tolerate image: nginx imagePullPolicy: IfNotPresent pradeep@learnk8s$ kubectl create -f pod-no-toleration.yaml pod/nginx-no-tolerate created pradeep@learnk8s$ kubectl get pods -o wide | grep no-tolerate nginx-no-tolerate 0/1 Pending 0 10m <none> <none> <none> <none> The Pod got created but is in Pending state, it is not Running yet. Let's find out why?! pradeep@learnk8s$ kubectl describe pods nginx-no-tolerate Name: nginx-no-tolerate Namespace: default Priority: 0 Node: <none> Labels: env=test Annotations: <none> Status: Pending IP: IPs: <none> Containers: nginx-no-tolerate: Image: nginx Port: <none> Host Port: <none> Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6mz6d (ro) Conditions: Type Status PodScheduled False Volumes: kube-api-access-6mz6d: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 43s (x11 over 11m) default-scheduler 0/2 nodes are available: 1 node(s) had taint {key1: value1}, that the pod didn't tolerate, 1 node(s) had taint {key2: value2}, that the pod didn't tolerate. Look at the Reason: FailedScheduling , none of the nodes (0/2) are available because both have some taint that our pod couldn't tolerate! Now, let's delete the Taint on one of the nodes and try creating the Pod again. To untaint a node, just add a - at the end of the taint that you plan to remove. pradeep@learnk8s$ kubectl taint node k8s key1=value1:NoSchedule- node/k8s untainted pradeep@learnk8s$ kubectl describe nodes | grep Taint Taints: <none> Taints: key2=value2:NoSchedule Now the nginx-no-tolerate pod changes to Running state and is scheduled on the k8s node which does not have any Taints at the moment. pradeep@learnk8s$ kubectl get pods -o wide | grep no-tolerate nginx-no-tolerate 1/1 Running 0 16h 10.244.0.16 k8s <none> <none>","title":"Taints and Tolerations"},{"location":"Kubernetes/#resource-limits","text":"Earlier, we have created a namespace called prod . Let's describe it to see if any resource quota applied to it. pradeep@learnk8s$ kubectl describe namespaces prod Name: prod Labels: kubernetes.io/metadata.name=prod Annotations: <none> Status: Active No resource quota. No LimitRange resource. Let's define a quota using the example shown here: pradeep@learnk8s$ kubectl create resourcequota -h Create a resource quota with the specified name, hard limits, and optional scopes. Aliases: quota, resourcequota Examples: # Create a new resource quota named my-quota kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10 # Create a new resource quota named best-effort kubectl create quota best-effort --hard=pods=100 --scopes=BestEffort pradeep@learnk8s$ kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3 -n prod resourcequota/my-quota created pradeep@learnk8s$ kubectl describe ns prod Name: prod Labels: kubernetes.io/metadata.name=prod Annotations: <none> Status: Active Resource Quotas Name: my-quota Resource Used Hard -------- --- --- cpu 0 1 memory 0 1G pods 0 2 services 0 3 No LimitRange resource. pradeep@learnk8s$ kubectl get quota -n prod NAME AGE REQUEST LIMIT my-quota 4m40s cpu: 0/1, memory: 0/1G, pods: 0/2, services: 0/3 pradeep@learnk8s$ kubectl describe quota -n prod Name: my-quota Namespace: prod Resource Used Hard -------- ---- ---- cpu 0 1 memory 0 1G pods 0 2 services 0 3 Let's try create a pod in this namespace and see what happens? pradeep@learnk8s$ kubectl run nginx-without-cpu --image=nginx -n prod Error from server (Forbidden): pods \"nginx-without-cpu\" is forbidden: failed quota: my-quota: must specify cpu,memory pradeep@learnk8s$ Pod creation failed with an error (Forbidden). Let's try create a deployment in this namespace and see what happens? pradeep@learnk8s$ kubectl create deployment test-quota --image=nginx --replicas=3 -n prod deployment.apps/test-quota created pradeep@learnk8s$ kubectl get deploy -n prod NAME READY UP-TO-DATE AVAILABLE AGE test-quota 0/3 0 0 47s pradeep@learnk8s$ kubectl get events -n prod LAST SEEN TYPE REASON OBJECT MESSAGE 104s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-bgsxd\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-w9p4z\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-swwgq\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-tjlsb\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-ll2m9\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-c72mn\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-hqzf4\" is forbidden: failed quota: my-quota: must specify cpu,memory 103s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-bdh4w\" is forbidden: failed quota: my-quota: must specify cpu,memory 102s Warning FailedCreate replicaset/test-quota-666dfd598f Error creating: pods \"test-quota-666dfd598f-gggw2\" is forbidden: failed quota: my-quota: must specify cpu,memory 21s Warning FailedCreate replicaset/test-quota-666dfd598f (combined from similar events): Error creating: pods \"test-quota-666dfd598f-b5rh6\" is forbidden: failed quota: my-quota: must specify cpu,memory 104s Normal ScalingReplicaSet deployment/test-quota Scaled up replica set test-quota-666dfd598f to 3 pradeep@learnk8s$ cat pod-quota.yaml apiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo namespace: prod spec: containers: - name: quota-mem-cpu-demo-ctr image: nginx resources: limits: memory: \"800Mi\" cpu: \"800m\" requests: memory: \"600Mi\" cpu: \"400m\" pradeep@learnk8s$ kubectl create -f pod-quota.yaml pod/quota-mem-cpu-demo created pradeep@learnk8s$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE quota-mem-cpu-demo 1/1 Running 0 9s pradeep@learnk8s$ kubectl describe ns prod Name: prod Labels: kubernetes.io/metadata.name=prod Annotations: <none> Status: Active Resource Quotas Name: my-quota Resource Used Hard -------- --- --- cpu 400m 1 memory 600Mi 1G pods 1 2 services 0 3 No LimitRange resource. pradeep@learnk8s$ cat pod-quota-2.yaml apiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo-2 namespace: prod spec: containers: - name: quota-mem-cpu-demo-ctr-2 image: nginx resources: limits: memory: \"800Mi\" cpu: \"800m\" requests: memory: \"600Mi\" cpu: \"400m\" pradeep@learnk8s$ kubectl create -f pod-quota-2.yaml Error from server (Forbidden): error when creating \"pod-quota-2.yaml\": pods \"quota-mem-cpu-demo-2\" is forbidden: exceeded quota: my-quota, requested: memory=600Mi, used: memory=600Mi, limited: memory=1G","title":"Resource Limits"},{"location":"Kubernetes/#daemon-sets","text":"A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. pradeep@learnk8s$ kubectl get daemonsets.apps -A NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system kindnet 2 2 2 2 2 <none> 5d12h kube-system kube-proxy 2 2 2 2 2 kubernetes.io/os=linux 5d12h As seen above, there are two daemonsets in the kube-system namespace. One is named kindnet and the other is kube-proxy . If you look at the DESIRED column, both of them have a value of 2 which is equal to the number of nodes in this (minikube) cluster. Let's look at those pods now. pradeep@learnk8s$ kubectl describe daemonsets.apps kindnet -n kube-system Name: kindnet Selector: app=kindnet Node-Selector: <none> Labels: app=kindnet k8s-app=kindnet tier=node Annotations: deprecated.daemonset.template.generation: 1 Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=kindnet k8s-app=kindnet tier=node Service Account: kindnet Containers: kindnet-cni: Image: kindest/kindnetd:v20210326-1e038dc5 Port: <none> Host Port: <none> Limits: cpu: 100m memory: 50Mi Requests: cpu: 100m memory: 50Mi Environment: HOST_IP: (v1:status.hostIP) POD_IP: (v1:status.podIP) POD_SUBNET: 10.244.0.0/16 Mounts: /etc/cni/net.d from cni-cfg (rw) /lib/modules from lib-modules (ro) /run/xtables.lock from xtables-lock (rw) Volumes: cni-cfg: Type: HostPath (bare host directory volume) Path: /etc/cni/net.mk HostPathType: DirectoryOrCreate xtables-lock: Type: HostPath (bare host directory volume) Path: /run/xtables.lock HostPathType: FileOrCreate lib-modules: Type: HostPath (bare host directory volume) Path: /lib/modules HostPathType: Events: <none> pradeep@learnk8s$ kubectl describe daemonsets.apps kube-proxy -n kube-system Name: kube-proxy Selector: k8s-app=kube-proxy Node-Selector: kubernetes.io/os=linux Labels: k8s-app=kube-proxy Annotations: deprecated.daemonset.template.generation: 1 Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: k8s-app=kube-proxy Service Account: kube-proxy Containers: kube-proxy: Image: k8s.gcr.io/kube-proxy:v1.23.1 Port: <none> Host Port: <none> Command: /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME) Environment: NODE_NAME: (v1:spec.nodeName) Mounts: /lib/modules from lib-modules (ro) /run/xtables.lock from xtables-lock (rw) /var/lib/kube-proxy from kube-proxy (rw) Volumes: kube-proxy: Type: ConfigMap (a volume populated by a ConfigMap) Name: kube-proxy Optional: false xtables-lock: Type: HostPath (bare host directory volume) Path: /run/xtables.lock HostPathType: FileOrCreate lib-modules: Type: HostPath (bare host directory volume) Path: /lib/modules HostPathType: Priority Class Name: system-node-critical Events: <none> As seen in this output, there are two pods, one pod on each node. The pod names follow the convention of <daemonsetname>-<randomidentifier> . pradeep@learnk8s$ kubectl get pods -A -o wide -o wide| grep kube-proxy kube-system kube-proxy-fszkr 1/1 Running 1 (2d ago) 5d12h 192.168.177.28 k8s-m02 <none> <none> kube-system kube-proxy-m747v 1/1 Running 1 5d12h 192.168.177.27 k8s <none> <none> Similarly, the pods from the other daemonset\u2014kindnet. pradeep@learnk8s$ kubectl get pods -A -o wide -o wide| grep kindnet kube-system kindnet-jpxdd 1/1 Running 1 5d12h 192.168.177.27 k8s <none> <none> kube-system kindnet-p77mb 1/1 Running 1 (2d ago) 5d12h 192.168.177.28 k8s-m02 <none> <none> A simple way to create a DaemonSet is to first generate a YAML file for a Deployment and remove the replicas, strategy fields. Change the kind from Deployment to DaemonSet. Here is an example (modfied from the demodeployment.yaml): pradeep@learnk8s$ cat demodaemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: demo-ds name: demo-ds spec: selector: matchLabels: app: demo-ds template: metadata: labels: app: demo-ds spec: containers: - image: nginx name: nginx-ds resources: {} pradeep@learnk8s$ kubectl create -f demodaemonset.yaml daemonset.apps/demo-ds created pradeep@learnk8s$ kubectl get ds NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE demo-ds 2 2 0 2 0 <none> 6s pradeep@learnk8s$ kubectl get pods -o wide | grep demo-ds demo-ds-gtcf7 1/1 Running 0 28s 10.244.1.5 k8s-m02 <none> <none> demo-ds-kkw4g 1/1 Running 0 28s 10.244.0.11 k8s <none> <none>","title":"Daemon Sets"},{"location":"Kubernetes/#static-pods","text":"Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Unlike Pods that are managed by the control plane; instead, the kubelet watches each static Pod (and restarts it if it fails). The Pod names will be suffixed with the node hostname with a leading hyphen. Manifests are standard Pod definitions in JSON or YAML format in a specific directory. Use the staticPodPath: <the directory> field in the kubelet configuration file, which periodically scans the directory and creates/deletes static Pods as YAML/JSON files appear/disappear there. Let's explore our minikube environment to see if any Static Pods are there. First SSH to the minikube node and search for the kubelet process. The result shows all configuration parameteres used by the kubelet . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ps -aux | grep kubelet root 4244 11.6 4.5 1946788 100436 ? Ssl Feb08 238:40 /var/lib/minikube/binaries/v1.23.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --cni-conf-dir=/etc/cni/net.mk --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=k8s --housekeeping-interval=5m --kubeconfig=/etc/kubernetes/kubelet.conf --network-plugin=cni --node-ip=192.168.177.27 root 412294 15.3 13.5 1042556 296888 ? Ssl 01:40 10:27 kube-apiserver --advertise-address=192.168.177.27 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-account-signing-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key docker 429542 0.0 0.0 3348 448 pts/0 S+ 02:48 0:00 grep kubelet As seen above, the kubelet config settings are present in --config=/var/lib/kubelet/config.yaml . You can explore this file to get the staticPodPath . $ more /var/lib/kubelet/config.yaml | grep static staticPodPath: /etc/kubernetes/manifests Now that we know the staticPodPath , lets see what manifests are currently defined. $ ls /etc/kubernetes/manifests/ etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml $ There are four manifest files defined in this location. Looking at the name of the files, you can relate them to the Kubernetes core components. Let us re-visit the kube-system namespace and get all the pods running there. $ exit logout pradeep@learnk8s$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-64897985d-r9tzv 1/1 Running 8 (114m ago) 6d2h etcd-k8s 1/1 Running 2 (114m ago) 6d2h kindnet-jpxdd 1/1 Running 5 (8h ago) 6d2h kindnet-p77mb 1/1 Running 50 (118m ago) 6d2h kube-apiserver-k8s 1/1 Running 3 (8h ago) 6d2h kube-controller-manager-k8s 1/1 Running 4 (113m ago) 6d2h kube-proxy-fszkr 1/1 Running 1 (2d15h ago) 6d2h kube-proxy-m747v 1/1 Running 1 6d2h kube-scheduler-k8s 1/1 Running 2 (8h ago) 6d2h storage-provisioner 1/1 Running 49 (81m ago) 6d2h Pay special attention to the -k8s string in the name of the Pods: etcd-k8s , kube-apiserver-k8s , kube-controller-manager-k8 , and kube-scheduler-k8s . As mentioned earlier, The StaticPod names will be suffixed with the node hostname ( k8s in our case) with a leading hyphen.","title":"Static Pods"},{"location":"Kubernetes/#create-a-static-pod-in-the-worker-node","text":"Now that we have seen the static pods deployed by the system during cluster setup, let us manually deploy an nginx pod in the second node ( k8s-m02 ) of our minikube cluster k8s . For this first, we need to SSH to that node: k8s-m02 . To do this, add -n k8s-m02 option to the minikube ssh command that we have been using so far. pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a show eth0 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether be:02:3c:97:9d:85 brd ff:ff:ff:ff:ff:ff inet 192.168.177.28/24 brd 192.168.177.255 scope global dynamic eth0 valid_lft 47485sec preferred_lft 47485sec This confirms that we logged in to the k8s-m02 node which has the IP address: 192.168.177.28. Create a new file called nginx.yaml in the staicPodPath location, that is /etc/kubernetes/manifests/ . $ sudo vi /etc/kubernetes/manifests/nginx.yaml $ cat /etc/kubernetes/manifests/nginx.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: nginx name: nginx spec: containers: - image: nginx name: nginx resources: {} dnsPolicy: ClusterFirst restartPolicy: Always $ exit logout By the presence of this file in the k8s-m02 node, the kubelet creates this Pod automatically now. To verify, pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-6c54f77c95-mgz7f 1/1 Running 1 5d22h demo-6c54f77c95-q679r 1/1 Running 1 4d22h demo-6c54f77c95-qqzbf 1/1 Running 1 4d22h demo-6c54f77c95-vjgc2 1/1 Running 1 4d22h demo-6c54f77c95-wv78b 1/1 Running 1 5d22h demo-ds-gtcf7 1/1 Running 0 14h demo-ds-kkw4g 1/1 Running 0 14h nginx-k8s-m02 1/1 Running 0 18s nginx-manual 1/1 Running 1 4d21h nginx-no-tolerate 1/1 Running 1 4d14h nginx-node-selector 1/1 Running 0 2d15h nginx-taint-demo 1/1 Running 1 4d14h with-node-affinity 1/1 Running 0 2d15h Our nginx static pod is successfully Running on k8s-m02 node, as seen above, as pod named nginx-k8s-m02 . Another way to confirm that this Pod is indeed running on the k8s-m02 node is pradeep@learnk8s$ kubectl get pods -o wide | grep m02 demo-ds-gtcf7 1/1 Running 0 14h 10.244.1.5 k8s-m02 <none> <none> nginx-k8s-m02 1/1 Running 0 18m 10.244.1.6 k8s-m02 <none> <none> nginx-node-selector 1/1 Running 0 2d15h 10.244.1.2 k8s-m02 <none> <none> with-node-affinity 1/1 Running 0 2d15h 10.244.1.3 k8s-m02 <none> <none>","title":"Create a Static Pod in the worker node"},{"location":"Kubernetes/#multiple-schedulers","text":"Kubernetes ships with a default scheduler ( kube-scheduler ) that we discussed earlier. If the default scheduler does not suit your needs you can implement your own scheduler. Moreover, you can even run multiple schedulers simultaneously alongside the default scheduler and instruct Kubernetes what scheduler to use for each of your pods. We can use the staticPod manifest file located at /etc/kubernetes/manifests/kube-scheduler.yaml to create our own scheduler. First, let us take a look at the definition of the default scheduler. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ cat /etc/kubernetes/manifests/ etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml $ sudo cat /etc/kubernetes/manifests/kube-scheduler.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: kube-scheduler tier: control-plane name: kube-scheduler namespace: kube-system spec: containers: - command: - kube-scheduler - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=false image: k8s.gcr.io/kube-scheduler:v1.23.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 10259 scheme: HTTPS initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 15 name: kube-scheduler resources: requests: cpu: 100m startupProbe: failureThreshold: 24 httpGet: host: 127.0.0.1 path: /healthz port: 10259 scheme: HTTPS initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 15 volumeMounts: - mountPath: /etc/kubernetes/scheduler.conf name: kubeconfig readOnly: true hostNetwork: true priorityClassName: system-node-critical securityContext: seccompProfile: type: RuntimeDefault volumes: - hostPath: path: /etc/kubernetes/scheduler.conf type: FileOrCreate name: kubeconfig status: {} $ The default scheduler listens on port 10259 , so for our custom scheduler, we have to choose another port, for example 10282 . Here is the definition of the new scheduler called my-scheduler , which is listening on the secure-port 10282 . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ sudo cat /etc/kubernetes/manifests/my-scheduler.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: my-scheduler tier: control-plane name: my-scheduler namespace: kube-system spec: containers: - command: - kube-scheduler - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=false - --secure-port=10282 image: k8s.gcr.io/kube-scheduler:v1.23.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 10282 scheme: HTTPS initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 15 name: my-scheduler resources: requests: cpu: 100m startupProbe: failureThreshold: 24 httpGet: host: 127.0.0.1 path: /healthz port: 10282 scheme: HTTPS initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 15 volumeMounts: - mountPath: /etc/kubernetes/scheduler.conf name: kubeconfig readOnly: true hostNetwork: true priorityClassName: system-node-critical securityContext: seccompProfile: type: RuntimeDefault volumes: - hostPath: path: /etc/kubernetes/scheduler.conf type: FileOrCreate name: kubeconfig status: {} $ Verify that both schedulers kube-scheduler-k8s and my-scheduler-k8s are in running state. pradeep@learnk8s$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-64897985d-r9tzv 1/1 Running 8 (3h2m ago) 6d3h etcd-k8s 1/1 Running 2 (3h2m ago) 6d3h kindnet-jpxdd 1/1 Running 5 (9h ago) 6d3h kindnet-p77mb 1/1 Running 50 (3h6m ago) 6d3h kube-apiserver-k8s 1/1 Running 3 (9h ago) 6d3h kube-controller-manager-k8s 1/1 Running 4 (3h1m ago) 6d3h kube-proxy-fszkr 1/1 Running 1 (2d16h ago) 6d3h kube-proxy-m747v 1/1 Running 1 6d3h kube-scheduler-k8s 1/1 Running 0 14m my-scheduler-k8s 1/1 Running 0 2m14s storage-provisioner 1/1 Running 49 (149m ago) 6d3h Now, it is time to deploy a Pod that makes use of this new scheduler. We just need to add one line for the schedulerName in the Pod spec. pradeep@learnk8s$ cat pod-my-scheduler.yaml apiVersion: v1 kind: Pod metadata: labels: run: nginx-my-scheduler name: nginx-my-scheduler spec: schedulerName: my-scheduler containers: - image: nginx name: nginx-my-scheduler resources: {} pradeep@learnk8s$ kubectl create -f pod-my-scheduler.yaml pod/nginx-my-scheduler created pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE demo-6c54f77c95-mgz7f 1/1 Running 1 6d demo-6c54f77c95-q679r 1/1 Running 1 5d demo-6c54f77c95-qqzbf 1/1 Running 1 5d demo-6c54f77c95-vjgc2 1/1 Running 1 5d demo-6c54f77c95-wv78b 1/1 Running 1 6d demo-ds-gtcf7 1/1 Running 0 16h demo-ds-kkw4g 1/1 Running 0 16h nginx-k8s-m02 1/1 Running 0 137m nginx-manual 1/1 Running 1 4d23h nginx-my-scheduler 0/1 Pending 0 2s nginx-no-tolerate 1/1 Running 1 4d17h nginx-node-selector 1/1 Running 0 2d17h nginx-taint-demo 1/1 Running 1 4d17h with-node-affinity 1/1 Running 0 2d17h :memo: NOTE: There seems to be some issue with this, the Pod is in Pending state, I could not find any events. In the recent Kubernetes version 1.23, there seems to be some changes related to Scheduler definition. I will have to park this aside for some time and continue investigating.","title":"Multiple Schedulers"},{"location":"Kubernetes/#logging-and-monitoring","text":"To monitor cluster components, we need to deploy metrics-server . Metrics Server is a cluster-wide aggregator of resource usage data. Resource usage metrics, such as container CPU and memory usage, are available in Kubernetes through the Metrics API. These metrics can be accessed either directly by the user with the kubectl top command, or by a controller in the cluster, for example Horizontal Pod Autoscaler, to make decisions. Through the Metrics API, you can get the amount of resource currently used by a given node or a given pod. The minikube tool includes a set of built-in addons that can be enabled, disabled and opened in the local Kubernetes environment. pradeep@learnk8s$ minikube addons list -p k8s |-----------------------------|---------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|---------|--------------|--------------------------------| | ambassador | k8s | disabled | third-party (ambassador) | | auto-pause | k8s | disabled | google | | csi-hostpath-driver | k8s | disabled | kubernetes | | dashboard | k8s | disabled | kubernetes | | default-storageclass | k8s | enabled \u2705 | kubernetes | | efk | k8s | disabled | third-party (elastic) | | freshpod | k8s | disabled | google | | gcp-auth | k8s | disabled | google | | gvisor | k8s | disabled | google | | helm-tiller | k8s | disabled | third-party (helm) | | ingress | k8s | disabled | unknown (third-party) | | ingress-dns | k8s | disabled | google | | istio | k8s | disabled | third-party (istio) | | istio-provisioner | k8s | disabled | third-party (istio) | | kubevirt | k8s | disabled | third-party (kubevirt) | | logviewer | k8s | disabled | unknown (third-party) | | metallb | k8s | disabled | third-party (metallb) | | metrics-server | k8s | disabled | kubernetes | | nvidia-driver-installer | k8s | disabled | google | | nvidia-gpu-device-plugin | k8s | disabled | third-party (nvidia) | | olm | k8s | disabled | third-party (operator | | | | | framework) | | pod-security-policy | k8s | disabled | unknown (third-party) | | portainer | k8s | disabled | portainer.io | | registry | k8s | disabled | google | | registry-aliases | k8s | disabled | unknown (third-party) | | registry-creds | k8s | disabled | third-party (upmc enterprises) | | storage-provisioner | k8s | enabled \u2705 | google | | storage-provisioner-gluster | k8s | disabled | unknown (third-party) | | volumesnapshots | k8s | disabled | kubernetes | |-----------------------------|---------|--------------|--------------------------------| \ud83d\udca1 To see addons list for other profiles use: `minikube addons -p name list` Let us enable the metric-server addon now in our cluster. pradeep@learnk8s$ minikube addons enable metrics-server -p k8s \u25aa Using image k8s.gcr.io/metrics-server/metrics-server:v0.4.2 \ud83c\udf1f The 'metrics-server' addon is enabled","title":"Logging and Monitoring"},{"location":"Kubernetes/#verify_2","text":"pradeep@learnk8s$ minikube addons list -p k8s |-----------------------------|---------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|---------|--------------|--------------------------------| | ambassador | k8s | disabled | third-party (ambassador) | | auto-pause | k8s | disabled | google | | csi-hostpath-driver | k8s | disabled | kubernetes | | dashboard | k8s | disabled | kubernetes | | default-storageclass | k8s | enabled \u2705 | kubernetes | | efk | k8s | disabled | third-party (elastic) | | freshpod | k8s | disabled | google | | gcp-auth | k8s | disabled | google | | gvisor | k8s | disabled | google | | helm-tiller | k8s | disabled | third-party (helm) | | ingress | k8s | disabled | unknown (third-party) | | ingress-dns | k8s | disabled | google | | istio | k8s | disabled | third-party (istio) | | istio-provisioner | k8s | disabled | third-party (istio) | | kubevirt | k8s | disabled | third-party (kubevirt) | | logviewer | k8s | disabled | unknown (third-party) | | metallb | k8s | disabled | third-party (metallb) | | metrics-server | k8s | enabled \u2705 | kubernetes | | nvidia-driver-installer | k8s | disabled | google | | nvidia-gpu-device-plugin | k8s | disabled | third-party (nvidia) | | olm | k8s | disabled | third-party (operator | | | | | framework) | | pod-security-policy | k8s | disabled | unknown (third-party) | | portainer | k8s | disabled | portainer.io | | registry | k8s | disabled | google | | registry-aliases | k8s | disabled | unknown (third-party) | | registry-creds | k8s | disabled | third-party (upmc enterprises) | | storage-provisioner | k8s | enabled \u2705 | google | | storage-provisioner-gluster | k8s | disabled | unknown (third-party) | | volumesnapshots | k8s | disabled | kubernetes | |-----------------------------|---------|--------------|--------------------------------| \ud83d\udca1 To see addons list for other profiles use: `minikube addons -p name list` pradeep@learnk8s$ kubectl get pod,svc,rs,deploy -n kube-system NAME READY STATUS RESTARTS AGE pod/coredns-64897985d-r9tzv 1/1 Running 8 (5h34m ago) 6d6h pod/etcd-k8s 1/1 Running 2 (5h34m ago) 6d6h pod/kindnet-jpxdd 1/1 Running 5 (11h ago) 6d6h pod/kindnet-p77mb 1/1 Running 50 (5h38m ago) 6d6h pod/kube-apiserver-k8s 1/1 Running 3 (11h ago) 6d6h pod/kube-controller-manager-k8s 1/1 Running 4 (5h34m ago) 6d6h pod/kube-proxy-fszkr 1/1 Running 1 (2d19h ago) 6d6h pod/kube-proxy-m747v 1/1 Running 1 6d6h pod/kube-scheduler-k8s 1/1 Running 0 166m pod/metrics-server-6b76bd68b6-7nbnx 1/1 Running 0 2m49s pod/my-scheduler-k8s 1/1 Running 0 89m pod/storage-provisioner 1/1 Running 51 (53m ago) 6d6h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 6d6h service/metrics-server ClusterIP 10.106.249.50 <none> 443/TCP 2m49s NAME DESIRED CURRENT READY AGE replicaset.apps/coredns-64897985d 1 1 1 6d6h replicaset.apps/metrics-server-6b76bd68b6 1 1 1 2m50s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/coredns 1/1 1 1 6d6h deployment.apps/metrics-server 1/1 1 1 2m50s As seen above, metric-server is deployed as a Deployment, and exposed as ClusterIP service. Use kubectl top node command to see CPU and memory utilization of each node in the cluster. pradeep@learnk8s$ kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s 334m 16% 1635Mi 76% k8s-m02 86m 4% 981Mi 45% Use kubectl top pod -A command to see CPU and memory utilization of each node in the cluster. pradeep@learnk8s$ kubectl top pod -A NAMESPACE NAME CPU(cores) MEMORY(bytes) default demo-6c54f77c95-mgz7f 0m 3Mi default demo-6c54f77c95-q679r 0m 3Mi default demo-6c54f77c95-qqzbf 0m 3Mi default demo-6c54f77c95-vjgc2 0m 3Mi default demo-6c54f77c95-wv78b 0m 3Mi default demo-ds-gtcf7 0m 3Mi default demo-ds-kkw4g 0m 3Mi default nginx-k8s-m02 0m 3Mi default nginx-manual 0m 3Mi default nginx-no-tolerate 0m 7Mi default nginx-node-selector 0m 9Mi default nginx-taint-demo 0m 3Mi default with-node-affinity 0m 3Mi kube-system coredns-64897985d-r9tzv 0m 12Mi kube-system etcd-k8s 0m 44Mi kube-system kindnet-jpxdd 0m 8Mi kube-system kindnet-p77mb 0m 7Mi kube-system kube-apiserver-k8s 0m 235Mi kube-system kube-controller-manager-k8s 0m 42Mi kube-system kube-proxy-fszkr 0m 16Mi kube-system kube-proxy-m747v 0m 22Mi kube-system kube-scheduler-k8s 0m 14Mi kube-system metrics-server-6b76bd68b6-7nbnx 0m 4Mi kube-system my-scheduler-k8s 0m 13Mi kube-system storage-provisioner 0m 8Mi prod quota-mem-cpu-demo 0m 3Mi You can also use the --sory-by field to display in descending order. pradeep@learnk8s$ kubectl top pod -A --sort-by=memory NAMESPACE NAME CPU(cores) MEMORY(bytes) kube-system kube-apiserver-k8s 0m 235Mi kube-system etcd-k8s 0m 44Mi kube-system kube-controller-manager-k8s 0m 42Mi kube-system kube-proxy-m747v 0m 22Mi kube-system kube-proxy-fszkr 0m 16Mi kube-system kube-scheduler-k8s 0m 14Mi kube-system my-scheduler-k8s 0m 13Mi kube-system coredns-64897985d-r9tzv 0m 12Mi default nginx-node-selector 0m 9Mi kube-system kindnet-jpxdd 0m 8Mi kube-system storage-provisioner 0m 8Mi default nginx-no-tolerate 0m 7Mi kube-system kindnet-p77mb 0m 7Mi kube-system metrics-server-6b76bd68b6-7nbnx 0m 4Mi default demo-ds-kkw4g 0m 3Mi default nginx-taint-demo 0m 3Mi default demo-6c54f77c95-mgz7f 0m 3Mi default nginx-manual 0m 3Mi default with-node-affinity 0m 3Mi default demo-ds-gtcf7 0m 3Mi default demo-6c54f77c95-wv78b 0m 3Mi default demo-6c54f77c95-q679r 0m 3Mi default demo-6c54f77c95-qqzbf 0m 3Mi default demo-6c54f77c95-vjgc2 0m 3Mi default nginx-k8s-m02 0m 3Mi prod quota-mem-cpu-demo 0m 3Mi","title":"Verify"},{"location":"Kubernetes/#examine-pod-logs","text":"Use the kubectl logs <pod> name to look at the logs of the affected container pradeep@learnk8s$ kubectl logs nginx-manual /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh /docker-entrypoint.sh: Configuration complete; ready for start up 2022/02/07 11:34:31 [notice] 1#1: using the \"epoll\" event method 2022/02/07 11:34:31 [notice] 1#1: nginx/1.21.6 2022/02/07 11:34:31 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 2022/02/07 11:34:31 [notice] 1#1: OS: Linux 4.19.202 2022/02/07 11:34:31 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576 2022/02/07 11:34:31 [notice] 1#1: start worker processes 2022/02/07 11:34:31 [notice] 1#1: start worker process 32 2022/02/07 11:34:31 [notice] 1#1: start worker process 33","title":"Examine Pod Logs"},{"location":"Kubernetes/#kubectl-exec","text":"If the container image includes debugging utilities, as is the case with images built from Linux and Windows OS base images, you can run commands inside a specific container with kubectl exec . Here is an example to login to the nginx-manual container and execute commands from the shell mode. pradeep@learnk8s$ kubectl exec -it nginx-manual -- sh # uname -a Linux nginx-manual 4.19.202 #1 SMP Thu Dec 23 10:44:17 UTC 2021 x86_64 GNU/Linux # exit 0","title":"Kubectl exec"},{"location":"Kubernetes/#application-life-cycle-management","text":"I ran into some issue with my cluster and had to rebuild from scratch. Here is how my new cluster looks like: pradeep@learnk8s$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s Ready control-plane,master 4m42s v1.23.1 192.168.177.29 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 k8s-m02 Ready <none> 4m21s v1.23.1 192.168.177.30 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 pradeep@learnk8s$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-64897985d-l9zmg 1/1 Running 0 4m34s kube-system etcd-k8s 1/1 Running 1 4m45s kube-system kindnet-4d4xb 1/1 Running 0 50s kube-system kindnet-z27gn 1/1 Running 0 4m30s kube-system kube-apiserver-k8s 1/1 Running 1 4m45s kube-system kube-controller-manager-k8s 1/1 Running 1 4m45s kube-system kube-proxy-n75w8 1/1 Running 0 4m34s kube-system kube-proxy-v2mq5 1/1 Running 0 4m30s kube-system kube-scheduler-k8s 1/1 Running 1 4m45s kube-system storage-provisioner 1/1 Running 0 4m43s Let us now create a deployment called kodekloud with image webapp-color and four replicas. Here is the Docker image that we are going to use: https://hub.docker.com/r/kodekloud/webapp-color/tags?page=1&ordering=-last_updated. It has many versions of the same image (like v1, v2, and v3). pradeep@learnk8s$ kubectl create deployment kodekloud --image=kodekloud/webapp-color --replicas=4 deployment.apps/kodekloud created pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kodekloud-589c9f4b47-6fbqg 1/1 Running 0 47s 10.244.1.8 k8s-m02 <none> <none> kodekloud-589c9f4b47-c75mz 1/1 Running 0 47s 10.244.1.7 k8s-m02 <none> <none> kodekloud-589c9f4b47-s54tz 1/1 Running 0 47s 10.244.0.7 k8s <none> <none> kodekloud-589c9f4b47-z2sqs 1/1 Running 0 47s 10.244.0.6 k8s <none> <none> pradeep@learnk8s$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 53s Verify the current Image being used by the deployment pradeep@learnk8s$ kubectl describe deployment kodekloud | grep Image Image: kodekloud/webapp-color pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.8:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-589c9f4b47-6fbqg!</h1> </div>$ curl 10.244.1.7:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-589c9f4b47-c75mz!</h1> </div>$ curl 10.244.0.7:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #2980b9;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-589c9f4b47-s54tz!</h1> </div>$ curl 10.244.0.6:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #be2edd;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-589c9f4b47-z2sqs!</h1> </div>$","title":"Application Life Cycle Management"},{"location":"Kubernetes/#rolling-update","text":"Also, find out the strategy used by this deployment pradeep@learnk8s$ kubectl describe deployment kodekloud | grep Strategy StrategyType: RollingUpdate RollingUpdateStrategy: 25% max unavailable, 25% max surge With RollingUpdate strategy, Pods are upgraded few at a time. Now update the deployment to use a different version of the application kodekloud/webapp-color:v2 . For this, we need not delete or re-create the existing deployment. We can edit the deployment and modify the image name. Save with Esc + wq. pradeep@learnk8s$ kubectl edit deployments kodekloud # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" creationTimestamp: \"2022-02-15T07:13:38Z\" generation: 1 labels: app: kodekloud name: kodekloud namespace: default resourceVersion: \"1592\" uid: e5728c54-8768-4d5c-b1b0-c3e33cf62062 spec: progressDeadlineSeconds: 600 replicas: 4 revisionHistoryLimit: 10 selector: matchLabels: app: kodekloud strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: kodekloud spec: containers: - image: kodekloud/webapp-color:v2 imagePullPolicy: Always name: webapp-color resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler -- INSERT -- pradeep@learnk8s$ kubectl edit deployments kodekloud deployment.apps/kodekloud edited The pods should be recreated with new image. Looking at the AGE column, we can conclude that all four pods got re-created few seconds ago. If you notice carefully, the replicates is having a different Identifier now (changed from 589c9f4b47 to 8477b7849). pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE kodekloud-8477b7849-4lgg2 1/1 Running 0 66s kodekloud-8477b7849-b7grh 1/1 Running 0 44s kodekloud-8477b7849-ndf4w 1/1 Running 0 66s kodekloud-8477b7849-ww5qn 1/1 Running 0 47s pradeep@learnk8s$ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 16m If we look at the Events section of this deployment, we can clearly see that the deployment-controller has Scaled Up the new replica set kodekloud-8477b7849 gradually from 1 to 4 and at the same time, old replica set kodekloud-589c9f4b47 got Scaled down from 4 to 0. This is becuase of the RollingUpdateStrategy: 25% max unavailable, 25% max surge . For this current strategy, only one Pod can be down during the upgrade process (25%, meaning 1 out of total 4 pods in the deployment). pradeep@learnk8s$ kubectl describe deployments.apps kodekloud Name: kodekloud Namespace: default CreationTimestamp: Tue, 15 Feb 2022 12:43:38 +0530 Labels: app=kodekloud Annotations: deployment.kubernetes.io/revision: 2 Selector: app=kodekloud Replicas: 4 desired | 4 updated | 4 total | 4 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=kodekloud Containers: webapp-color: Image: kodekloud/webapp-color:v2 Port: <none> Host Port: <none> Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: kodekloud-8477b7849 (4/4 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 23m deployment-controller Scaled up replica set kodekloud-589c9f4b47 to 4 Normal ScalingReplicaSet 8m22s deployment-controller Scaled up replica set kodekloud-8477b7849 to 1 Normal ScalingReplicaSet 8m22s deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 3 Normal ScalingReplicaSet 8m22s deployment-controller Scaled up replica set kodekloud-8477b7849 to 2 Normal ScalingReplicaSet 8m4s deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 2 Normal ScalingReplicaSet 8m3s deployment-controller Scaled up replica set kodekloud-8477b7849 to 3 Normal ScalingReplicaSet 8m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 1 Normal ScalingReplicaSet 8m deployment-controller Scaled up replica set kodekloud-8477b7849 to 4 Normal ScalingReplicaSet 7m55s deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 0 Verify the Image being used by the deployment now. It says, v2 . pradeep@learnk8s$kubectl describe deployment kodekloud| grep Image Image: kodekloud/webapp-color:v2 Let's collect the new IP address of the Pods and verify the application from the minikube node. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kodekloud-8477b7849-4lgg2 1/1 Running 0 4m2s 10.244.0.8 k8s <none> <none> kodekloud-8477b7849-b7grh 1/1 Running 0 3m40s 10.244.0.9 k8s <none> <none> kodekloud-8477b7849-ndf4w 1/1 Running 0 4m2s 10.244.1.9 k8s-m02 <none> <none> kodekloud-8477b7849-ww5qn 1/1 Running 0 3m43s 10.244.1.10 k8s-m02 <none> <none> As seen here, the application is now running the v2 version as indicated by the new headers: <h2> Application Version: v2 </h2> pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.0.8:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-8477b7849-4lgg2!</h1> <h2> Application Version: v2 </h2> </div>$ curl 10.244.0.9:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-8477b7849-b7grh!</h1> <h2> Application Version: v2 </h2> </div>$ curl 10.244.1.9:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-8477b7849-ndf4w!</h1> <h2> Application Version: v2 </h2> </div>$ curl 10.244.1.10:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-8477b7849-ww5qn!</h1> <h2> Application Version: v2 </h2> </div>$ Another test is to verify the color of the web-app, it says green . $ curl 10.244.0.8:8080/color green$ curl 10.244.0.9:8080/color green$ curl 10.244.1.9:8080/color green$ curl 10.244.1.10:8080/color green$","title":"Rolling Update"},{"location":"Kubernetes/#recreate","text":"There is another deployment strategy called Recreate . Let us edit the deployment again to use another version, v3, this time and change the strategy from RollingUpdate to Recreate . pradeep@learnk8s$ kubectl edit deployment kodekloud deployment.apps/kodekloud edited # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"2\" creationTimestamp: \"2022-02-15T07:13:38Z\" generation: 2 labels: app: kodekloud name: kodekloud namespace: default resourceVersion: \"2408\" uid: e5728c54-8768-4d5c-b1b0-c3e33cf62062 spec: progressDeadlineSeconds: 600 replicas: 4 revisionHistoryLimit: 10 selector: matchLabels: app: kodekloud strategy: type: Recreate template: metadata: creationTimestamp: null labels: app: kodekloud spec: containers: - image: kodekloud/webapp-color:v3 imagePullPolicy: Always name: webapp-color resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: -- INSERT -- pradeep@learnk8s$ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 33m pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kodekloud-676c6b9fcd-6bm6z 1/1 Running 0 53s 10.244.1.12 k8s-m02 <none> <none> kodekloud-676c6b9fcd-dggk7 1/1 Running 0 53s 10.244.0.10 k8s <none> <none> kodekloud-676c6b9fcd-m2q7x 1/1 Running 0 53s 10.244.0.11 k8s <none> <none> kodekloud-676c6b9fcd-p8tnl 1/1 Running 0 53s 10.244.1.11 k8s-m02 <none> <none> If we look at the latest events of the deployment, the deployment-controller Scaled down replica set kodekloud-8477b7849 to 0 at a time and Scaled up new replica set kodekloud-676c6b9fcd to 4 . pradeep@learnk8s$ kubectl describe deployments.apps Name: kodekloud Namespace: default CreationTimestamp: Tue, 15 Feb 2022 12:43:38 +0530 Labels: app=kodekloud Annotations: deployment.kubernetes.io/revision: 3 Selector: app=kodekloud Replicas: 4 desired | 4 updated | 4 total | 4 available | 0 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=kodekloud Containers: webapp-color: Image: kodekloud/webapp-color:v3 Port: <none> Host Port: <none> Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: kodekloud-676c6b9fcd (4/4 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 35m deployment-controller Scaled up replica set kodekloud-589c9f4b47 to 4 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set kodekloud-8477b7849 to 1 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 3 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set kodekloud-8477b7849 to 2 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 2 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set kodekloud-8477b7849 to 3 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 1 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set kodekloud-8477b7849 to 4 Normal ScalingReplicaSet 19m deployment-controller Scaled down replica set kodekloud-589c9f4b47 to 0 Normal ScalingReplicaSet 2m58s deployment-controller Scaled down replica set kodekloud-8477b7849 to 0 Normal ScalingReplicaSet 2m27s deployment-controller Scaled up replica set kodekloud-676c6b9fcd to 4 Just to verify, let us connect to one of the pods from the new version and check the output. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.12:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #e74c3c;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-676c6b9fcd-6bm6z!</h1> <h2> Application Version: v3 </h2> </div>$ From this, it is clear that the application got upgraded to latest version. Also, there is another test to verify the color used by the web-app. All four pods returned red . $ curl 10.244.1.12:8080/color red$ curl 10.244.1.11:8080/color red$ curl 10.244.0.11:8080/color red$ curl 10.244.0.10:8080/color red$","title":"Recreate"},{"location":"Kubernetes/#commands-and-arguments","text":"Let us make use of help, to understand the usage of commands and arguments. As shown here, with the help of --command option we can pass a different command and custom arguments to our container. pradeep@learnk8s$ kubectl run -h Create and run a particular image in a pod. Examples: <SNIP> # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command kubectl run nginx --image=nginx -- <arg1> <arg2> ... <argN> # Start the nginx pod using a different command and custom arguments kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN> Options: <SNIP> As an example, let us try to create a Ubuntu container that just prints date. pradeep@learnk8s$ kubectl run ubuntu-date --image=ubuntu --command date pod/ubuntu-date created Look at the Command header under Containers section. pradeep@learnk8s$ kubectl describe pods ubuntu-date Name: ubuntu-date Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Tue, 15 Feb 2022 13:55:40 +0530 Labels: run=ubuntu-date Annotations: <none> Status: Running IP: 10.244.1.21 IPs: IP: 10.244.1.21 Containers: ubuntu-date: Container ID: docker://3c6c897fe4ad67e0a90540ec086901eb63a5f8ab1d4afae85b7d0a734e363a9c Image: ubuntu Image ID: docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be Port: <none> Host Port: <none> Command: date State: Terminated Reason: Completed Exit Code: 0 Started: Tue, 15 Feb 2022 13:55:45 +0530 Finished: Tue, 15 Feb 2022 13:55:45 +0530 Ready: False Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-87bh2 (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: kube-api-access-87bh2: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9s default-scheduler Successfully assigned default/ubuntu-date to k8s-m02 Normal Pulled 4s kubelet Successfully pulled image \"ubuntu\" in 4.705982332s Normal Created 4s kubelet Created container ubuntu-date Normal Started 4s kubelet Started container ubuntu-date Normal Pulling 3s (x2 over 8s) kubelet Pulling image \"ubuntu\" The pod is Terminated with status Completed . pradeep@learnk8s$ kubectl get pods | grep date ubuntu-date 0/1 CrashLoopBackOff 3 (55s ago) 111s Let us create another container that uses an argument as well, in addition to the command. pradeep@learnk8s$ kubectl run ubuntu-sleep --image=ubuntu --command sleep 5000 pod/ubuntu-sleep created pradeep@learnk8s$ kubectl describe pods ubuntu-sleep Name: ubuntu-sleep Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Tue, 15 Feb 2022 13:59:46 +0530 Labels: run=ubuntu-sleep Annotations: <none> Status: Running IP: 10.244.1.22 IPs: IP: 10.244.1.22 Containers: ubuntu-sleep: Container ID: docker://4a12725e6951fd785eadb5bb692b25755ecfac7c664d2eb97305f46369f9d898 Image: ubuntu Image ID: docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be Port: <none> Host Port: <none> Command: sleep 5000 State: Running Started: Tue, 15 Feb 2022 13:59:56 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gpjg5 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-gpjg5: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 26s default-scheduler Successfully assigned default/ubuntu-sleep to k8s-m02 Normal Pulling 25s kubelet Pulling image \"ubuntu\" Normal Pulled 17s kubelet Successfully pulled image \"ubuntu\" in 8.387291792s Normal Created 17s kubelet Created container ubuntu-sleep Normal Started 16s kubelet Started container ubuntu-sleep Let us take a look at the definition of this Pod with command and arguments in YAML. pradeep@learnk8s$ kubectl get pods ubuntu-sleep -o yaml > pod-command-arg.yaml pradeep@learnk8s$ cat pod-command-arg.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: \"2022-02-15T08:29:46Z\" labels: run: ubuntu-sleep name: ubuntu-sleep namespace: default resourceVersion: \"5745\" uid: 971c6c08-da5e-4360-901b-7b8ca65a693a spec: containers: - command: - sleep - \"5000\" image: ubuntu imagePullPolicy: Always name: ubuntu-sleep resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-gpjg5 readOnly: true dnsPolicy: ClusterFirst <SNIP> Now let us use the same kodecloud/web-app image and try to pass the color argument while initializing the pod. Looking at the source code of this container kodekloudhub/webapp-color , we can find out that, A color can be specified in two ways: either as a command line argument or as an environment variable. from flask import Flask from flask import render_template import socket import random import os import argparse app = Flask(__name__) color_codes = { \"red\": \"#e74c3c\", \"green\": \"#16a085\", \"blue\": \"#2980b9\", \"blue2\": \"#30336b\", \"pink\": \"#be2edd\", \"darkblue\": \"#130f40\" } SUPPORTED_COLORS = \",\".join(color_codes.keys()) # Get color from Environment variable COLOR_FROM_ENV = os.environ.get('APP_COLOR') # Generate a random color COLOR = random.choice([\"red\", \"green\", \"blue\", \"blue2\", \"darkblue\", \"pink\"]) @app.route(\"/\") def main(): # return 'Hello' return render_template('hello.html', name=socket.gethostname(), color=color_codes[COLOR]) if __name__ == \"__main__\": print(\" This is a sample web application that displays a colored background. \\n\" \" A color can be specified in two ways. \\n\" \"\\n\" \" 1. As a command line argument with --color as the argument. Accepts one of \" + SUPPORTED_COLORS + \" \\n\" \" 2. As an Environment variable APP_COLOR. Accepts one of \" + SUPPORTED_COLORS + \" \\n\" \" 3. If none of the above then a random color is picked from the above list. \\n\" \" Note: Command line argument precedes over environment variable.\\n\" \"\\n\" \"\") # Check for Command Line Parameters for color parser = argparse.ArgumentParser() parser.add_argument('--color', required=False) args = parser.parse_args() if args.color: print(\"Color from command line argument =\" + args.color) COLOR = args.color if COLOR_FROM_ENV: print(\"A color was set through environment variable -\" + COLOR_FROM_ENV + \". However, color from command line argument takes precendence.\") elif COLOR_FROM_ENV: print(\"No Command line argument. Color from environment variable =\" + COLOR_FROM_ENV) COLOR = COLOR_FROM_ENV else: print(\"No command line argument or environment variable. Picking a Random Color =\" + COLOR) # Check if input color is a supported one if COLOR not in color_codes: print(\"Color not supported. Received '\" + COLOR + \"' expected one of \" + SUPPORTED_COLORS) exit(1) # Run Flask Application app.run(host=\"0.0.0.0\", port=8080) Let us try the first method that we just discussed: as a command argument. Create a pod using kodekloud/webapp-color:v3 but pass the --color argument to change the default color from red to blue . Earlier, when we ran this image (with defaults), the color was red . This can also be confirmed by looking at the Dockerfile definition. pradeep@learnk8s$ kubectl run kodekloud-change-color --image=kodekloud/webapp-color:v3 --dry-run=client -o yaml -- \"--color\" \"blue\" > pod-change-color.yaml Now we can see we are passing the --color and blue as arguements to this container. pradeep@learnk8s$ cat pod-change-color.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: kodekloud-change-color name: kodekloud-change-color spec: containers: - args: - --color - blue image: kodekloud/webapp-color:v3 name: kodekloud-change-color resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} Let us create a pod from this definition file. pradeep@learnk8s$ kubectl create -f pod-change-color.yaml pod/kodekloud-change-color created pradeep@learnk8s$ kubectl describe pods kodekloud-change-color Name: kodekloud-change-color Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Tue, 15 Feb 2022 16:31:54 +0530 Labels: run=kodekloud-change-color Annotations: <none> Status: Running IP: 10.244.1.29 IPs: IP: 10.244.1.29 Containers: kodekloud-change-color: Container ID: docker://3c6bf9642cb90fcc2f969a3043f3879d9a42917d7e15ed26905b2a340461609e Image: kodekloud/webapp-color:v3 Image ID: docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177 Port: <none> Host Port: <none> Args: --color blue State: Running Started: Tue, 15 Feb 2022 16:31:57 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6l7n4 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-6l7n4: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned default/kodekloud-change-color to k8s-m02 Normal Pulled 10s kubelet Container image \"kodekloud/webapp-color:v3\" already present on machine Normal Created 10s kubelet Created container kodekloud-change-color Normal Started 9s kubelet Started container kodekloud-change-color pradeep@learnk8s$ kubectl get pods -o wide | grep color kodekloud-change-color 1/1 Running 0 40s 10.244.1.29 k8s-m02 <none> <none> Verify the color of the webapp now, it should say blue instead of the default red . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.29:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #2980b9;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-change-color!</h1> <h2> Application Version: v3 </h2> </div>$ $ curl 10.244.1.29:8080/color blue$ exit logout It is confirmed the arguments that we passed worked! (\"background: #2980b9 indicates blue, as defined in the color_code, \"blue\": \"#2980b9\").","title":"Commands and Arguments"},{"location":"Kubernetes/#environment-variables","text":"Now, it is time to try the other metod of passing arguments, via environment variables. From the kubectl run examples, we can see that environment variables can be set with the --env option. pradeep@learnk8s$ kubectl run -h Create and run a particular image in a pod. Examples: <SNIP> # Start a hazelcast pod and set environment variables \"DNS_DOMAIN=cluster\" and \"POD_NAMESPACE=default\" in the container kubectl run hazelcast --image=hazelcast/hazelcast --env=\"DNS_DOMAIN=cluster\" --env=\"POD_NAMESPACE=default\" <SNIP> Let us redploy the same image kodekloud/webapp-color:v3 but this time changing the color of the app with environment variable. pradeep@learnk8s$ kubectl run kodekloud-env-color --image=kodekloud/webapp-color:v3 --env=APP_COLOR=pink pod/kodekloud-env-color created pradeep@learnk8s$ kubectl get pods -o wide | grep env kodekloud-env-color 1/1 Running 0 12s 10.244.1.30 k8s-m02 <none> <none> pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.30:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #be2edd;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-env-color!</h1> <h2> Application Version: v3 </h2> </div>$ curl 10.244.1.30:8080/color pink$ $ exit logout The curl tests confirm that the application is using the pink color now, which is passed with the environment variable, APP_COLOR . If we look at the Pod description, Environment section, APP_COLOR: pink is seen as expected. pradeep@learnk8s$ kubectl describe pod kodekloud-env-color Name: kodekloud-env-color Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Tue, 15 Feb 2022 19:22:07 +0530 Labels: run=kodekloud-env-color Annotations: <none> Status: Running IP: 10.244.1.30 IPs: IP: 10.244.1.30 Containers: kodekloud-env-color: Container ID: docker://376016abd3ab8f65faa794cb746831f37c3ac16c0436598b08f9365bdd6176d4 Image: kodekloud/webapp-color:v3 Image ID: docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177 Port: <none> Host Port: <none> State: Running Started: Tue, 15 Feb 2022 19:22:08 +0530 Ready: True Restart Count: 0 Environment: APP_COLOR: pink Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xj9jz (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-xj9jz: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m26s default-scheduler Successfully assigned default/kodekloud-env-color to k8s-m02 Normal Pulled 2m25s kubelet Container image \"kodekloud/webapp-color:v3\" already present on machine Normal Created 2m25s kubelet Created container kodekloud-env-color Normal Started 2m25s kubelet Started container kodekloud-env-color Save the current running pod as a YAML file. pradeep@learnk8s$ kubectl get pods kodekloud-env-color -o yaml > pod-env-variable.yaml As this is a running Pod, the YAML output shows a lot of other details on Status, which is not required for now (to change the environment variable). pradeep@learnk8s$ cat pod-env-variable.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: \"2022-02-15T13:52:07Z\" labels: run: kodekloud-env-color name: kodekloud-env-color namespace: default resourceVersion: \"15077\" uid: bd7c01ae-553e-4f77-b589-6af6560dcc88 spec: containers: - env: - name: APP_COLOR value: pink image: kodekloud/webapp-color:v3 imagePullPolicy: IfNotPresent name: kodekloud-env-color resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-xj9jz readOnly: true dnsPolicy: ClusterFirst <SNIP> Create another copy of this YAML file and modify the definition to change the color fo the App to darkblue . pradeep@learnk8s$ cp pod-env-variable.yaml pod-env-variable-2.yaml pradeep@learnk8s$ vi pod-env-variable-2.yaml The modified version looks like this. pradeep@learnk8s$ cat pod-env-variable-2.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: \"2022-02-15T13:52:07Z\" labels: run: kodekloud-env-color-2 name: kodekloud-env-color-2 namespace: default spec: containers: - env: - name: APP_COLOR value: darkblue image: kodekloud/webapp-color:v3 imagePullPolicy: IfNotPresent name: kodekloud-env-color-2 resources: {} pradeep@learnk8s$ kubectl create -f pod-env-variable-2.yaml pod/kodekloud-env-color-2 created pradeep@learnk8s$ kubectl get pods -o wide | grep env kodekloud-env-color 1/1 Running 0 14m 10.244.1.30 k8s-m02 <none> <none> kodekloud-env-color-2 1/1 Running 0 16s 10.244.1.31 k8s-m02 <none> <none> We can verify that this new pod is using the darkblue color. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.31:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #130f40;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-env-color-2!</h1> <h2> Application Version: v3 </h2> </div>$ curl 10.244.1.31:8080/color darkblue$ $ curl 10.244.1.31:8080/color darkblue$ $ exit logout","title":"Environment Variables"},{"location":"Kubernetes/#configmaps","text":"As seen in the explanation, ConfigMaps hold the configuration data for pods to consume. For example, we could store the color of the Webapp from the previous example, in the form of a ConfigMap. pradeep@learnk8s$ kubectl explain configmaps KIND: ConfigMap VERSION: v1 DESCRIPTION: ConfigMap holds configuration data for pods to consume. Before we create any configmaps, let us verify if we have any configmaps already in our cluster. pradeep@learnk8s$ kubectl get configmaps NAME DATA AGE kube-root-ca.crt 1 7h16m There is one configmap named kube-root-ca.crt already present with one DATA. To see what is stored in this configmap, we can describe it and look at the Annotations. pradeep@learnk8s$ kubectl describe configmaps Name: kube-root-ca.crt Namespace: default Labels: <none> Annotations: kubernetes.io/description: Contains a CA bundle that can be used to verify the kube-apiserver when using internal endpoints such as the internal service IP or kubern... Data ==== ca.crt: ---- -----BEGIN CERTIFICATE----- MIIDBjCCAe6gAwIBAgIBATANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p a3ViZUNBMB4XDTIxMDYyODA0NTM1NloXDTMxMDYyNzA0NTM1NlowFTETMBEGA1UE AxMKbWluaWt1YmVDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKZu VYaX2I0BbQ7le2fCQiYL0UaoadaYm0UtRoYHCkGovRH4MBJlJipYp7IVZ+4oOgqZ VtBC2o9oRzXnWEM6pJyTsrMrEhtDyTxAeCLY5iVwkLRCxrqxvsCVeGsTaF+0SMCy PUXFaC20jrDVAxAUXs26Le0Fl2BjOv9k9K2hEScRuC0ogkF/oL+aC3BoGpdFmPVG S+R18kS1UyBGpUWkktyXhtEAmZVdHn+PGeMW2W3cuHyF4OEqQiXE5xcXt6TQG64F qgmE5xISVDp1/6VjlJeiVMqlyqZVm7dEKdtwcx6p8UKqnr7nL14STkzv6pDywk5v egCFa7sP+amk41K6VMECAwEAAaNhMF8wDgYDVR0PAQH/BAQDAgKkMB0GA1UdJQQW MBQGCCsGAQUFBwMCBggrBgEFBQcDATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQW BBSL+jGni3bexvU9wL8lBR0FeLmCQDANBgkqhkiG9w0BAQsFAAOCAQEAd/sJdAwa 6hOYM52vQj0CiYfNqFAs1rK7klxzpPVa07sYIO8n8FjyzGTead+FxzeuxnOBxibu y/447esB/RPE3d7hv+piQqT9FD7H/lskpHyIvffz4ai15P3DFtKQeY464bxVnynQ eK4dvzySrqxDs5Q5mMC1PQn9ap7VvIRnz1wEr6hlHMqJ31G58rmnZ5V+eJBBUYvG a39/4Q1PIMyk98T6QmJDTuLQngD8QJagcRMm4D2mzdieomt9jmIsrd5jINMhsC6d pL/PMobEfOxGsZxSKW9RC4/mabcuo+Dty+xAN/cYlOrq6zGSatvQXu/60iDSx8un D9XARLPvbRrYnA== -----END CERTIFICATE----- BinaryData ==== Events: <none> We can create configmaps in multiple different ways. Let us use the help option to see some examples. pradeep@learnk8s$ kubectl create configmap -h Create a config map based on a file, directory, or specified literal value. A single config map may package one or more key/value pairs. When creating a config map based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key, you may specify an alternate key. When creating a config map based on a directory, each file whose basename is a valid key in the directory will be packaged into the config map. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc). Aliases: configmap, cm Examples: # Create a new config map named my-config based on folder bar kubectl create configmap my-config --from-file=path/to/bar # Create a new config map named my-config with specified keys instead of file basenames on disk kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt # Create a new config map named my-config with key1=config1 and key2=config2 kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2 # Create a new config map named my-config from the key=value pairs in the file kubectl create configmap my-config --from-file=path/to/bar # Create a new config map named my-config from an env file kubectl create configmap my-config --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env <SNIP> Using this examples, let us create a configmap named webapp-color using --from-literal option to set the color to green . pradeep@learnk8s$ kubectl create configmap webapp-color --from-literal=APP_COLOR=green configmap/webapp-color created pradeep@learnk8s$ kubectl get cm NAME DATA AGE kube-root-ca.crt 1 7h26m webapp-color 1 5s pradeep@learnk8s$ kubectl describe cm webapp-color Name: webapp-color Namespace: default Labels: <none> Annotations: <none> Data ==== APP_COLOR: ---- green BinaryData ==== Events: <none> To use this configmap in a Pod definition, we need to make use of -envFrom and -configMapRef options in the Pod Spec. pradeep@learnk8s$ cat pod-config-map.yaml apiVersion: v1 kind: Pod metadata: name: kodekloud-cm spec: containers: - name: kodekloud-cm image: kodekloud/webapp-color:v3 envFrom: - configMapRef: name: webapp-color restartPolicy: Never pradeep@learnk8s$ kubectl create -f pod-config-map.yaml pod/kodekloud-cm created pradeep@learnk8s$ kubectl get pods -o wide | grep cm kodekloud-cm 1/1 Running 0 106s 10.244.0.14 k8s <none> <none> Verify that app color is changed to green . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.0.14:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #16a085;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-cm!</h1> <h2> Application Version: v3 </h2> </div>$ curl 10.244.0.14:8080/color green$ $ exit logout If we describe this pod, we can see the following section. Environment Variables from: webapp-color ConfigMap Optional: false pradeep@learnk8s$ kubectl describe pod kodekloud-cm Name: kodekloud-cm Namespace: default Priority: 0 Node: k8s/192.168.177.29 Start Time: Tue, 15 Feb 2022 20:09:51 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.0.14 IPs: IP: 10.244.0.14 Containers: kodekloud-cm: Container ID: docker://53b0a746b4c95c65bbdce09f416ca6762ec9ce78d2a9618187a86d80465ec349 Image: kodekloud/webapp-color:v3 Image ID: docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177 Port: <none> Host Port: <none> State: Running Started: Tue, 15 Feb 2022 20:09:53 +0530 Ready: True Restart Count: 0 Environment Variables from: webapp-color ConfigMap Optional: false Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sq9q4 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-sq9q4: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 10h default-scheduler Successfully assigned default/kodekloud-cm to k8s Normal Pulled 10h kubelet Container image \"kodekloud/webapp-color:v3\" already present on machine Normal Created 10h kubelet Created container kodekloud-cm Normal Started 10h kubelet Started container kodekloud-cm Warning NodeNotReady 9h (x2 over 10h) node-controller Node is not ready","title":"ConfigMaps"},{"location":"Kubernetes/#secrets","text":"Secret holds secret data of a certain type. pradeep@learnk8s$ kubectl explain secrets KIND: Secret VERSION: v1 DESCRIPTION: Secret holds secret data of a certain type. The total bytes of the values in the Data field must be less than MaxSecretSize bytes. <SNIP> Verify if there are any secrets present in our cluster already! pradeep@learnk8s$ kubectl get secrets NAME TYPE DATA AGE default-token-b2xs6 kubernetes.io/service-account-token 3 17h There is one secret of type service-account-token . Describe it to see additional details of the secret. pradeep@learnk8s$ kubectl describe secrets Name: default-token-b2xs6 Namespace: default Labels: <none> Annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 5500622d-56e8-47c9-9440-7882c1d35512 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1111 bytes namespace: 7 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IkJGbThhZWVoY01TV3VDZ2hMU1RMenFlY0o2ckdsY2N3ZjB2ZFl5QWEtQm8ifQ.eyJpc3MiOiJrdWJlck5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VddC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tYjJ4czYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMjaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjU1MDA2MjJkLTU2ZTgtNDdjOS05NDQwLTc4ODJjMWQzNTUxMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.NGKbj5mw1EsZm4E14zDDpS2vNgL_L0WuGgP7Ex6k2TrXqKaO4wxX-ca3US1iSaUeYOIM4DLnXjDbXbP5J6YV-8ke--Yxhkks8iC_3tA9k1Q0YvK0RXS0T4WsL9i12sD44i-9LoJpL4Zpu3qJO-s4V5Plg9ifC69NpKoEu3CoOTMGmOX7DqDmGotogl5BJHflZekweX8GMOGP5WAv1FkUcROWyhj8wVMEFOzZIS8O8ssF67wHukWuUZ3IYDpCHy3QjQSKOWgeGGfFSDUXfW_mEQCE1ryl_c_-3TK7kGtlqN4l8aKrd2iBF-1MP4mdbq9WKDGBJDlZlGbHgPOZe2eBMw How to create secrets? Secrets can be of three types in Kubernetes: Docker Registry, Generic TLS. pradeep@learnk8s$ kubectl create secret -h Create a secret using specified subcommand. Available Commands: docker-registry Create a secret for use with a Docker registry generic Create a secret from a local file, directory, or literal value tls Create a TLS secret Usage: kubectl create secret [flags] [options] Use \"kubectl <command> --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). pradeep@learnk8s$ kubectl create secret generic -h Create a secret based on a file, directory, or specified literal value. A single secret may package one or more key/value pairs. When creating a secret based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key or you wish to chose your own, you may specify an alternate key. When creating a secret based on a directory, each file whose basename is a valid key in the directory will be packaged into the secret. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc). Examples: # Create a new secret named my-secret with keys for each file in folder bar kubectl create secret generic my-secret --from-file=path/to/bar # Create a new secret named my-secret with specified keys instead of names on disk kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub # Create a new secret named my-secret with key1=supersecret and key2=topsecret kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret # Create a new secret named my-secret using a combination of a file and a literal kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret # Create a new secret named my-secret from env files kubectl create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env <SNIP> Create a secret called webapp-color-secret and store the webapp color blue in it. pradeep@learnk8s$ kubectl create secret generic webapp-color-secret --from-literal=APP_COLOR=blue secret/webapp-color-secret created pradeep@learnk8s$ kubectl get secrets NAME TYPE DATA AGE default-token-b2xs6 kubernetes.io/service-account-token 3 18h webapp-color-secret Opaque 1 5s pradeep@learnk8s$ kubectl describe secrets webapp-color-secret Name: webapp-color-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== APP_COLOR: 4 bytes It is time to use this secret in a Pod definition. We use the secrets in the same way we used configmaps. Instead of configMapRef , it would be secretRef now in the envFrom section. pradeep@learnk8s$ cat pod-secret.yaml apiVersion: v1 kind: Pod metadata: name: kodekloud-secret spec: containers: - name: kodekloud-secret image: kodekloud/webapp-color:v3 envFrom: - secretRef: name: webapp-color-secret restartPolicy: Never pradeep@learnk8s$ kubectl create -f pod-secret.yaml pod/kodekloud-secret created pradeep@learnk8s$ kubectl get pods -o wide| grep secret kodekloud-secret 1/1 Running 0 114s 10.244.1.32 k8s-m02 <none> <none> To verify the color, login to the minikube node. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.1.32:8080 <!doctype html> <title>Hello from Flask</title> <body style=\"background: #2980b9;\"></body> <div style=\"color: #e4e4e4; text-align: center; height: 90px; vertical-align: middle;\"> <h1>Hello from kodekloud-secret!</h1> <h2> Application Version: v3 </h2> </div>$ curl 10.244.1.32:8080/color blue$ exit logout We can see that the app is using the blue color as defined in the secret. As another final confirmation, we can describe this pod, to see Environment Variables from: webapp-color-secret Secret Optional: false pradeep@learnk8s$ kubectl describe pods kodekloud-secret Name: kodekloud-secret Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Wed, 16 Feb 2022 06:47:47 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.32 IPs: IP: 10.244.1.32 Containers: kodekloud-secret: Container ID: docker://799356d294bd0e1e6d37143ae8b12babcd9d50410d879ed099032dd4ceaec68a Image: kodekloud/webapp-color:v3 Image ID: docker-pullable://kodekloud/webapp-color@sha256:3ecd19b1b85db381a0b6f78272458c3c274ac2a38e878d65700393899adb3177 Port: <none> Host Port: <none> State: Running Started: Wed, 16 Feb 2022 06:47:49 +0530 Ready: True Restart Count: 0 Environment Variables from: webapp-color-secret Secret Optional: false Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xw2ml (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-xw2ml: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned default/kodekloud-secret to k8s-m02 Normal Pulled 17s kubelet Container image \"kodekloud/webapp-color:v3\" already present on machine Normal Created 16s kubelet Created container kodekloud-secret Normal Started 16s kubelet Started container kodekloud-secret","title":"Secrets"},{"location":"Kubernetes/#multi-container-pods","text":"As per Kubernetes documentation, The primary reason that Pods can have multiple containers is to support helper applications that assist a primary application. Typical examples of helper applications are data pullers, data pushers, and proxies. Helper and primary applications often need to communicate with each other. Typically this is done through a shared filesystem, as shown in this example. We have not yet discussed volumes, but for now, it is sufficient to know that it is a storage component. pradeep@learnk8s$ cat multi-container-pod.yaml apiVersion: v1 kind: Pod metadata: name: multi-containers spec: restartPolicy: Never volumes: - name: common-data emptyDir: {} containers: - name: nginx image: nginx volumeMounts: - name: common-data mountPath: /usr/share/nginx/html - name: ubuntu image: ubuntu volumeMounts: - name: common-data mountPath: /ubuntu-data command: [\"/bin/sh\"] args: [\"-c\", \"echo Hello from the Ubuntu container which is visible in the nginx container as they are sharing the same storage common-data > /ubuntu-data/index.html\"] pradeep@learnk8s$ kubectl create -f multi-container-pod.yaml pod/multi-containers created pradeep@learnk8s$ kubectl get pods -o wide | grep multi multi-containers 1/2 NotReady 0 8m3s 10.244.1.34 k8s-m02 <none> <none> If you notice, though we have defined two containers (nginx and ubuntu) in this Pod definition, there is only a single IP (10.244.1.34 in this case). This is important to understand. All containers in a pod share the same network resources. Also, READY column shows 1/2 meaning, there are two containers but only one is Running. Let us find out more on this by describing it. pradeep@learnk8s$ kubectl describe pods multi-containers Name: multi-containers Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Wed, 16 Feb 2022 07:19:50 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.34 IPs: IP: 10.244.1.34 Containers: nginx: Container ID: docker://1cb2560ba88bf7f5a100c74246f10adecf6ce026d9f6031e43947c32ba021d3c Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: <none> Host Port: <none> State: Running Started: Wed, 16 Feb 2022 07:20:09 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /usr/share/nginx/html from common-data (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7jtgb (ro) ubuntu: Container ID: docker://d896bd674b06da660ee2e179d104af43344628dcf49edeb90630dc33fa63197e Image: ubuntu Image ID: docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be Port: <none> Host Port: <none> Command: /bin/sh Args: -c echo Hello from the Ubuntu container which is visible in the nginx container as they are sharing the same storage common-data > /ubuntu-data/index.html State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 16 Feb 2022 07:20:20 +0530 Finished: Wed, 16 Feb 2022 07:20:20 +0530 Ready: False Restart Count: 0 Environment: <none> Mounts: /ubuntu-data from common-data (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7jtgb (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: common-data: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: SizeLimit: <unset> kube-api-access-7jtgb: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m8s default-scheduler Successfully assigned default/multi-containers to k8s-m02 Normal Pulling 3m6s kubelet Pulling image \"nginx\" Normal Pulled 2m49s kubelet Successfully pulled image \"nginx\" in 17.095268831s Normal Created 2m49s kubelet Created container nginx Normal Started 2m49s kubelet Started container nginx Normal Pulling 2m49s kubelet Pulling image \"ubuntu\" Normal Pulled 2m38s kubelet Successfully pulled image \"ubuntu\" in 10.692871602s Normal Created 2m38s kubelet Created container ubuntu Normal Started 2m38s kubelet Started container ubuntu We can see that the nginx container is Running but the ubuntu container is terminated with Reason as Completed. That means, Ubuntu container has finished its task. The task that we have given to this container is to echo a message and write that to a file. That's all. To continue the verification, login to the nginx container. Remember the kubectl exec command?! pradeep@learnk8s$ kubectl exec -it multi-containers -c nginx -- /bin/bash root@multi-containers:/# curl localhost Hello from the Ubuntu container which is visible in the nginx container as they are sharing the same storage common-data root@multi-containers:/# exit exit The one difference that you might have noticed is the use of -c option here. When there are multiple containers in a pod, we need to be specific about which container we want to work with, by specifying its name. In this case , we want to login to the shell (bash) of the nginx container, and henc the -c nginx option. Also, we specify the command (/bin/bash) that we want to execute after the two dashes followed by a space ( -- ). We logged in to the nginx container, but the data written by the ubuntu container is visible here as both of them are using the common storage volume called common-data .","title":"Multi Container Pods"},{"location":"Kubernetes/#init-containers","text":"Earlier, we saw that a Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started. Init containers are exactly like regular containers, except: Init containers always run to completion. Each init container must complete successfully before the next one starts. If init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed. Here is an example with one InitContainer called wait-for-service which is basically looking for the presence of a service called my-service . It does this check by performing nslookup . Till the kebernetes service called my-service is present, the name resolution will not succeed and this initContainer will not be finishing. Once the service is created, the initContainer job is done and it proceeds to the main container, in this example, it is the ubuntu container. pradeep@learnk8s$ more pod-with-init-container.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-init-container spec: containers: - name: ubuntu image: ubuntu command: ['sh', '-c', 'echo The ubuntu container is running! && sleep 3600'] initContainers: - name: wait-for-service image: busybox:1.28 command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"] pradeep@learnk8s$ kubectl create -f pod-with-init-container.yaml pod/pod-with-init-container created Once we created the pod with initContainer, we can see the state as Init:0/1 meaning there is one init container but it is not yet ready. pradeep@learnk8s$ kubectl get pods -o wide | grep init pod-with-init-container 0/1 Init:0/1 0 7s <none> k8s <none> <none> Waited for some more time, still it is in Init:0/1 state. pradeep@learnk8s$ kubectl get pods -o wide | grep init pod-with-init-container 0/1 Init:0/1 0 96s 10.244.0.15 k8s <none> <none> Let us describe this pod to get additional details. pradeep@learnk8s$ kubectl describe pods pod-with-init-container Name: pod-with-init-container Namespace: default Priority: 0 Node: k8s/192.168.177.29 Start Time: Wed, 16 Feb 2022 07:48:13 +0530 Labels: <none> Annotations: <none> Status: Pending IP: IPs: <none> Init Containers: wait-for-service: Container ID: Image: busybox:1.28 Image ID: Port: <none> Host Port: <none> Command: sh -c until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done State: Waiting Reason: PodInitializing Ready: False Restart Count: 0 Environment: <none> apiVersion: v1 Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro) Containers: ubuntu: Container ID: Image: ubuntu Image ID: Port: <none> Host Port: <none> Command: sh -c echo The ubuntu container is running! && sleep 3600 State: Waiting Reason: PodInitializing Ready: False Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro) Conditions: Type Status Initialized False Ready False ContainersReady False PodScheduled True Volumes: kube-api-access-7gp7r: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 17s default-scheduler Successfully assigned default/pod-with-init-container to k8s Normal Pulling 14s kubelet Pulling image \"busybox:1.28\" As seen above, Init Container is in Waiting state and overall Pod status is Pending. To make the InitiContainer successful, let us create a service called myservice . Here is the sample definition of the service. pradeep@learnk8s$ cat init-container-service.yaml apiVersion: v1 kind: Service metadata: name: myservice spec: ports: - protocol: TCP port: 80 targetPort: 9376 Create the myservice from the YAML file. pradeep@learnk8s$ kubectl create -f init-container-service.yaml service/myservice created Once the service is created, verify if there is any change in the main Pod status. It seems there is a change. pradeep@learnk8s$ kubectl get pods -o wide | grep init pod-with-init-container 0/1 PodInitializing 0 108s 10.244.0.15 k8s <none> <none> It is not yet in running state, it might take some more time. In the meanwhile, let us inspect the logs of the InitContainer that we created wait-for-service . Remember the kubectl logs command?! and the -c option to specify a container name when there are multiple containers in a pod! pradeep@learnk8s$ kubectl logs pod-with-init-container -c wait-for-service Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local nslookup: can't resolve 'myservice.default.svc.cluster.local' waiting for myservice Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local waiting for myservice nslookup: can't resolve 'myservice.default.svc.cluster.local' Server: 10.96.0.10 nslookup: can't resolve 'myservice.default.svc.cluster.local' Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local <SNIP> nslookup: can't resolve 'myservice.default.svc.cluster.local' waiting for myservice Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: myservice.default.svc.cluster.local Address 1: 10.111.136.80 myservice.default.svc.cluster.local A lot of similar output is omitted for brevity, but we can clearly see that this container is continuosly checking for the resolution of myservice.default.svc.cluster.local but nslookup: can't resolve it. Finally, after we created the myservice Service, name resolution was successful ( the last line). It ( myservice.default.svc.cluster.local ) is resolved to the IP address of 10.111.136.80. This is nothing but the ClusterIP assigned to the myservice . pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 19h myservice ClusterIP 10.111.136.80 <none> 80/TCP 9m13s Going back to the main Pod, we can see it is in Running State. pradeep@learnk8s$ kubectl get pods -o wide | grep init pod-with-init-container 1/1 Running 0 12m 10.244.0.15 k8s <none> <none> If we describe it one more time, Init Container is in Terminated State with Reason as Completed. And the Ubuntu container is Running. pradeep@learnk8s$ kubectl describe pods pod-with-init-container Name: pod-with-init-container Namespace: default Priority: 0 Node: k8s/192.168.177.29 Start Time: Wed, 16 Feb 2022 07:48:13 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.0.15 IPs: IP: 10.244.0.15 Init Containers: wait-for-service: Container ID: docker://8b5668914aeef2c9c43eebf3ebf7b7289eff4efa96166198fc0c6db8cdfd358b Image: busybox:1.28 Image ID: docker-pullable://busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 Port: <none> Host Port: <none> Command: sh -c until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 16 Feb 2022 07:48:52 +0530 Finished: Wed, 16 Feb 2022 07:49:52 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro) Containers: ubuntu: Container ID: docker://60caee7150e608901a2fda93fdc6758c8aa5208ec69320ed8c7982d57efde9fb Image: ubuntu Image ID: docker-pullable://ubuntu@sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be Port: <none> Host Port: <none> Command: sh -c echo The ubuntu container is running! && sleep 3600 State: Running Started: Wed, 16 Feb 2022 07:59:33 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7gp7r (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-7gp7r: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12m default-scheduler Successfully assigned default/pod-with-init-container to k8s Normal Pulling 12m kubelet Pulling image \"busybox:1.28\" Normal Pulled 11m kubelet Successfully pulled image \"busybox:1.28\" in 35.840326348s Normal Created 11m kubelet Created container wait-for-service Normal Started 11m kubelet Started container wait-for-service Normal Pulling 10m kubelet Pulling image \"ubuntu\" Normal Pulled 50s kubelet Successfully pulled image \"ubuntu\" in 9m39.673051136s Normal Created 50s kubelet Created container ubuntu Normal Started 50s kubelet Started container ubuntu :new: :date:","title":"Init Containers"},{"location":"Kubernetes/#rollout","text":"Let us take a look at the current deployments. pradeep@learnk8s$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 13d To manage the rollout of a resource like deployments, kubernetes provides the rollout option, with which we can view rollout history, see the status of the rollout and even undo a previous rollout. Here is the detailed help information. pradeep@learnk8s$ kubectl rollout -h Manage the rollout of a resource. Valid resource types include: * deployments * daemonsets * statefulsets Examples: # Rollback to the previous deployment kubectl rollout undo deployment/abc # Check the rollout status of a daemonset kubectl rollout status daemonset/foo Available Commands: history View rollout history pause Mark the provided resource as paused restart Restart a resource resume Resume a paused resource status Show the status of the rollout undo Undo a previous rollout Usage: kubectl rollout SUBCOMMAND [options] Use \"kubectl <command> --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). Let us use this option to view the history of our kodekloud deployment. pradeep@learnk8s$ kubectl rollout history deployment kodekloud deployment.apps/kodekloud REVISION CHANGE-CAUSE 1 <none> 3 <none> 4 <none> We do see that there are multiple revisions to this kodekloud deployment, but we do not see any CHANGE-CAUSE . Even in the description, we do see only one line related to deployment.kubernetes.io/revision . Pay attention to the Annotations section. pradeep@learnk8s$ kubectl describe deployments.apps Name: kodekloud Namespace: default CreationTimestamp: Tue, 15 Feb 2022 12:43:38 +0530 Labels: app=kodekloud Annotations: deployment.kubernetes.io/revision: 4 Selector: app=kodekloud Replicas: 4 desired | 4 updated | 4 total | 4 available | 0 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=kodekloud Containers: webapp-color: Image: kodekloud/webapp-color:v2 Port: <none> Host Port: <none> Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailable OldReplicaSets: <none> NewReplicaSet: kodekloud-8477b7849 (4/4 replicas created) Events: <none> Create a new deployment pradeep@learnk8s$ cat nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.20 ports: - containerPort: 80 ```` ```sh pradeep@learnk8s$ kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deployment created pradeep@learnk8s$ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 13d nginx-deployment 3/3 3 3 31s pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 1 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.20 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-7b96fbf5d8 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 33s deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 33s deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3","title":"Rollout"},{"location":"Kubernetes/#update-deployment-using-kubectl-set","text":"Earlier we have used kubectl edit to make changes to an existing deployment. Here we will explore another option kubectl set . pradeep@learnk8s$ kubectl set -h Configure application resources. These commands help you make changes to existing application resources. Available Commands: env Update environment variables on a pod template image Update the image of a pod template resources Update resource requests/limits on objects with pod templates selector Set the selector on a resource serviceaccount Update the service account of a resource subject Update the user, group, or service account in a role binding or cluster role binding Usage: kubectl set SUBCOMMAND [options] Use \"kubectl <command> --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). Let us set the image for our nginx-deployment to a new version 1.21 . pradeep@learnk8s$ kubectl set image deployment/nginx-deployment nginx=nginx:1.21 deployment.apps/nginx-deployment image updated We can see the rollout status like this, using the kubectl rollout status command. pradeep@learnk8s$ kubectl rollout status deployment/nginx-deployment deployment \"nginx-deployment\" successfully rolled out Describe the deployment pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 2 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-5778cd94ff (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 10m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 10m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 3m18s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 3m12s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 3m12s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 3m10s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 3m10s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 3m3s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0","title":"Update Deployment (using Kubectl Set)"},{"location":"Kubernetes/#kubectl-annotate","text":"Annotations are like labels (key/value pairs), they store arbitrary string values. pradeep@learnk8s$ kubectl annotate -h Update the annotations on one or more resources. All Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and system extensions may use annotations to store their own data. Attempting to set an annotation that already exists will fail unless --overwrite is set. If --resource-version is specified and does not match the current resource version on the server the command will fail. Use \"kubectl api-resources\" for a complete list of supported resources. Examples: # Update pod 'foo' with the annotation 'description' and the value 'my frontend' # If the same annotation is set multiple times, only the last value will be applied kubectl annotate pods foo description='my frontend' # Update a pod identified by type and name in \"pod.json\" kubectl annotate -f pod.json description='my frontend' # Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any existing value kubectl annotate --overwrite pods foo description='my frontend running nginx' # Update all pods in the namespace kubectl annotate pods --all description='my frontend running nginx' # Update pod 'foo' only if the resource is unchanged from version 1 kubectl annotate pods foo description='my frontend running nginx' --resource-version=1 # Update pod 'foo' by removing an annotation named 'description' if it exists # Does not require the --overwrite flag kubectl annotate pods foo description- Options: --all=false: Select all resources, in the namespace of the specified resource types. -A, --all-namespaces=false: If true, check the specified action in all namespaces. --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats. --dry-run='none': Must be \"none\", \"server\", or \"client\". If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource. --field-manager='kubectl-annotate': Name of the manager used to track field ownership. --field-selector='': Selector (field query) to filter on, supports '=', '==', and '!='.(e.g. --field-selector key1=value1,key2=value2). The server only supports a limited number of field queries per type. -f, --filename=[]: Filename, directory, or URL to files identifying the resource to update the annotation -k, --kustomize='': Process the kustomization directory. This flag can't be used together with -f or -R. --list=false: If true, display the annotations for a given resource. --local=false: If true, annotation will NOT contact api-server but run locally. -o, --output='': Output format. One of: json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file. --overwrite=false: If true, allow annotations to be overwritten, otherwise reject annotation updates that overwrite existing annotations. -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory. --resource-version='': If non-empty, the annotation update will only succeed if this is the current resource-version for the object. Only valid when specifying a single resource. -l, --selector='': Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). --show-managed-fields=false: If true, keep the managedFields when printing objects in JSON or YAML format. --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview]. Usage: kubectl annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version] [options] Use \"kubectl options\" for a list of global command-line options (applies to all commands). pradeep@learnk8s$ kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.21\" deployment.apps/nginx-deployment annotated pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 1 <none> 2 image updated to 1.21 Look at the Annotations section of the description. pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 2 kubernetes.io/change-cause: image updated to 1.21 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-5778cd94ff (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 14m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 14m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 6m57s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 6m51s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 6m51s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 6m49s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 6m49s deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 6m42s deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 We can see the details of a particular revision using the --revision option. pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment --revision=1 deployment.apps/nginx-deployment with revision #1 Pod Template: Labels: app=nginx pod-template-hash=7b96fbf5d8 Containers: nginx: Image: nginx:1.20 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment --revision=2 deployment.apps/nginx-deployment with revision #2 Pod Template: Labels: app=nginx pod-template-hash=5778cd94ff Annotations: kubernetes.io/change-cause: image updated to 1.21 Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none>","title":"Kubectl Annotate"},{"location":"Kubernetes/#rollback","text":"If you've decided to undo the current rollout and rollback to the previous revision, you can do so. pradeep@learnk8s$ kubectl rollout undo deployment/nginx-deployment deployment.apps/nginx-deployment rolled back pradeep@learnk8s$ kubectl rollout status deployment nginx-deployment deployment \"nginx-deployment\" successfully rolled out pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 2 image updated to 1.21 3 <none> pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 3 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.20 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-7b96fbf5d8 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 20m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 13m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 12m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 12m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 12m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 Normal ScalingReplicaSet 50s deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 48s deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 48s deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 46s deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 45s (x2 over 20m) deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 43s deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 0 Let us annotate revision 3. pradeep@learnk8s$ kubectl annotate deployments.apps nginx-deployment kubernetes.io/change-cause=\"image rolledback to 1.20\" deployment.apps/nginx-deployment annotated pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 2 image updated to 1.21 3 image rolledback to 1.20 What happens if we rollback the deployment again? pradeep@learnk8s$ kubectl rollout undo deployment/nginx-deployment deployment.apps/nginx-deployment rolled back pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 3 image rolledback to 1.20 4 image updated to 1.21 pradeep@learnk8s$ kubectl rollout history deployment/nginx-deployment --revision=4 deployment.apps/nginx-deployment with revision #4 Pod Template: Labels: app=nginx pod-template-hash=5778cd94ff Annotations: kubernetes.io/change-cause: image updated to 1.21 Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 4 kubernetes.io/change-cause: image updated to 1.21 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.21 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-5778cd94ff (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 31m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 23m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 23m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 23m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 11m (x2 over 31m) deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 0 Normal ScalingReplicaSet 2m27s (x2 over 24m) deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 2m25s (x2 over 23m) deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 2m25s (x2 over 23m) deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 2m20s (x3 over 2m23s) deployment-controller (combined from similar events): Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 Let us update the deployment one more time, finally to the latest version. pradeep@learnk8s$ kubectl set image deployment/nginx-deployment nginx=nginx:latest deployment.apps/nginx-deployment image updated Check the rollout history, and note that the existing annotation continued to the latest revision. pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 3 image rolledback to 1.20 4 image updated to 1.21 5 image updated to 1.21 Change the annotation pradeep@learnk8s$ kubectl annotate deployments.apps nginx-deployment kubernetes.io/change-cause=\"image updated to the latest\" deployment.apps/nginx-deployment annotated pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 3 image rolledback to 1.20 4 image updated to 1.21 5 image updated to the latest pradeep@learnk8s$ kubectl describe deployments.apps nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 01 Mar 2022 06:05:50 +0530 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 5 kubernetes.io/change-cause: image updated to the latest Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:latest Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-67dffbbbb (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ReplicaSetCreateError 40m deployment-controller Failed to create new replica set \"nginx-deployment-7b96fbf5d8\": Unauthorized Normal ScalingReplicaSet 32m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 32m deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 3 Normal ScalingReplicaSet 32m deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 0 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 1 Normal ScalingReplicaSet 20m deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 20m (x2 over 40m) deployment-controller Scaled up replica set nginx-deployment-7b96fbf5d8 to 3 Normal ScalingReplicaSet 20m deployment-controller Scaled down replica set nginx-deployment-5778cd94ff to 0 Normal ScalingReplicaSet 11m (x2 over 33m) deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 1 Normal ScalingReplicaSet 11m (x2 over 33m) deployment-controller Scaled up replica set nginx-deployment-5778cd94ff to 2 Normal ScalingReplicaSet 11m (x2 over 33m) deployment-controller Scaled down replica set nginx-deployment-7b96fbf5d8 to 2 Normal ScalingReplicaSet 2m52s (x9 over 11m) deployment-controller (combined from similar events): Scaled down replica set nginx-deployment-5778cd94ff to 0 pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE kodekloud-8477b7849-blzrs 1/1 Running 1 13d kodekloud-8477b7849-m65m8 1/1 Running 1 13d kodekloud-8477b7849-p7psw 1/1 Running 0 32h kodekloud-8477b7849-vf9zs 1/1 Running 0 32h kodekloud-cm 0/1 ContainerStatusUnknown 1 13d nginx-deployment-67dffbbbb-7ttxq 1/1 Running 0 3m35s nginx-deployment-67dffbbbb-8dtsf 1/1 Running 0 3m38s nginx-deployment-67dffbbbb-sz5bb 1/1 Running 0 3m40s pod-with-hostpath-volume 0/1 ContainerCreating 1 8d pod-with-init-container 1/1 Running 45 (4m46s ago) 12d security-context-demo-cap 1/1 Running 15 (5m16s ago) 9d pradeep@learnk8s$ kubectl get rs NAME DESIRED CURRENT READY AGE kodekloud-589c9f4b47 0 0 0 13d kodekloud-676c6b9fcd 0 0 0 13d kodekloud-8477b7849 4 4 4 13d nginx-deployment-5778cd94ff 0 0 0 34m nginx-deployment-67dffbbbb 3 3 3 3m52s nginx-deployment-7b96fbf5d8 0 0 0 41m pradeep@learnk8s$ kubectl rollout history deployment nginx-deployment --revision=5 deployment.apps/nginx-deployment with revision #5 Pod Template: Labels: app=nginx pod-template-hash=67dffbbbb Annotations: kubernetes.io/change-cause: image updated to the latest Containers: nginx: Image: nginx:latest Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none>","title":"Rollback"},{"location":"Kubernetes/#security","text":"","title":"Security"},{"location":"Kubernetes/#certificates","text":"Take a look at the description of the kube-apiserver pod in the kube-system namespace. Pay attention to the kube-apiserver command arguments. It is a long list!! Primarily, there are many pairs of certfile and keyfile for apiserver, etcd, kubectl etc. In Minikube setup, most of these are located in /var/lib/minikube/certs/ . It would be different when the cluster is setup with kubeadm which we have not discussed yet. pradeep@learnk8s$ kubectl describe -n kube-system pods kube-apiserver-k8s Name: kube-apiserver-k8s Namespace: kube-system Priority: 2000001000 Priority Class Name: system-node-critical Node: k8s/192.168.177.29 Start Time: Tue, 15 Feb 2022 12:28:03 +0530 Labels: component=kube-apiserver tier=control-plane Annotations: kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.177.29:8443 kubernetes.io/config.hash: cca804e910f3a6e748c66a6963d63fdd kubernetes.io/config.mirror: cca804e910f3a6e748c66a6963d63fdd kubernetes.io/config.seen: 2022-02-15T06:58:02.427574419Z kubernetes.io/config.source: file seccomp.security.alpha.kubernetes.io/pod: runtime/default Status: Running IP: 192.168.177.29 IPs: IP: 192.168.177.29 Controlled By: Node/k8s Containers: kube-apiserver: Container ID: docker://0ec9bc91aa13e593b1518fac7a4f9f39c7e16a0e478c2362336b8c050f9c085c Image: k8s.gcr.io/kube-apiserver:v1.23.1 Image ID: docker-pullable://k8s.gcr.io/kube-apiserver@sha256:f54681a71cce62cbc1b13ebb3dbf1d880f849112789811f98b6aebd2caa2f255 Port: <none> Host Port: <none> Command: kube-apiserver --advertise-address=192.168.177.29 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-account-signing-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key State: Running Started: Tue, 15 Feb 2022 12:27:50 +0530 Ready: True Restart Count: 1 Requests: cpu: 250m Liveness: http-get https://192.168.177.29:8443/livez delay=10s timeout=15s period=10s #success=1 #failure=8 Readiness: http-get https://192.168.177.29:8443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3 Startup: http-get https://192.168.177.29:8443/livez delay=10s timeout=15s period=10s #success=1 #failure=24 Environment: <none> Mounts: /etc/ssl/certs from ca-certs (ro) /usr/share/ca-certificates from usr-share-ca-certificates (ro) /var/lib/minikube/certs from k8s-certs (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: ca-certs: Type: HostPath (bare host directory volume) Path: /etc/ssl/certs HostPathType: DirectoryOrCreate k8s-certs: Type: HostPath (bare host directory volume) Path: /var/lib/minikube/certs HostPathType: DirectoryOrCreate usr-share-ca-certificates: Type: HostPath (bare host directory volume) Path: /usr/share/ca-certificates HostPathType: DirectoryOrCreate QoS Class: Burstable Node-Selectors: <none> Tolerations: :NoExecute op=Exists Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 178m (x5 over 30h) kubelet Liveness probe failed: Get \"https://192.168.177.29:8443/livez\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 177m (x17 over 30h) kubelet Readiness probe failed: Get \"https://192.168.177.29:8443/readyz\": net/http: TLS handshake timeout Warning Unhealthy 170m (x7 over 27h) kubelet Readiness probe failed: Get \"https://192.168.177.29:8443/readyz\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 170m (x6 over 27h) kubelet Liveness probe failed: Get \"https://192.168.177.29:8443/livez\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 170m (x8 over 30h) kubelet Readiness probe failed: Get \"https://192.168.177.29:8443/readyz\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) We can login to the minikube node and use the openssl x509 command to view the actual certificate. Here is an example. Pass the certificate path as -in argument and use -text to display the certificate in plain-text. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ openssl x509 -in /var/lib/minikube/certs/apiserver.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 2 (0x2) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = minikubeCA Validity Not Before: Feb 14 06:54:36 2022 GMT Not After : Feb 14 06:54:36 2025 GMT Subject: O = system:masters, CN = minikube Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: 00:c7:e1:19:9b:17:43:df:ff:31:4e:fe:66:7c:4b: c4:1a:63:e6:5d:a6:a2:85:1c:11:b9:5a:72:42:50: 8b:25:a4:f0:89:eb:24:bf:3b:a6:e6:79:26:b8:18: ed:9b:7b:76:01:68:a4:b1:1b:39:d8:b8:36:14:21: 44:b5:26:57:a3:a6:d3:55:e2:8c:32:5b:55:71:1e: 47:3e:56:b6:e8:92:86:af:aa:90:d1:4a:5a:36:ac: a7:4f:a4:6c:09:a6:16:3b:e7:76:bc:41:18:89:7e: be:87:df:c7:a9:ee:b7:da:34:43:ae:9f:37:cd:5d: 8d:e2:71:5c:e6:4c:e4:60:46:8c:b1:ef:7d:90:4b: 51:c3:e3:7f:a7:84:fe:06:28:1a:28:18:fd:9a:00: b0:a7:d9:c9:b1:61:c9:d7:81:2d:c1:5d:5b:d2:f3: f6:13:e4:d8:7f:d6:5c:c0:39:56:b1:14:04:f6:b7: ea:9b:50:d7:aa:4d:f2:20:89:8b:8b:bc:81:b0:91: 4f:9b:f2:b9:69:b5:ce:80:67:a4:9e:f3:ba:17:03: f6:89:ee:22:0e:8d:65:61:ef:16:96:67:dc:d7:4f: ea:aa:36:7a:0c:59:53:2d:2a:fd:01:0f:93:15:fa: 8d:42:94:da:f1:0d:c8:8e:6b:15:b3:8b:4f:de:1d: 03:2b Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Authority Key Identifier: keyid:8B:FA:31:A7:8B:76:DE:C6:F5:3D:C0:BF:25:05:1D:05:78:B9:82:40 X509v3 Subject Alternative Name: DNS:minikubeCA, DNS:control-plane.minikube.internal, DNS:kubernetes.default.svc.cluster.local, DNS:kubernetes.default.svc, DNS:kubernetes.default, DNS:kubernetes, DNS:localhost, IP Address:192.168.177.29, IP Address:10.96.0.1, IP Address:127.0.0.1, IP Address:10.0.0.1 Signature Algorithm: sha256WithRSAEncryption 8e:94:63:81:ad:57:80:84:2d:89:8b:3c:af:7c:13:d1:6c:49: 53:53:61:cc:cb:bc:9d:63:93:9b:4e:b2:0e:a0:e3:9d:22:e4: 4e:a9:de:75:88:05:23:46:bb:75:4c:be:ff:ba:68:e3:19:d0: 15:b2:6a:01:5d:5b:ea:d0:a2:2d:53:80:99:25:e9:4f:f0:1a: 65:47:c3:e4:8e:06:6c:db:23:55:57:64:f3:0d:5a:4a:e8:63: b2:b6:57:00:13:85:29:fe:e0:de:06:d6:e3:ec:f3:96:1d:5c: e7:03:8f:46:d9:bf:6b:f5:dd:1a:41:db:15:23:14:36:42:c3: c7:34:28:2e:a3:c4:e8:99:29:6c:28:9b:40:35:aa:58:0e:4a: b4:fd:0b:b4:11:a6:c5:f4:10:97:9b:c8:1c:ec:ea:a0:77:7c: c2:b1:70:c6:7b:85:34:8a:36:b0:ca:35:6a:7c:1c:e9:4e:08: 9c:f9:be:de:41:ce:84:5e:51:60:52:e0:63:89:a7:18:1f:23: 3e:f2:8e:0c:d6:9d:d2:38:04:cd:cc:2c:2e:70:c8:57:99:2b: 3e:ba:08:1f:86:f4:0f:39:63:55:71:33:bc:49:ac:44:cf:e6: 4f:27:dd:78:45:88:13:a7:57:d1:a3:09:76:cb:06:00:4b:84: df:ac:cb:0e -----BEGIN CERTIFICATE----- MIID3DCCAsSgAwIBAgIBAjANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p a3ViZUNBMB4XDTIyMDIxNDA2NTQzNloXDTI1MDIxNDA2NTQzNlowLDEXMBUGA1UE ChMOc3lzdGVtOm1hc3RlcnMxETAPBgNVBAMTCG1pbmlrdWJlMIIBIjANBgkqhkiG 9w0BAQEFAAOCAQ8AMIIBCgKCAQEAx+EZmxdD3/8xTv5mfEvEGmPmXaaihRwRuVpy QlCLJaTwieskvzum5nkmuBjtm3t2AWigsRs52Lg2FCFEtSZXo6bTVeKMMltVcR5H Pla26JKGr6qQ0UpaNqynT6RsCaYWO+d2vEEYiX6+h9/Hqe632jRDrp83zV2N4nFc 5kzkYEaMse99kEtRw+N/p4T+BigaKBj9mgCwp9nJsWHJ14EtwV1b0vP2E+TYf9Zc wDlWsRQE9rfqm1DXqk3yIImLi7yBsJFPm/K5abXOgGeknvO6FwP2ie4iDo1lYe8W lmfc10/qqjZ6DFlTLSr9AQ+TFfqNQpTa8Q3IjmsVs4tP3h0DKwIDAQABo4IBHjCC ARowDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEFBQcD AjAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFIv6MadLdt7G9T3AvyUFHQV4uYJA MIG5BgNVHREEgbEwgb6CCm1pbmlrdWJlQ0GCH2NvbnRyb2wtcGxhbmUubWluaWt1 YmUuaW50ZXJuYWyCJGt1YmVybmV0ZXMuZGVmYXVsdC5zdmMuY2x1c3Rlci5sb2Nh bIIWa3ViZXJuZXRlcy5kZWZhdWx0LnN2Y4ISa3ViZXJuZXRlcy5kZWZhdWx0ggpr dWJlcm5ldGVzgglsb2NhbGhvc3SHBMCosR2HBApgAAGHBH8AAAGHBAoAAAEwDQYJ KoZIhvcNAQELBQADggEBAI6UY4GtV4CELYmLPK98E9FsSVNTYczLvJ1jk5tOsg6g 450i5E6p3nWIBSNGu3VMvv+6aOMZ0BWyagFdW+rQoi1TgJkl6U/wGmVHw+SOBmzb I1VXZPMNWkroY7K2VwAThSn+4N4G1uPs85YdXOcDj0bZv2v13Roh2xUjFDZCw8c0 KC6jxOiZKWwom0A1qlgOSrT9C7QRpsX0EJtbyBzs6qB3fMKxcMZ7hTSKNrDKNWp8 HOlOCJz5vt5BzoReUWBS4GOJpxgfIz7yjgzWndI4BM3MLC5wyFeZKz66CB+G9A85 Y1VxM7xJrETP5k8n3XhFiBOnV9GjCXbLBgBLhN+syw4= -----END CERTIFICATE----- $ We can see that the Issuer: CN = minikubeCA , Subject: O = system:masters, CN = minikube and Subject Alternative Name: DNS:minikubeCA, DNS:control-plane.minikube.internal, DNS:kubernetes.default.svc.cluster.local, DNS:kubernetes.default.svc, DNS:kubernetes.default, DNS:kubernetes, DNS:localhost, IP Address:192.168.177.29, IP Address:10.96.0.1, IP Address:127.0.0.1, IP Address:10.0.0.1 . Similarly, let us take a look at the description of the etcd pod in the kube-system namespace and pay attention to the etcd command arguments. Again a long list, but for now, just look for cert and key files. pradeep@learnk8s$ kubectl describe -n kube-system pods etcd-k8s Name: etcd-k8s Namespace: kube-system Priority: 2000001000 Priority Class Name: system-node-critical Node: k8s/192.168.177.29 Start Time: Tue, 15 Feb 2022 12:28:03 +0530 Labels: component=etcd tier=control-plane Annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.177.29:2379 kubernetes.io/config.hash: 553a1d887eba16384375f475194d677c kubernetes.io/config.mirror: 553a1d887eba16384375f475194d677c kubernetes.io/config.seen: 2022-02-15T06:58:02.427571167Z kubernetes.io/config.source: file seccomp.security.alpha.kubernetes.io/pod: runtime/default Status: Running IP: 192.168.177.29 IPs: IP: 192.168.177.29 Controlled By: Node/k8s Containers: etcd: Container ID: docker://7ad2e9b1c5fb22610382e026ecea82c4def63655f4771b398416b6dfd7b88374 Image: k8s.gcr.io/etcd:3.5.1-0 Image ID: docker-pullable://k8s.gcr.io/etcd@sha256:64b9ea357325d5db9f8a723dcf503b5a449177b17ac87d69481e126bb724c263 Port: <none> Host Port: <none> Command: etcd --advertise-client-urls=https://192.168.177.29:2379 --cert-file=/var/lib/minikube/certs/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/minikube/etcd --initial-advertise-peer-urls=https://192.168.177.29:2380 --initial-cluster=k8s=https://192.168.177.29:2380 --key-file=/var/lib/minikube/certs/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.177.29:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.177.29:2380 --name=k8s --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/var/lib/minikube/certs/etcd/peer.key --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt --proxy-refresh-interval=70000 --snapshot-count=10000 --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt State: Running Started: Tue, 15 Feb 2022 12:27:49 +0530 Ready: True Restart Count: 1 Requests: cpu: 100m memory: 100Mi Liveness: http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8 Startup: http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24 Environment: <none> Mounts: /var/lib/minikube/certs/etcd from etcd-certs (rw) /var/lib/minikube/etcd from etcd-data (rw) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: etcd-certs: Type: HostPath (bare host directory volume) Path: /var/lib/minikube/certs/etcd HostPathType: DirectoryOrCreate etcd-data: Type: HostPath (bare host directory volume) Path: /var/lib/minikube/etcd HostPathType: DirectoryOrCreate QoS Class: Burstable Node-Selectors: <none> Tolerations: :NoExecute op=Exists Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 3h18m (x12 over 28h) kubelet Liveness probe failed: Get \"http://127.0.0.1:2381/health\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) ETCD can have its own Certificate Authority (CA). Let us verify if both api-server and etcd are using the same CA certificate or different. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ openssl x509 -in /var/lib/minikube/certs/etcd/ca.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 0 (0x0) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = etcd-ca Validity Not Before: Feb 15 06:54:39 2022 GMT Not After : Feb 13 06:54:39 2032 GMT Subject: CN = etcd-ca Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: 00:c2:8c:1f:12:73:c7:89:bc:f9:8d:14:c2:5e:61: 2f:ae:d4:f0:48:89:f7:cc:bc:be:a6:19:9d:a8:4d: 85:99:59:d6:96:57:35:9e:6c:62:39:31:12:e6:ee: 32:f9:a4:aa:ce:91:8b:5f:1d:7d:99:cc:b5:fc:c7: 14:75:f0:a2:43:4f:fd:e6:e7:b3:ab:11:c2:5f:db: a8:72:b8:80:2f:66:5f:98:21:3b:06:af:b9:09:69: 94:1a:06:33:96:2f:1c:c7:f8:b2:ca:bd:87:d9:13: 36:c5:f6:de:aa:6a:81:c2:d4:94:2b:9e:63:dc:56: 27:b4:32:31:1d:49:ab:69:0e:dc:d1:14:d3:bb:f1: 43:80:19:31:73:29:7e:7b:d4:3b:2d:cf:14:7f:3b: 3c:84:4b:21:a4:2d:a6:59:79:bb:6f:1d:dc:5c:d5: 44:fd:3f:bd:34:b4:33:38:0d:cc:76:48:e1:de:53: 02:2d:54:79:44:22:64:f5:a6:39:1e:87:24:6c:91: 3c:5f:eb:7f:1f:84:38:e0:96:19:1b:46:9d:fc:e6: 79:98:c7:2e:6e:2a:2e:63:f0:28:42:57:16:14:45: f3:de:bf:32:8e:d9:49:e5:ab:a5:06:6e:0d:d2:9c: 8b:65:40:46:17:43:3e:d2:46:53:bf:22:89:72:d0: 6f:49 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment, Certificate Sign X509v3 Basic Constraints: critical CA:TRUE X509v3 Subject Key Identifier: E3:7B:E5:78:30:93:55:DB:08:E4:EB:D0:2E:0C:30:6C:DB:17:40:40 X509v3 Subject Alternative Name: DNS:etcd-ca Signature Algorithm: sha256WithRSAEncryption 1a:29:48:3a:dc:24:57:35:20:b7:21:31:ed:c5:12:88:d5:79: 7e:42:ca:21:12:92:64:49:5e:eb:2e:60:ce:16:71:5f:43:76: 09:97:3d:6c:19:15:16:b0:6d:fc:c9:84:14:e7:5b:c2:c8:d4: 24:77:bf:fd:87:3d:e4:c9:e4:39:49:ba:f6:41:bb:a0:9c:97: ef:71:b0:46:a1:86:dc:00:6b:19:26:39:32:26:c4:0c:70:4e: bf:1b:6b:93:55:54:d7:97:89:07:5e:e9:3b:63:a5:49:4e:6c: 21:aa:75:8b:b1:a9:94:68:bf:1c:2e:cf:84:09:b0:52:03:62: 72:54:b0:e2:c8:63:88:31:c0:1e:de:38:89:39:25:92:df:b9: 1d:56:fb:c5:3b:71:fa:4e:70:e7:ec:1b:c5:fc:39:bb:71:90: ab:d3:36:c1:80:c5:30:6f:4d:8b:7c:8a:ee:24:15:f5:fc:5c: 63:47:51:a0:9f:eb:30:ee:4e:95:a7:10:41:10:44:37:1e:19: 0d:37:65:f5:94:66:4a:93:5e:fb:df:f3:24:28:17:4e:7e:7f: 4f:d0:97:3a:24:b2:95:27:42:6f:42:0d:32:c7:b6:a6:a2:0f: 66:df:91:e9:af:c7:66:a9:eb:01:d4:74:ae:2c:1f:72:b8:40: 5e:15:d6:bc -----BEGIN CERTIFICATE----- MIIC9TCCAd2gAwIBAgIBADANBgkqhkiG9w0BAQsFADASMRAwDgYDVQQDEwdldGNk LWNhMB4XDTIyMDIxNTA2NTQzOVoXDTMyMDIxMzA2NTQzOVowEjEQMA4GA1UEAxMH ZXRjZC1jYTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMKMHxJzx4m8 +Y0Uwl5hL67U8EiJ98y8vqYZnahNhZlZ1pZXNZ5sYjkxEubuMvmkqs6Ri18dfZnM tfzHFHXwokNP/ebns6sRwl/bqHK4gC9mX5ghOwabuQlplBoGM5YvHMf4ssq9h9kT NsX23qpqgcLUlCueY9xWJ7QyMR1Jq2kO3NEU07vxQ4AZMXMpfnvUOy3PFH87PIRL IaQtpll5u28d3FzVRP0/vTS0MzgNzHZI4d5TAi1UeUQiZPWmOR6HJGyRPF/rfx+E OOCWGRtGnfzmeZjHLm4qLmPwKEJXFhRF896/Mo7ZSeWrpQZuDdKci2VARhdDPtJG U78iiXLQb0kCAwEAAaNWMFQwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB/wQFMAMB Af8wHQYDVR0OBBYEFON75Xgwk1XbCOTr0C4MMGzbF0BAMBIGA1UdEQQLMAmCB2V0 Y2QtY2EwDQYJKoZIhvcNAQELBQADggEBABopSDrfJFc1ILchMe3FEojVeX5CyiES kmRJXusuYM4WcV9DdgmXPWwZFRawbfzJhBTnW8LI1CR3v/2HPeTJ5DlJuvZBu6Cc l+9xsEahhtwAaxkmOTImxAxwTr8ba5NVVNeXiQde6TtjpUlObCGqdYuxqZRovxwu z4QJsFIDYnJUsOLIY4gxwB7eOIk5JZLfuR1W+8U7cfpOcOfsG8X8ObtxkKvTNsGA xTBvTYt8iu4kFfX8XGNHUaCf6zDuTpWnEEEQRDceGQ03ZfWUZkqTXvvf8yQoF05+ f0/QlzokspUnQm9CDTLHtqaiD2bfkemvx2ap6wHUdK4sH3K4QF4V1rw= -----END CERTIFICATE----- $ We can see it is a different Issuer: CN = etcd-ca, and Subject: CN = etcd-ca, X509v3 Subject Alternative Name: DNS:etcd-ca. If we look at the ETCD server certificate subject details, we can see the CN as k8s and subject alternative names of k8s , localhost and the node IP address 192.168.177.29 . $ openssl x509 -in /var/lib/minikube/certs/etcd/server.crt -text | grep -e Subject -e DNS Subject: CN = k8s Subject Public Key Info: X509v3 Subject Alternative Name: DNS:k8s, DNS:localhost, IP Address:192.168.177.29, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 $ Also, we can see that this certificate is valid for one year. openssl x509 -in /var/lib/minikube/certs/etcd/server.crt -text | grep -A 2 Validity Validity Not Before: Feb 15 06:54:39 2022 GMT Not After : Feb 15 06:54:39 2023 GMT $ Where as the minikubeCA certificate is valid for ten years. $ openssl x509 -in /var/lib/minikube/certs/ca.crt -text | grep -A 2 Validity Validity Not Before: Jun 28 04:53:56 2021 GMT Not After : Jun 27 04:53:56 2031 GMT","title":"Certificates"},{"location":"Kubernetes/#certificates-api","text":"Let us create a new user called pradeep and generate a key pair. Once the keypair is generated, created a certificate signing request to be submitted to the Kubernetes Cluster CA. $ sudo adduser -G wheel pradeep Changing password for pradeep New password: Bad password: similar to username Retype password: passwd: password for pradeep changed by root $ su - pradeep Password: $ mkdir .certs $ cd .certs/ $ openssl genrsa -out pradeep.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) ........+++++ ......................................................................................................+++++ e is 65537 (0x010001) $ openssl req -new -key pradeep.key -out pradeep.csr -subj \"/CN=pradeep\" $ exit logout We need to convert this CSR file to base64 format and trim newline character, which needs to be passed in the CSR request. $ ls pradeep.csr pradeep.key $ cat pradeep.csr | base64 | tr -d \"\\n\" LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=$ $ Create an YAML manifest for the CertificateSigningRequest API resource. pradeep@learnk8s$ cat <<EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: pradeep spec: request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo= expirationSeconds: 86400 # one day usages: - client auth signerName: kubernetes.io/kube-apiserver-client EOF certificatesigningrequest.certificates.k8s.io/pradeep created Verify that the new request is in Pending state. pradeep@learnk8s$ kubectl get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION pradeep 4s kubernetes.io/kube-apiserver-client minikube-user 24h Pending As we know this is a valid request, we can go ahead and approve this. pradeep@learnk8s$ kubectl certificate approve pradeep certificatesigningrequest.certificates.k8s.io/pradeep approved pradeep@learnk8s$ kubectl get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION pradeep 66s kubernetes.io/kube-apiserver-client minikube-user 24h Approved,Issued pradeep@learnk8s$ kubectl describe csr Name: pradeep Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"certificates.k8s.io/v1\",\"kind\":\"CertificateSigningRequest\",\"metadata\":{\"annotations\":{},\"name\":\"pradeep\"},\"spec\":{\"expirationSeconds\":86400,\"request\":\"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=\",\"signerName\":\"kubernetes.io/kube-apiserver-client\",\"usages\":[\"client auth\"]}} CreationTimestamp: Thu, 17 Feb 2022 11:17:16 +0530 Requesting User: minikube-user Signer: kubernetes.io/kube-apiserver-client Requested Duration: 24h Status: Approved,Issued Subject: Common Name: pradeep Serial Number: Events: <none>","title":"Certificates API"},{"location":"Kubernetes/#kube-config","text":"Kubectl command uses a configuration file that contains cluster and context information along with users . This configuration file can be viewed directly using the common commands like cat , or kubectl config view command. Usually the configuraiton is stored in a filed called config in the hidden directory .kube inside the user home directory ( /Users/pradeep/ in this case). Here we can see the same output using both methods. Using the kubectl config view command: pradeep@learnk8s$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /Users/pradeep/.minikube/ca.crt extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: cluster_info server: https://192.168.177.29:8443 name: k8s contexts: - context: cluster: k8s extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: context_info namespace: default user: k8s name: k8s current-context: k8s kind: Config preferences: {} users: - name: k8s user: client-certificate: /Users/pradeep/.minikube/profiles/k8s/client.crt client-key: /Users/pradeep/.minikube/profiles/k8s/client.key Using the standard cat command with full path Users/pradeep/.kube/config : pradeep@learnk8s$ cat /Users/pradeep/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: /Users/pradeep/.minikube/ca.crt extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: cluster_info server: https://192.168.177.29:8443 name: k8s contexts: - context: cluster: k8s extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: context_info namespace: default user: k8s name: k8s current-context: k8s kind: Config preferences: {} users: - name: k8s user: client-certificate: /Users/pradeep/.minikube/profiles/k8s/client.crt client-key: /Users/pradeep/.minikube/profiles/k8s/client.key The three important sections from this file are: Cluster: k8s Context: k8s User: k8s When we setup Minikube, all of this work is done for us automatically. We can either get or set or even delete all of these three settings with the kubectl config command as seen here. pradeep@learnk8s$ kubectl config -h Modify kubeconfig files using subcommands like \"kubectl config set current-context my-context\" The loading order follows these rules: 1. If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes place. 2. If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list. 3. Otherwise, ${HOME}/.kube/config is used and no merging takes place. Available Commands: current-context Display the current-context delete-cluster Delete the specified cluster from the kubeconfig delete-context Delete the specified context from the kubeconfig delete-user Delete the specified user from the kubeconfig get-clusters Display clusters defined in the kubeconfig get-contexts Describe one or many contexts get-users Display users defined in the kubeconfig rename-context Rename a context from the kubeconfig file set Set an individual value in a kubeconfig file set-cluster Set a cluster entry in kubeconfig set-context Set a context entry in kubeconfig set-credentials Set a user entry in kubeconfig unset Unset an individual value in a kubeconfig file use-context Set the current-context in a kubeconfig file view Display merged kubeconfig settings or a specified kubeconfig file Usage: kubectl config SUBCOMMAND [options] Use \"kubectl <command> --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands). Here is the output for all three get commands. While working with multiple clusters, this becomes really useful and we have to keep switching contexts. This is how we work in production environments. For our little minikube environment also, when we created multiple profiles with the -p option (if you forget what it is, refer to the Initial setup section), minikube automatically does this context switching! Here is that last line from the minikube start command output. \ud83c\udfc4 Done! kubectl is now configured to use \"k8s\" cluster and \"default\" namespace by default pradeep@learnk8s$ kubectl config get-users NAME k8s pradeep@learnk8s$ kubectl config get-clusters NAME k8s pradeep@learnk8s$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * k8s k8s k8s default We can see that the current user is k8s , how can we change this to some other user, like pradeep user? For that we need the signed certificate for that user first. We can retrieve the signed/issued certificate using this. Pay attention to the Status section, there is the certificate. pradeep@learnk8s$ kubectl get csr/pradeep -o yaml apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"certificates.k8s.io/v1\",\"kind\":\"CertificateSigningRequest\",\"metadata\":{\"annotations\":{},\"name\":\"pradeep\"},\"spec\":{\"expirationSeconds\":86400,\"request\":\"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=\",\"signerName\":\"kubernetes.io/kube-apiserver-client\",\"usages\":[\"client auth\"]}} creationTimestamp: \"2022-02-17T05:47:16Z\" name: pradeep resourceVersion: \"63056\" uid: 585f7670-134c-4062-a649-442054314407 spec: expirationSeconds: 86400 groups: - system:masters - system:authenticated request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0hjSEpoWkdWbGNEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNVzRxa29NK0ZDRUxwa2oySUtiTnZjeGRNNGxnMS8xMnlXZjA1NHpPMVdrCmJEQTlrQnpFdEtRb0ptR1ZZRXVLLzZHT0l0TE90WU10Z1BscTJyblJZMHF5c1VrTTMwOUo1R0RUblhpbDJsZUUKU0pjL2tuWkYxRlB3SFVxeWxGdHg5OUZwa3N3OWI1YTNSVmQweXp6K1JyOVhGcFlTR2d5WjgrcUQ5WHIvOE9jWgo0NW1PQnpoaGpKeDRYRW9NSC94aGpIK1FPc1pkaHY2Q2wrdDdZVmgrU2dzdExpVEQ3YTZ0ZXh2NUVVdFNvMllxCkJabXpEcGUzUnVMTlZYd2Z1bnVQdlNUTXdvYTZDQ3h4VEJYK2tZdTJQa2NRcVBTeTNzZlR6Y2xrbWVNbXVKVUQKdmZZTDFxNnFCa3c0a2hFckxXVVVLcGVMckdvMi9vZ01TcXJNVXF0M0k4Y0NBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUF1aHV3NWIvV2l0cms2b2YrNDAwaWxqQXEyY3dmdjdRdDNaaWZTUVVzOVEzMVdnM2d3CnZZNUd4djFDV2xtd002cWttU0lXRC9MY0lzcTJFanNJbkNSNzFhbXBKaEhHYWFOZGFrVlIxNlcyU2lxd040NjUKcGtpYmlpVEVqQUJZQ1pOWnk1dG5KVkpKSVFLM1lUM3lNK2EwVXhOQXFTTHFkU3FIV2d3am5MVktCVVBMWFBrdQpMa1VqejBWTEFXRkI3TXN4OXR2OTJQT1NZNFVULzJSN0xPdC9yZzFYeklyTmxtTHRKUUJYMUo1YkpRb0NrU1lPCmI1eklDTXZrS2FpVlloZUZ6Qi8vWitNcnZiWHgwREdkYTBWTnNKT29QeDF5VzRpRTRTQVY1SE9pbDlpd0Q2emkKdjQyblZlb2R5WjdQNlp3M241OG5tTmRWUXk1dmVGYlI5UEZCCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo= signerName: kubernetes.io/kube-apiserver-client usages: - client auth username: minikube-user status: certificate: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMrRENDQWVDZ0F3SUJBZ0lSQUlHNEdtUk5tdmczZENXZHhFcTBIUEV3RFFZSktvWklodmNOQVFFTEJRQXcKRlRFVE1CRUdBMVVFQXhNS2JXbHVhV3QxWW1WRFFUQWVGdzB5TWpBeU1UY3dOVFF6TWpCYUZ3MHlNakF5TVRndwpOVFF6TWpCYU1CSXhFREFPQmdOVkJBTVRCM0J5WVdSbFpYQXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRREZ1S3BLRFBoUWhDNlpJOWlDbXpiM01YVE9KWU5mOWRzbG45T2VNenRWcEd3d1BaQWMKeExTa0tDWmhsV0JMaXYraGppTFN6cldETFlENWF0cTUwV05Lc3JGSkROOVBTZVJnMDUxNHBkcFhoRWlYUDVKMgpSZFJUOEIxS3NwUmJjZmZSYVpMTVBXK1d0MFZYZE1zOC9rYS9WeGFXRWhvTW1mUHFnL1Y2Ly9EbkdlT1pqZ2M0CllZeWNlRnhLREIvOFlZeC9rRHJHWFliK2dwZnJlMkZZZmtvTExTNGt3KzJ1clhzYitSRkxVcU5tS2dXWnN3NlgKdDBiaXpWVjhIN3A3ajcwa3pNS0d1Z2dzY1V3Vi9wR0x0ajVIRUtqMHN0N0gwODNKWkpuakpyaVZBNzMyQzlhdQpxZ1pNT0pJUkt5MWxGQ3FYaTZ4cU52NklERXFxekZLcmR5UEhBZ01CQUFHalJqQkVNQk1HQTFVZEpRUU1NQW9HCkNDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUNNQUF3SHdZRFZSMGpCQmd3Rm9BVWkvb3hwNHQyM3NiMVBjQy8KSlFVZEJYaTVna0F3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUp3R1dVR1IzRVd3bnc2cVdoUVJRTHY1aUFuOQpESmVhb3o1M25VVmhuV0tzRWdETXhzcTgvcHRVK0NJalgvR3NXS2hSaE1nU3Q0ZHR1cEswMHROVnB3cGRQTExQCm5kWHRzZmhmQmRPV1hNWDVBSmNXNTQ2Sk02eVhkQ1VXbnBmSE5LTTJ5bnRqb0FPSXp4azNhUGcxeDNuckdCZVkKQzZ6UmhRdWRYTitwdFFNK0lRa0pYVzF4TWhSeVRoNHU2bXI3c2RTTGdQZ0dsaXZGNGE2Ukx3YWttOWdoNWJtOApIUkxYZWpuTVo0TFVUOExYWmgyR0tRenRrR1QwMkgxanNDMTZpYnJpbmNFSFpxUXVvd0Z3TUMxSUdhS1l0d1BkCnMwVFQxcFZBR3N1VzRlYUpPT3B2MmtpcVVITkExLzBrb1IwYnpscFhObFZIZzVySlY1dFBtWXp4YjRjPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== conditions: - lastTransitionTime: \"2022-02-17T05:48:20Z\" lastUpdateTime: \"2022-02-17T05:48:20Z\" message: This CSR was approved by kubectl certificate approve. reason: KubectlApprove status: \"True\" type: Approved The certificate value is in Base64-encoded format under status.certificate. Let us export the issued certificate from the CertificateSigningRequest. pradeep@learnk8s$ echo \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMrRENDQWVDZ0F3SUJBZ0lSQUlHNEdtUk5tdmczZENXZHhFcTBIUEV3RFFZSktvWklodmNOQVFFTEJRQXcKRlRFVE1CRUdBMVVFQXhNS2JXbHVhV3QxWW1WRFFUQWVGdzB5TWpBeU1UY3dOVFF6TWpCYUZ3MHlNakF5TVRndwpOVFF6TWpCYU1CSXhFREFPQmdOVkJBTVRCM0J5WVdSbFpYQXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRREZ1S3BLRFBoUWhDNlpJOWlDbXpiM01YVE9KWU5mOWRzbG45T2VNenRWcEd3d1BaQWMKeExTa0tDWmhsV0JMaXYraGppTFN6cldETFlENWF0cTUwV05Lc3JGSkROOVBTZVJnMDUxNHBkcFhoRWlYUDVKMgpSZFJUOEIxS3NwUmJjZmZSYVpMTVBXK1d0MFZYZE1zOC9rYS9WeGFXRWhvTW1mUHFnL1Y2Ly9EbkdlT1pqZ2M0CllZeWNlRnhLREIvOFlZeC9rRHJHWFliK2dwZnJlMkZZZmtvTExTNGt3KzJ1clhzYitSRkxVcU5tS2dXWnN3NlgKdDBiaXpWVjhIN3A3ajcwa3pNS0d1Z2dzY1V3Vi9wR0x0ajVIRUtqMHN0N0gwODNKWkpuakpyaVZBNzMyQzlhdQpxZ1pNT0pJUkt5MWxGQ3FYaTZ4cU52NklERXFxekZLcmR5UEhBZ01CQUFHalJqQkVNQk1HQTFVZEpRUU1NQW9HCkNDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUNNQUF3SHdZRFZSMGpCQmd3Rm9BVWkvb3hwNHQyM3NiMVBjQy8KSlFVZEJYaTVna0F3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUp3R1dVR1IzRVd3bnc2cVdoUVJRTHY1aUFuOQpESmVhb3o1M25VVmhuV0tzRWdETXhzcTgvcHRVK0NJalgvR3NXS2hSaE1nU3Q0ZHR1cEswMHROVnB3cGRQTExQCm5kWHRzZmhmQmRPV1hNWDVBSmNXNTQ2Sk02eVhkQ1VXbnBmSE5LTTJ5bnRqb0FPSXp4azNhUGcxeDNuckdCZVkKQzZ6UmhRdWRYTitwdFFNK0lRa0pYVzF4TWhSeVRoNHU2bXI3c2RTTGdQZ0dsaXZGNGE2Ukx3YWttOWdoNWJtOApIUkxYZWpuTVo0TFVUOExYWmgyR0tRenRrR1QwMkgxanNDMTZpYnJpbmNFSFpxUXVvd0Z3TUMxSUdhS1l0d1BkCnMwVFQxcFZBR3N1VzRlYUpPT3B2MmtpcVVITkExLzBrb1IwYnpscFhObFZIZzVySlY1dFBtWXp4YjRjPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\" | base64 -d > pradeep.crt Just to confirm, let us view the certificate. pradeep@learnk8s$ cat pradeep.crt -----BEGIN CERTIFICATE----- MIIC+DCCAeCgAwIBAgIRAIG4GmRNmvg3dCWdxEq0HPEwDQYJKoZIhvcNAQELBQAw FTETMBEGA1UEAxMKbWluaWt1YmVDQTAeFw0yMjAyMTcwNTQzMjBaFw0yMjAyMTgw NTQzMjBaMBIxEDAOBgNVBAMTB3ByYWRlZXAwggEiMA0GCSqGSIb3DQEBAQUAA4IB DwAwggEKAoIBAQDFuKpKDPhQhC6ZI9iCmzb3MXTOJYNf9dsln9OeMztVpGwwPZAc xLSkKCZhlWBLiv+hjiLSzrWDLYD5atq50WNKsrFJDN9PSeRg0514pdpXhEiXP5J2 RdRT8B1KspRbcffRaZLMPW+Wt0VXdMs8/ka/VxaWEhoMmfPqg/V6//DnGeOZjgc4 YYyceFxKDB/8YYx/kDrGXYb+gpfre2FYfkoLLS4kw+2urXsb+RFLUqNmKgWZsw6X t0bizVV8H7p7j70kzMKGuggscUwV/pGLtj5HEKj0st7H083JZJnjJriVA732C9au qgZMOJIRKy1lFCqXi6xqNv6IDEqqzFKrdyPHAgMBAAGjRjBEMBMGA1UdJQQMMAoG CCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUi/oxp4t23sb1PcC/ JQUdBXi5gkAwDQYJKoZIhvcNAQELBQADggEBAJwGWUGR3EWwnw6qWhQRQLv5iAn9 DJeaoz53nUVhnWKsEgDMxsq8/ptU+CIjX/GsWKhRhMgSt4dtupK00tNVpwpdPLLP ndXtsfhfBdOWXMX5AJcW546JM6yXdCUWnpfHNKM2yntjoAOIzxk3aPg1x3nrGBeY C6zRhQudXN+ptQM+IQkJXW1xMhRyTh4u6mr7sdSLgPgGlivF4a6RLwakm9gh5bm8 HRLXejnMZ4LUT8LXZh2GKQztkGT02H1jsC16ibrincEHZqQuowFwMC1IGaKYtwPd s0TT1pVAGsuW4eaJOOpv2kiqUHNA1/0koR0bzlpXNlVHg5rJV5tPmYzxb4c= -----END CERTIFICATE-----","title":"Kube Config"},{"location":"Kubernetes/#roles-and-rolebindings","text":"With the certificate created it is time to define the Role and RoleBinding for this user pradeep to access Kubernetes cluster resources. First let us create a role called developer who can create, get, list and update all pods. pradeep@learnk8s$ kubectl create role developer --verb=create --verb=get --verb=list --verb=update --verb=delete --resource=pods role.rbac.authorization.k8s.io/developer created Next, we have to bind this role to the user, by creating a role binding and giving it a name, in this case developer-pradeep . pradeep@learnk8s$ kubectl create rolebinding developer-pradeep --role=developer --user=pradeep rolebinding.rbac.authorization.k8s.io/developer-pradeep created To verify the role: pradeep@learnk8s$ kubectl get roles NAME CREATED AT developer 2022-02-17T06:25:08Z Rolebinding: pradeep@learnk8s$ kubectl get rolebindings NAME ROLE AGE developer-pradeep Role/developer 26s The last step is to add this user pradeep into the kubeconfig file. For this, first, you need to add new credentials: pradeep@learnk8s$ kubectl config set-credentials pradeep --client-key=pradeep.key --client-certificate=pradeep.crt --embed-certs=true User \"pradeep\" set. Then, you need to add the context: pradeep@learnk8s$ kubectl config set-context pradeep --cluster=k8s --user=pradeep Context \"pradeep\" created. Let us check the available contexts first. pradeep@learnk8s$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * k8s k8s k8s default pradeep k8s pradeep We can see there are two, k8s the old (default) one and the pradeep context which is new. Looking at the first column CURRENT , the * mark is present for the k8s context, indicating that it is the active context. With this change, let us take another look at the kubectl config file. pradeep@learnk8s$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /Users/pradeep/.minikube/ca.crt extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: cluster_info server: https://192.168.177.29:8443 name: k8s contexts: - context: cluster: k8s extensions: - extension: last-update: Tue, 15 Feb 2022 12:28:03 IST provider: minikube.sigs.k8s.io version: v1.25.1 name: context_info namespace: default user: k8s name: k8s - context: cluster: k8s user: pradeep name: pradeep current-context: pradeep kind: Config preferences: {} users: - name: k8s user: client-certificate: /Users/pradeep/.minikube/profiles/k8s/client.crt client-key: /Users/pradeep/.minikube/profiles/k8s/client.key - name: pradeep user: client-certificate-data: REDACTED client-key-data: REDACTED We can see that , there is still one cluster, but two contexts and two users. It is time to test all the work done in the last half an hour by switching the context. pradeep@learnk8s$ kubectl config use-context pradeep Switched to context \"pradeep\". Verify it one more way. pradeep@learnk8s$ kubectl config current-context pradeep Now that we are sure of the current-context, let us get the pods as this new user pradeep . pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE kodekloud-8477b7849-blzrs 1/1 Running 0 46h kodekloud-8477b7849-jkfmr 1/1 Running 0 46h kodekloud-8477b7849-m65m8 1/1 Running 0 46h kodekloud-8477b7849-rgbmn 1/1 Running 0 46h kodekloud-change-color 1/1 Running 0 43h kodekloud-cm 1/1 Running 0 39h kodekloud-env-color 1/1 Running 0 40h kodekloud-env-color-2 1/1 Running 0 40h kodekloud-secret 1/1 Running 0 29h multi-containers 1/2 NotReady 0 28h pod-with-init-container 1/1 Running 14 (12m ago) 28h Yay! :clap: We could get the pods as user pradeep . Let us try to get the nodes this time. pradeep@learnk8s$ kubectl get nodes Error from server (Forbidden): nodes is forbidden: User \"pradeep\" cannot list resource \"nodes\" in API group \"\" at the cluster scope Try accessing deployments. pradeep@learnk8s$ kubectl get deployments Error from server (Forbidden): deployments.apps is forbidden: User \"pradeep\" cannot list resource \"deployments\" in API group \"apps\" in the namespace \"default\" What happened? It is Forbidden. But, this is expected, right?! We only gave permission for Pods. Isn't it? We can't verify that (if we don't remember what we have given for this user/role) from the current context. pradeep@learnk8s$ kubectl describe roles developer Error from server (Forbidden): roles.rbac.authorization.k8s.io \"developer\" is forbidden: User \"pradeep\" cannot get resource \"roles\" in API group \"rbac.authorization.k8s.io\" in the namespace \"default\" So, we have to switch the context back to k8s . pradeep@learnk8s$ kubectl config use-context k8s Switched to context \"k8s\". Describe the developer role and check what all Resources this role has access to. pradeep@learnk8s$ kubectl describe roles developer Name: developer Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [create get list update delete] This confirms that, any user who is given this role of developer can only work with pods . Here is the same in YAML form. pradeep@learnk8s$ kubectl get roles developer -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: \"2022-02-17T06:25:08Z\" name: developer namespace: default resourceVersion: \"64839\" uid: 9eae4036-0858-4c09-8c06-7532d74ca1aa rules: - apiGroups: - \"\" resources: - pods verbs: - create - get - list - update - delete How can we give access to deployments for this developer? We just need to add additional resource of deployments. Let us edit the developer role. pradeep@learnk8s$ kubectl edit role developer role.rbac.authorization.k8s.io/developer edited Describe the role after editing it, just to confirm we have additional access to deployments. We have modified few Verbs as well (removed some access!) pradeep@learnk8s$ kubectl describe role developer Name: developer Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- deployments [] [] [get list] pods [] [] [get list] This is modified YAML version of the developer role. pradeep@learnk8s$ kubectl get role developer -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: \"2022-02-17T06:25:08Z\" name: developer namespace: default resourceVersion: \"66960\" uid: 9eae4036-0858-4c09-8c06-7532d74ca1aa rules: - apiGroups: - \"\" resources: - deployments - pods verbs: - get - list Now switch back to the pradeep context and verify if you can get the nodes. pradeep@learnk8s$ kubectl config use-context pradeep Switched to context \"pradeep\". Let us see if pradeep can get deployments. pradeep@learnk8s$ kubectl get deployments Error from server (Forbidden): deployments.apps is forbidden: User \"pradeep\" cannot list resource \"deployments\" in API group \"apps\" in the namespace \"default\" Still Forbidden. It is do with the apiGroups settings. Currently we did not specify anything, it was left blank. We can edit again, change the apiGroups to any with the * . Though in production environment, we might want to be more specific and only configured limited groups as needed. pradeep@learnk8s$ kubectl config use-context k8s Switched to context \"k8s\". pradeep@learnk8s$ kubectl edit role developer role.rbac.authorization.k8s.io/developer edited Look at the apiGroups value of * . pradeep@learnk8s$ kubectl get role developer -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: \"2022-02-17T06:25:08Z\" name: developer namespace: default resourceVersion: \"67224\" uid: 9eae4036-0858-4c09-8c06-7532d74ca1aa rules: - apiGroups: - '*' resources: - deployments - pods verbs: - get - list To test if this is working or not for pradeep user, there is another away, without switching context. We can make use of the --as option (impersonation). Before modifying the apiGroup setting, it was like this, for the same test. pradeep@learnk8s$ kubectl get deployment --as pradeep Error from server (Forbidden): deployments.apps is forbidden: User \"pradeep\" cannot list resource \"deployments\" in API group \"apps\" in the namespace \"default\" Now that we modified, pradeep@learnk8s$ kubectl get deployment --as pradeep NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 2d Though this test confirms that pradeep can get the deployments, let us switch context and verify one more time. pradeep@learnk8s$ kubectl config use-context pradeep Switched to context \"pradeep\". pradeep@learnk8s$ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE kodekloud 4/4 4 4 2d :tada: Congratulations!","title":"Roles and RoleBindings"},{"location":"Kubernetes/#cluster-roles-and-cluster-role-bindings","text":"If we notice, user pradeep still can't get the nodes information. It is becuase, he still does not have access to resources at the cluster scope. pradeep@learnk8s$ kubectl get nodes Error from server (Forbidden): nodes is forbidden: User \"pradeep\" cannot list resource \"nodes\" in API group \"\" at the cluster scope To get this access, we need to create a Cluste role and Cluste role binding. pradeep@learnk8s$ kubectl config use-context k8s Switched to context \"k8s\". pradeep@learnk8s$ kubectl create clusterrole pradeep-cluster --verb=get,list,watch, --resource=nodes Warning: '' is not a standard resource verb clusterrole.rbac.authorization.k8s.io/pradeep-cluster created Get all available clusterroles. There are many but we can see our newly created clusterrole pradeep-cluster . pradeep@learnk8s$ kubectl get clusterrole NAME CREATED AT admin 2022-02-15T06:57:58Z cluster-admin 2022-02-15T06:57:58Z edit 2022-02-15T06:57:58Z kindnet 2022-02-15T06:58:03Z kubeadm:get-nodes 2022-02-15T06:58:01Z pradeep-cluster 2022-02-17T07:33:43Z system:aggregate-to-admin 2022-02-15T06:57:58Z system:aggregate-to-edit 2022-02-15T06:57:58Z system:aggregate-to-view 2022-02-15T06:57:58Z system:auth-delegator 2022-02-15T06:57:58Z system:basic-user 2022-02-15T06:57:58Z system:certificates.k8s.io:certificatesigningrequests:nodeclient 2022-02-15T06:57:58Z system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 2022-02-15T06:57:58Z system:certificates.k8s.io:kube-apiserver-client-approver 2022-02-15T06:57:58Z system:certificates.k8s.io:kube-apiserver-client-kubelet-approver 2022-02-15T06:57:58Z system:certificates.k8s.io:kubelet-serving-approver 2022-02-15T06:57:58Z system:certificates.k8s.io:legacy-unknown-approver 2022-02-15T06:57:58Z system:controller:attachdetach-controller 2022-02-15T06:57:58Z system:controller:certificate-controller 2022-02-15T06:57:58Z system:controller:clusterrole-aggregation-controller 2022-02-15T06:57:58Z system:controller:cronjob-controller 2022-02-15T06:57:58Z system:controller:daemon-set-controller 2022-02-15T06:57:58Z system:controller:deployment-controller 2022-02-15T06:57:58Z system:controller:disruption-controller 2022-02-15T06:57:58Z system:controller:endpoint-controller 2022-02-15T06:57:58Z system:controller:endpointslice-controller 2022-02-15T06:57:58Z system:controller:endpointslicemirroring-controller 2022-02-15T06:57:58Z system:controller:ephemeral-volume-controller 2022-02-15T06:57:58Z system:controller:expand-controller 2022-02-15T06:57:58Z system:controller:generic-garbage-collector 2022-02-15T06:57:58Z system:controller:horizontal-pod-autoscaler 2022-02-15T06:57:58Z system:controller:job-controller 2022-02-15T06:57:58Z system:controller:namespace-controller 2022-02-15T06:57:58Z system:controller:node-controller 2022-02-15T06:57:58Z system:controller:persistent-volume-binder 2022-02-15T06:57:58Z system:controller:pod-garbage-collector 2022-02-15T06:57:58Z system:controller:pv-protection-controller 2022-02-15T06:57:58Z system:controller:pvc-protection-controller 2022-02-15T06:57:58Z system:controller:replicaset-controller 2022-02-15T06:57:58Z system:controller:replication-controller 2022-02-15T06:57:58Z system:controller:resourcequota-controller 2022-02-15T06:57:58Z system:controller:root-ca-cert-publisher 2022-02-15T06:57:58Z system:controller:route-controller 2022-02-15T06:57:58Z system:controller:service-account-controller 2022-02-15T06:57:58Z system:controller:service-controller 2022-02-15T06:57:58Z system:controller:statefulset-controller 2022-02-15T06:57:58Z system:controller:ttl-after-finished-controller 2022-02-15T06:57:58Z system:controller:ttl-controller 2022-02-15T06:57:58Z system:coredns 2022-02-15T06:58:01Z system:discovery 2022-02-15T06:57:58Z system:heapster 2022-02-15T06:57:58Z system:kube-aggregator 2022-02-15T06:57:58Z system:kube-controller-manager 2022-02-15T06:57:58Z system:kube-dns 2022-02-15T06:57:58Z system:kube-scheduler 2022-02-15T06:57:58Z system:kubelet-api-admin 2022-02-15T06:57:58Z system:monitoring 2022-02-15T06:57:58Z system:node 2022-02-15T06:57:58Z system:node-bootstrapper 2022-02-15T06:57:58Z system:node-problem-detector 2022-02-15T06:57:58Z system:node-proxier 2022-02-15T06:57:58Z system:persistent-volume-provisioner 2022-02-15T06:57:58Z system:public-info-viewer 2022-02-15T06:57:58Z system:service-account-issuer-discovery 2022-02-15T06:57:58Z system:volume-scheduler 2022-02-15T06:57:58Z view 2022-02-15T06:57:58Z Create a clusterrolebinding called pradeep-cluster-binding and bind the user pradeep and clusterrole pradeep-cluster . pradeep@learnk8s$ kubectl create clusterrolebinding pradeep-cluster-binding --clusterrole=pradeep-cluster --user=pradeep clusterrolebinding.rbac.authorization.k8s.io/pradeep-cluster-binding created Get all available clusterrolebindings. pradeep@learnk8s$ kubectl get clusterrolebindings.rbac.authorization.k8s.io NAME ROLE AGE cluster-admin ClusterRole/cluster-admin 2d kindnet ClusterRole/kindnet 2d kubeadm:get-nodes ClusterRole/kubeadm:get-nodes 2d kubeadm:kubelet-bootstrap ClusterRole/system:node-bootstrapper 2d kubeadm:node-autoapprove-bootstrap ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient 2d kubeadm:node-autoapprove-certificate-rotation ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 2d kubeadm:node-proxier ClusterRole/system:node-proxier 2d minikube-rbac ClusterRole/cluster-admin 2d pradeep-cluster-binding ClusterRole/pradeep-cluster 5s storage-provisioner ClusterRole/system:persistent-volume-provisioner 2d system:basic-user ClusterRole/system:basic-user 2d system:controller:attachdetach-controller ClusterRole/system:controller:attachdetach-controller 2d system:controller:certificate-controller ClusterRole/system:controller:certificate-controller 2d system:controller:clusterrole-aggregation-controller ClusterRole/system:controller:clusterrole-aggregation-controller 2d system:controller:cronjob-controller ClusterRole/system:controller:cronjob-controller 2d system:controller:daemon-set-controller ClusterRole/system:controller:daemon-set-controller 2d system:controller:deployment-controller ClusterRole/system:controller:deployment-controller 2d system:controller:disruption-controller ClusterRole/system:controller:disruption-controller 2d system:controller:endpoint-controller ClusterRole/system:controller:endpoint-controller 2d system:controller:endpointslice-controller ClusterRole/system:controller:endpointslice-controller 2d system:controller:endpointslicemirroring-controller ClusterRole/system:controller:endpointslicemirroring-controller 2d system:controller:ephemeral-volume-controller ClusterRole/system:controller:ephemeral-volume-controller 2d system:controller:expand-controller ClusterRole/system:controller:expand-controller 2d system:controller:generic-garbage-collector ClusterRole/system:controller:generic-garbage-collector 2d system:controller:horizontal-pod-autoscaler ClusterRole/system:controller:horizontal-pod-autoscaler 2d system:controller:job-controller ClusterRole/system:controller:job-controller 2d system:controller:namespace-controller ClusterRole/system:controller:namespace-controller 2d system:controller:node-controller ClusterRole/system:controller:node-controller 2d system:controller:persistent-volume-binder ClusterRole/system:controller:persistent-volume-binder 2d system:controller:pod-garbage-collector ClusterRole/system:controller:pod-garbage-collector 2d system:controller:pv-protection-controller ClusterRole/system:controller:pv-protection-controller 2d system:controller:pvc-protection-controller ClusterRole/system:controller:pvc-protection-controller 2d system:controller:replicaset-controller ClusterRole/system:controller:replicaset-controller 2d system:controller:replication-controller ClusterRole/system:controller:replication-controller 2d system:controller:resourcequota-controller ClusterRole/system:controller:resourcequota-controller 2d system:controller:root-ca-cert-publisher ClusterRole/system:controller:root-ca-cert-publisher 2d system:controller:route-controller ClusterRole/system:controller:route-controller 2d system:controller:service-account-controller ClusterRole/system:controller:service-account-controller 2d system:controller:service-controller ClusterRole/system:controller:service-controller 2d system:controller:statefulset-controller ClusterRole/system:controller:statefulset-controller 2d system:controller:ttl-after-finished-controller ClusterRole/system:controller:ttl-after-finished-controller 2d system:controller:ttl-controller ClusterRole/system:controller:ttl-controller 2d system:coredns ClusterRole/system:coredns 2d system:discovery ClusterRole/system:discovery 2d system:kube-controller-manager ClusterRole/system:kube-controller-manager 2d system:kube-dns ClusterRole/system:kube-dns 2d system:kube-scheduler ClusterRole/system:kube-scheduler 2d system:monitoring ClusterRole/system:monitoring 2d system:node ClusterRole/system:node 2d system:node-proxier ClusterRole/system:node-proxier 2d system:public-info-viewer ClusterRole/system:public-info-viewer 2d system:service-account-issuer-discovery ClusterRole/system:service-account-issuer-discovery 2d system:volume-scheduler ClusterRole/system:volume-scheduler 2d Again, there are many pre-defined but our newly created clusterrolebinding pradeep-cluster-binding is shown. With that confirmation, let us verify if user pradeep can get the nodes without context-switching. pradeep@learnk8s$ kubectl get nodes --as pradeep NAME STATUS ROLES AGE VERSION k8s Ready control-plane,master 2d v1.23.1 k8s-m02 Ready <none> 2d v1.23.1 Before switching context, let us describe these clusterrole and clusterrolebindings. ClusterRole: pradeep@learnk8s$ kubectl describe clusterrole pradeep-cluster Name: pradeep-cluster Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- nodes [] [] [get list watch ] ClusterRoleBinding: pradeep@learnk8s$ kubectl describe clusterrolebindings pradeep-cluster-binding Name: pradeep-cluster-binding Labels: <none> Annotations: <none> Role: Kind: ClusterRole Name: pradeep-cluster Subjects: Kind Name Namespace ---- ---- --------- User pradeep Switch context now. pradeep@learnk8s$ kubectl config use-context pradeep Switched to context \"pradeep\". Finally, verify user pradeep can get the nodes. pradeep@learnk8s$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s Ready control-plane,master 2d v1.23.1 k8s-m02 Ready <none> 2d v1.23.1 Now, we have accomplished our simple goal w.r.t authorization in Kubernetes.","title":"Cluster Roles and Cluster Role Bindings"},{"location":"Kubernetes/#kubectl-auth","text":"To Inspect authorization, we can make use of kubectl auth command. It will show an :+1: or :no_entry_sign:. Here is the related help and examples. pradeep@learnk8s$ kubectl auth can-i -h Check whether an action is allowed. VERB is a logical Kubernetes API verb like 'get', 'list', 'watch', 'delete', etc. TYPE is a Kubernetes resource. Shortcuts and groups will be resolved. NONRESOURCEURL is a partial URL that starts with \"/\". NAME is the name of a particular Kubernetes resource. This command pairs nicely with impersonation. See --as global flag. Examples: # Check to see if I can create pods in any namespace kubectl auth can-i create pods --all-namespaces # Check to see if I can list deployments in my current namespace kubectl auth can-i list deployments.apps # Check to see if I can do everything in my current namespace (\"*\" means all) kubectl auth can-i '*' '*' # Check to see if I can get the job named \"bar\" in namespace \"foo\" kubectl auth can-i list jobs.batch/bar -n foo # Check to see if I can read pod logs kubectl auth can-i get pods --subresource=log # Check to see if I can access the URL /logs/ kubectl auth can-i get /logs/ # List all allowed actions in namespace \"foo\" kubectl auth can-i --list --namespace=foo Options: -A, --all-namespaces=false: If true, check the specified action in all namespaces. --list=false: If true, prints all allowed actions. --no-headers=false: If true, prints allowed actions without headers -q, --quiet=false: If true, suppress output and just return the exit code. --subresource='': SubResource such as pod/log or deployment/scale Usage: kubectl auth can-i VERB [TYPE | TYPE/NAME | NONRESOURCEURL] [options] Use \"kubectl options\" for a list of global command-line options (applies to all commands). For example, from the pradeep context, pradeep@learnk8s$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE k8s k8s k8s default * pradeep k8s pradeep pradeep@learnk8s$ kubectl auth can-i '*' '*' no From the k8s context, pradeep@learnk8s$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * k8s k8s k8s default pradeep k8s pradeep pradeep@learnk8s$ kubectl auth can-i '*' '*' yes","title":"Kubectl Auth"},{"location":"Kubernetes/#security-context","text":"To understand security context, first let us try to create a Pod without any security context and check few things like, whoami , id , and ps command outputs to verify the current user inside the Pod. pradeep@learnk8s$ cat no-security-context-demo.yaml apiVersion: v1 kind: Pod metadata: name: no-security-context-demo spec: containers: - name: no-security-context-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] pradeep@learnk8s$ kubectl create -f no-security-context-demo.yaml pod/no-security-context-demo created Login to the Pod, and launch the shell . pradeep@learnk8s$ kubectl exec -it no-security-context-demo -- sh / # whoami root / # id uid=0(root) gid=0(root) groups=10(wheel) / # ps PID USER TIME COMMAND 1 root 0:00 sleep 1h 9 root 0:00 sh 18 root 0:00 ps / # exit 0 From all three command outputs, it is clear that it is the root user with id 0 . Now, we can make use of a security context to define privilege and access control settings for a Pod or Container. For example, based on user ID (UID) and group ID (GID). Let us modify the Pod defintion to include securityContext and create another Pod. pradeep@learnk8s$ cat security-context-demo.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: runAsUser: 1000 containers: - name: security-context-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] pradeep@learnk8s$ kubectl create -f security-context-demo.yaml pod/security-context-demo created pradeep@learnk8s$ kubectl get pods | grep security no-security-context-demo 1/1 Running 0 9m24s security-context-demo 1/1 Running 0 31s pradeep@learnk8s$ kubectl exec -it security-context-demo -- sh / $ whoami whoami: unknown uid 1000 / $ id uid=1000 gid=0(root) / $ ps PID USER TIME COMMAND 1 1000 0:00 sleep 1h 7 1000 0:00 sh 16 1000 0:00 ps / $ cat /etc/passwd root:x:0:0:root:/root:/bin/sh daemon:x:1:1:daemon:/usr/sbin:/bin/false bin:x:2:2:bin:/bin:/bin/false sys:x:3:3:sys:/dev:/bin/false sync:x:4:100:sync:/bin:/bin/sync mail:x:8:8:mail:/var/spool/mail:/bin/false www-data:x:33:33:www-data:/var/www:/bin/false operator:x:37:37:Operator:/var:/bin/false nobody:x:65534:65534:nobody:/home:/bin/false / $ We can see the difference. Now, inside the Pod, its all the user with ID 1000 as defined in our security context. As we are still inside the security-context-demo pod, let us check few things like date and try to modify the date value. / $ date Sat Feb 19 01:06:08 UTC 2022 / $ date -s 202206020937 date: can't set date: Operation not permitted Thu Jun 2 09:37:00 UTC 2022 / $ date Sat Feb 19 01:08:11 UTC 2022 / $ It says that date: can't set date: Operation not permitted and date did not change. Let us create a new Pod definition to run the container as root user with SYS_TIME capability. pradeep@learnk8s$ cat security-context-cap-demo.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo-cap spec: containers: - name: security-context-demo-cap image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] securityContext: runAsUser: 0 capabilities: add: [\"SYS_TIME\"] pradeep@learnk8s$ kubectl create -f security-context-cap-demo.yaml pod/security-context-demo-cap created pradeep@learnk8s$ kubectl exec -it security-context-demo-cap -- sh / # id uid=0(root) gid=0(root) groups=10(wheel) / # whoami root / # ps PID USER TIME COMMAND 1 root 0:00 sleep 1h 9 root 0:00 sh 18 root 0:00 ps / # date -s 202206020937 Thu Jun 2 09:37:00 UTC 2022 / # date Sat Feb 19 13:29:05 UTC 2022 This time, it is the root user and setting date worked fine (partially!). Important difference to observe is that there is no Operation not permitted warning. It seems to took the new date, but issuing a date again, still shows the current date. I haven't explored this further yet.","title":"Security Context"},{"location":"Kubernetes/#storage","text":"","title":"Storage"},{"location":"Kubernetes/#volumes","text":"","title":"Volumes"},{"location":"Kubernetes/#emptydir","text":"There are many volume types. An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently. Here is an example for creating a volume of type emptyDir and use it in a Pod. pradeep@learnk8s$ more pod-with-emptydir-volume.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-emptydir-volume spec: volumes: - name: logs-volume emptyDir: {} containers: - image: nginx name: pod-with-emptydir-volume volumeMounts: - mountPath: /var/logs name: logs-volume pradeep@learnk8s$ kubectl create -f pod-with-emptydir-volume.yaml pod/pod-with-emptydir-volume created pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 9s pradeep@learnk8s$ kubectl exec -it pod-with-emptydir-volume -- /bin/sh # ls bin boot dev docker-entrypoint.d docker-entrypoint.sh etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var # cd /var/logs # ls # pwd /var/logs # touch testing-volumes.txt # ls testing-volumes.txt # exit 0","title":"emptyDir"},{"location":"Kubernetes/#hostpath","text":"pradeep@learnk8s$ cat pod-with-hostpath-volume.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-hostpath-volume spec: containers: - image: nginx name: pod-with-hostpath-volume volumeMounts: - mountPath: /usr/share/nginx/html name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data-for-pod-with-hostpath-volume # this field is optional type: Directory pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ls $ sudo mkdir /data-for-pod-with-hostpath-volume $ sudo vi /data-for-pod-with-hostpath-volume/index.html $ sudo cat /data-for-pod-with-hostpath-volume/index.html testing Pod with hostPath volume mount! $ exit logout pradeep@learnk8s$ kubectl create -f pod-with-hostpath-volume.yaml pod/pod-with-hostpath-volume created pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 13m pod-with-hostpath-volume 0/1 ContainerCreating 0 53s Even after 50+ seconds, the pod-with-hostpath-volume seems to be not yet Running. Let us describe it to understand the reason. Pay attention to Mounts section under Containers and Volumes section. Finally look at the events. pradeep@learnk8s$ kubectl describe pods pod-with-hostpath-volume Name: pod-with-hostpath-volume Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Sun, 20 Feb 2022 18:43:42 +0530 Labels: <none> Annotations: <none> Status: Pending IP: IPs: <none> Containers: pod-with-hostpath-volume: Container ID: Image: nginx Image ID: Port: <none> Host Port: <none> State: Waiting Reason: ContainerCreating Ready: False Restart Count: 0 Environment: <none> Mounts: /usr/share/nginx/html from test-volume (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f7lsg (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: test-volume: Type: HostPath (bare host directory volume) Path: /data-for-pod-with-hostpath-volume HostPathType: Directory kube-api-access-f7lsg: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 61s default-scheduler Successfully assigned default/pod-with-hostpath-volume to k8s-m02 Warning FailedMount 28s (x7 over 60s) kubelet MountVolume.SetUp failed for volume \"test-volume\" : hostPath type check failed: /data-for-pod-with-hostpath-volume is not a directory From the events, we can see that the default-scheduler tried to create this pod on the node k8s-m02 but MountVolume.SetUp failed for volume test-volume : hostPath type check failed: /data-for-pod-with-hostpath-volume is not a directory. This is expected, right. A moment ago, we created a directory with this name on the k8s node. To fix this, we can manually schedule this pod on the k8s node with the nodeName spec. We did something like this already. Didn't we? First, let us delete the pod which is still in creatingContainer stage. pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 21m pod-with-hostpath-volume 0/1 ContainerCreating 0 9m32s pradeep@learnk8s$ kubectl delete pods pod-with-hostpath-volume pod \"pod-with-hostpath-volume\" deleted Modify the YAML definition file like this. Only thing different (from the previous definition) here is the nodeName: k8s line. pradeep@learnk8s$ cat pod-with-hostpath-volume.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-hostpath-volume spec: nodeName: k8s containers: - image: nginx name: pod-with-hostpath-volume volumeMounts: - mountPath: /usr/share/nginx/html name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data-for-pod-with-hostpath-volume # this field is optional type: Directory pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 26m pod-with-hostpath-volume 1/1 Running 0 42s This time it is running fine. If we describe this Pod, pradeep@learnk8s$ kubectl describe pods pod-with-hostpath-volume Name: pod-with-hostpath-volume Namespace: default Priority: 0 Node: k8s/192.168.177.29 Start Time: Sun, 20 Feb 2022 18:56:51 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.0.18 IPs: IP: 10.244.0.18 Containers: pod-with-hostpath-volume: Container ID: docker://f1e79dd787c5327766bbdde7b35cab8314de09e1c406fe274ee3f7c364cc5845 Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: <none> Host Port: <none> State: Running Started: Sun, 20 Feb 2022 18:57:25 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /usr/share/nginx/html from test-volume (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7fv4m (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: test-volume: Type: HostPath (bare host directory volume) Path: /data-for-pod-with-hostpath-volume HostPathType: Directory kube-api-access-7fv4m: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Pulling 50s kubelet Pulling image \"nginx\" Normal Pulled 19s kubelet Successfully pulled image \"nginx\" in 30.708250752s Normal Created 19s kubelet Created container pod-with-hostpath-volume Normal Started 18s kubelet Started container pod-with-hostpath-volume Get the IP assigned to the Pod, either from the previous command or with the -o wide option. pradeep@learnk8s$ kubectl get pods -o wide | grep volume pod-with-emptydir-volume 1/1 Running 0 38m 10.244.1.39 k8s-m02 <none> <none> pod-with-hostpath-volume 1/1 Running 0 13m 10.244.0.18 k8s <none> <none> Let us connect to this Pod on the IP address assigned 10.244.0.18 . pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ curl 10.244.0.18 testing Pod with hostPath volume mount! The test is working. The nginx container in the pod-with-hostpath-volume is readigng the data from the host directory.","title":"HostPath"},{"location":"Kubernetes/#config-maps","text":"A ConfigMap provides a way to inject configuration data into pods. The data stored in a ConfigMap can be referenced in a volume of type configMap and then consumed by containerized applications running in a pod. https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/ Let us try this example as it is in our Minikube setup. pradeep@learnk8s$ cat redis-pod-configmap.yaml apiVersion: v1 kind: Pod metadata: name: redis spec: containers: - name: redis image: redis:5.0.4 command: - redis-server - \"/redis-master/redis.conf\" env: - name: MASTER value: \"true\" ports: - containerPort: 6379 resources: limits: cpu: \"0.1\" volumeMounts: - mountPath: /redis-master-data name: data - mountPath: /redis-master name: config volumes: - name: data emptyDir: {} - name: config configMap: name: example-redis-config items: - key: redis-config path: redis.conf pradeep@learnk8s$ cat <<EOF >./example-redis-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: example-redis-config data: redis-config: \"\" EOF pradeep@learnk8s$ kubectl apply -f example-redis-config.yaml configmap/example-redis-config created pradeep@learnk8s$ kubectl apply -f redis-pod-configmap.yaml pod/redis created pradeep@learnk8s$ kubectl get pod/redis configmap/example-redis-config -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/redis 1/1 Running 0 53s 10.244.1.40 k8s-m02 <none> <none> NAME DATA AGE configmap/example-redis-config 1 64s Describe the configmap. pradeep@learnk8s$ kubectl describe configmap/example-redis-config Name: example-redis-config Namespace: default Labels: <none> Annotations: <none> Data ==== redis-config: ---- BinaryData ==== Events: <none> You should see an empty redis-config key. Use kubectl exec to enter the pod and run the redis-cli tool to check the current configuration: Check maxmemory and maxmemory-policy . pradeep@learnk8s$ kubectl exec -it redis -- redis-cli 127.0.0.1:6379> CONFIG GET maxmemory 1) \"maxmemory\" 2) \"0\" 127.0.0.1:6379> CONFIG GET maxmemory-policy 1) \"maxmemory-policy\" 2) \"noeviction\" 127.0.0.1:6379> Now let's add some configuration values to the example-redis-config ConfigMap: pradeep@learnk8s$ cat example-redis-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: example-redis-config data: redis-config: | maxmemory 2mb maxmemory-policy allkeys-lru Apply the updated configmap. pradeep@learnk8s$ kubectl apply -f example-redis-config.yaml configmap/example-redis-config configured Confirm configmap is updated. pradeep@learnk8s$ kubectl describe configmap/example-redis-config Name: example-redis-config Namespace: default Labels: <none> Annotations: <none> Data ==== redis-config: ---- maxmemory 2mb maxmemory-policy allkeys-lru BinaryData ==== Events: <none> Check the Redis Pod again using redis-cli via kubectl exec to see if the configuration was applied: pradeep@learnk8s$ kubectl exec -it redis -- redis-cli 127.0.0.1:6379> CONFIG GET maxmemory 1) \"maxmemory\" 2) \"0\" 127.0.0.1:6379> CONFIG GET maxmemory-policy 1) \"maxmemory-policy\" 2) \"noeviction\" 127.0.0.1:6379> exit Are the changes reflecting now? Nope! Not yet. The configuration values have not changed because the Pod needs to be restarted to grab updated values from associated ConfigMaps. Let's delete and recreate the Pod: pradeep@learnk8s$ kubectl delete pod redis pod \"redis\" deleted pradeep@learnk8s$ kubectl apply -f redis-pod-configmap.yaml pod/redis created pradeep@learnk8s$ kubectl get pods | grep redis redis 1/1 Running 0 10s pradeep@learnk8s$ kubectl exec -it redis -- redis-cli 127.0.0.1:6379> CONFIG GET maxmemory 1) \"maxmemory\" 2) \"2097152\" 127.0.0.1:6379> CONFIG GET maxmemory-policy 1) \"maxmemory-policy\" 2) \"allkeys-lru\" 127.0.0.1:6379> exit This time we can seen updated (from ConfigMap) values inside the redis container.","title":"Config Maps"},{"location":"Kubernetes/#secrets_1","text":"We are re-visiting secrets again, but this time not as an Envrionment variable but as a volume Mount. pradeep@learnk8s$ kubectl create secret generic my-secret --from-literal=user=pradeep --from-literal=password=topsecret secret/my-secret created pradeep@learnk8s$ kubectl describe secrets my-secret Name: my-secret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== password: 9 bytes user: 7 bytes Now, let us consume this secret as a volume in a Pod. pradeep@learnk8s$ cat pod-with-secret-volume.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-secret-volume spec: containers: - name: pod-with-secret-volume image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: my-secret pradeep@learnk8s$ kubectl create -f pod-with-secret-volume.yaml pod/pod-with-secret-volume created pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 81m pod-with-hostpath-volume 1/1 Running 0 55m pod-with-secret-volume 1/1 Running 0 15s pradeep@learnk8s$ kubectl describe pod pod-with-secret-volume Name: pod-with-secret-volume Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Sun, 20 Feb 2022 19:52:10 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.42 IPs: IP: 10.244.1.42 Containers: pod-with-secret-volume: Container ID: docker://a21403023c1bb4d5348f1aa1eee282072394e3c6785a8f0ef1203f67355f357b Image: redis Image ID: docker-pullable://redis@sha256:0d9c9aed1eb385336db0bc9b976b6b49774aee3d2b9c2788a0d0d9e239986cb3 Port: <none> Host Port: <none> State: Running Started: Sun, 20 Feb 2022 19:52:24 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /etc/foo from foo (ro) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wlmjs (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: foo: Type: Secret (a volume populated by a Secret) SecretName: my-secret Optional: false kube-api-access-wlmjs: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 93s default-scheduler Successfully assigned default/pod-with-secret-volume to k8s-m02 Normal Pulling 91s kubelet Pulling image \"redis\" Normal Pulled 80s kubelet Successfully pulled image \"redis\" in 11.292924683s Normal Created 80s kubelet Created container pod-with-secret-volume Normal Started 79s kubelet Started container pod-with-secret-volume pradeep@learnk8s$ kubectl exec -it pod-with-secret-volume -- sh # ls /etc/foo password user # ls -l /etc/foo total 0 lrwxrwxrwx 1 root root 15 Feb 20 14:22 password -> ..data/password lrwxrwxrwx 1 root root 11 Feb 20 14:22 user -> ..data/user # # cat /etc/foo/user pradeep# # cat /etc/foo/password topsecret# # exit One problem here is that the password is in plain text. We can create another YAML defitinition with a slight modification. pradeep@learnk8s$ cat pod-with-secret-volume-2.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-secret-volume-2 spec: containers: - name: pod-with-secret-volume-2 image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: my-secret items: - key: user path: my-group/my-username mode: 0777 pradeep@learnk8s$ kubectl create -f pod-with-secret-volume-2.yaml pod/pod-with-secret-volume-2 created kubectl describe pod pod-with-secret-volume-2 Name: pod-with-secret-volume-2 Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Sun, 20 Feb 2022 20:10:40 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.43 IPs: IP: 10.244.1.43 Containers: pod-with-secret-volume-2: Container ID: docker://56198fa123177a7c913973c16e686d25df1eeafa0787eef39fbf5306ac7e0b1d Image: redis Image ID: docker-pullable://redis@sha256:0d9c9aed1eb385336db0bc9b976b6b49774aee3d2b9c2788a0d0d9e239986cb3 Port: <none> Host Port: <none> State: Running Started: Sun, 20 Feb 2022 20:10:47 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /etc/foo from foo (ro) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fwthr (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: foo: Type: Secret (a volume populated by a Secret) SecretName: my-secret Optional: false kube-api-access-fwthr: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 38s default-scheduler Successfully assigned default/pod-with-secret-volume-2 to k8s-m02 Normal Pulling 36s kubelet Pulling image \"redis\" Normal Pulled 32s kubelet Successfully pulled image \"redis\" in 4.744969468s Normal Created 32s kubelet Created container pod-with-secret-volume-2 Normal Started 31s kubelet Started container pod-with-secret-volume-2 pradeep@learnk8s$ kubectl exec -it pod-with-secret-volume-2 -- sh # ls /etc/foo my-group # ls -l /etc/foo total 0 lrwxrwxrwx 1 root root 15 Feb 20 14:40 my-group -> ..data/my-group # cat /etc/foo/my-group/my-username pradeep# at /etc/foo/my-group/my-password sh: 9: at: not found Only username is exposed while the password is not projected becuase the key is not exposed. pradeep@learnk8s$ cat pod-with-secret-volume-3.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-secret-volume-3 spec: containers: - name: pod-with-secret-volume-3 image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: my-secret items: - key: user mode: 0777 path: my-group/my-username - key: password mode: 0777 path: my-group/my-pass pradeep@learnk8s$ kubectl create -f pod-with-secret-volume-3.yaml pod/pod-with-secret-volume-3 created pradeep@learnk8s$ kubectl get pods | grep volume pod-with-emptydir-volume 1/1 Running 0 110m pod-with-hostpath-volume 1/1 Running 0 84m pod-with-secret-volume 1/1 Running 0 29m pod-with-secret-volume-2 1/1 Running 0 3m40s pod-with-secret-volume-3 1/1 Running 0 16s pradeep@learnk8s$ kubectl exec -it pod-with-secret-volume-3 sh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. # ls -l /etc/foo total 0 lrwxrwxrwx 1 root root 15 Feb 20 14:51 my-group -> ..data/my-group # cat /etc/foo/my-group/my-username pradeep# # cat /etc/foo/my-group/my-pass topsecret# # exit","title":"Secrets"},{"location":"Kubernetes/#persistentvolume","text":"Create a directory called /mnt/data on the k8s node. Within that directory, create index.html file with some text. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ sudo mkdir /mnt/data $ sudo sh -c \"echo 'Hello from Kubernetes storage' > /mnt/data/indext.html\" $ cat /mnt/data/indext.html Hello from Kubernetes storage $ exit logout Now create a PersistentVolume named my-pv-volume of 1Gi using the hostPath pointing to the newly created directory. There are three types of accessModes , for this example use, RWO . RWO ReadWriteOnce RWX ReadWriteMany ROX ReadyOnlyMany pradeep@learnk8s$ cat persistent-volume-demo.yaml apiVersion: v1 kind: PersistentVolume metadata: name: my-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\" Create the PV using this YAML file. pradeep@learnk8s$ kubectl create -f persistent-volume-demo.yaml persistentvolume/my-pv-volume created Verify the available PVs. pradeep@learnk8s$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE my-pv-volume 1Gi RWO Retain Available manual 4s Describe the PersistentVolume for addtional details. pradeep@learnk8s$ kubectl describe pv Name: my-pv-volume Labels: type=local Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Available Claim: Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 1Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mnt/data HostPathType: Events: <none>","title":"PersistentVolume"},{"location":"Kubernetes/#persistentvolumeclaim","text":"pradeep@learnk8s$ cat persistent-volume-claim-demo.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 300Mi pradeep@learnk8s$ kubectl create -f persistent-volume-claim-demo.yaml persistentvolumeclaim/my-pv-claim created pradeep@learnk8s$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-pv-claim Bound my-pv-volume 1Gi RWO manual 40s pradeep@learnk8s$ kubectl describe pvc Name: my-pv-claim Namespace: default StorageClass: manual Status: Bound Volume: my-pv-volume Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Gi Access Modes: RWO VolumeMode: Filesystem Used By: <none> Events: <none> pradeep@learnk8s$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE my-pv-volume 1Gi RWO Retain Bound default/my-pv-claim manual 12m pradeep@learnk8s$ kubectl describe pv Name: my-pv-volume Labels: type=local Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: default/my-pv-claim Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 1Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mnt/data HostPathType: Events: <none> The next step is to create a Pod that uses this PersistentVolumeClaim as a volume. Here is the configuration file for the Pod: pradeep@learnk8s$ cat my-pv-pod.yaml apiVersion: v1 kind: Pod metadata: name: my-pv-pod spec: volumes: - name: my-pv-storage persistentVolumeClaim: claimName: my-pv-claim containers: - name: my-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: my-pv-storage pradeep@learnk8s$ kubectl create -f my-pv-pod.yaml pod/my-pv-pod created The scheduler has placed this Pod on k8s-m02 node. pradeep@learnk8s$ kubectl get pods my-pv-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES my-pv-pod 1/1 Running 0 10s 10.244.1.2 k8s-m02 <none> <none> Let us describe the pod and look at the Volumes section. pradeep@learnk8s$ kubectl describe pods my-pv-pod Name: my-pv-pod Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Sun, 27 Feb 2022 22:35:45 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.2 IPs: IP: 10.244.1.2 Containers: my-pv-container: Container ID: docker://9dc92c5c1037ad4d534fe6440275f62a68f521e3bd22fe47b57c06a8b3414e24 Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: 80/TCP Host Port: 0/TCP State: Running Started: Sun, 27 Feb 2022 22:35:49 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /usr/share/nginx/html from my-pv-storage (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xk6kh (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: my-pv-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: my-pv-claim ReadOnly: false kube-api-access-xk6kh: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 19s default-scheduler Successfully assigned default/my-pv-pod to k8s-m02 Normal Pulling 18s kubelet Pulling image \"nginx\" Normal Pulled 15s kubelet Successfully pulled image \"nginx\" in 2.858388422s Normal Created 15s kubelet Created container my-pv-container Normal Started 15s kubelet Started container my-pv-container It looks like no issues, but let us verify the nginx application by logging into the pod and issuing the curl localhost command. pradeep@learnk8s$ kubectl exec -it my-pv-pod -- /bin/sh # curl localhost <html> <head><title>403 Forbidden</title></head> <body> <center><h1>403 Forbidden</h1></center> <hr><center>nginx/1.21.6</center> </body> </html> # exit Hmm! This seems to be not working. It is forbidden. But this is expected right, we did not have the volume (hostPath) on this k8s-m02 node. We created it on k8s node only. To solve this, we can add a nodeName spec to the Pod definition. Delete this pod and after adding the nodeName recreate the Pod. pradeep@learnk8s$ kubectl delete pod my-pv-pod pod \"my-pv-pod\" deleted pradeep@learnk8s$ vi my-pv-pod.yaml pradeep@learnk8s$ cat my-pv-pod.yaml apiVersion: v1 kind: Pod metadata: name: my-pv-pod spec: nodeName: k8s volumes: - name: my-pv-storage persistentVolumeClaim: claimName: my-pv-claim containers: - name: my-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: my-pv-storage pradeep@learnk8s$ kubectl create -f my-pv-pod.yaml pod/my-pv-pod created :warning: There seems to be some issue with this. Even after scheduling this pod on the node k8s , nginx container is reporting Forbidden. There are couple of issues already reported, but could not find any resolution to this problem yet. https://github.com/kubernetes/website/issues/9523","title":"PersistentVolumeClaim"},{"location":"Kubernetes/#storageclass","text":"In the previous example, we used a storageClass named manual , but that does not seem to be present. pradeep@learnk8s$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 12d There is a storage class by name standard, and is the default storage class. pradeep@learnk8s$ kubectl describe sc Name: standard IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"EnsureExists\"},\"name\":\"standard\"},\"provisioner\":\"k8s.io/minikube-hostpath\"} ,storageclass.kubernetes.io/is-default-class=true Provisioner: k8s.io/minikube-hostpath Parameters: <none> AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> :memo: Changing the storage class name to standard also did not help solve the Forbidden issue with the nginx . Strangely, inside the Pod, we can access the index.html file with cat command, but curl is not working. pradeep@learnk8s$ kubectl exec -it my-pv-pod -- /bin/bash root@my-pv-pod:/# cat /usr/share/nginx/html/indext.html Hello from Kubernetes storage pradeep@learnk8s$ kubectl explain sc KIND: StorageClass VERSION: storage.k8s.io/v1 DESCRIPTION: StorageClass describes the parameters for a class of storage for which PersistentVolumes can be dynamically provisioned. StorageClasses are non-namespaced; the name of the storage class according to etcd is in ObjectMeta.Name. FIELDS: allowVolumeExpansion <boolean> AllowVolumeExpansion shows whether the storage class allow volume expand allowedTopologies <[]Object> Restrict the node topologies where volumes can be dynamically provisioned. Each volume plugin defines its own supported topology specifications. An empty TopologySelectorTerm list means there is no topology restriction. This field is only honored by servers that enable the VolumeScheduling feature. apiVersion <string> APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources kind <string> Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata <Object> Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata mountOptions <[]string> Dynamically provisioned PersistentVolumes of this storage class are created with these mountOptions, e.g. [\"ro\", \"soft\"]. Not validated - mount of the PVs will simply fail if one is invalid. parameters <map[string]string> Parameters holds the parameters for the provisioner that should create volumes of this storage class. provisioner <string> -required- Provisioner indicates the type of the provisioner. reclaimPolicy <string> Dynamically provisioned PersistentVolumes of this storage class are created with this reclaimPolicy. Defaults to Delete. volumeBindingMode <string> VolumeBindingMode indicates how PersistentVolumeClaims should be provisioned and bound. When unset, VolumeBindingImmediate is used. This field is only honored by servers that enable the VolumeScheduling feature.","title":"StorageClass"},{"location":"Kubernetes/#static-binding","text":"Here is the PersistentVolume definition. If we omit the storageClassName, PVC is net getting bound. As you see, there is a default storageClass named standard and PVC is using it by default. pradeep@learnk8s$ cat pv.yaml kind: PersistentVolume apiVersion: v1 metadata: name: pv spec: storageClassName: standard capacity: storage: 512m accessModes: - ReadWriteMany hostPath: path: /data/config Create the PV from the YAML file. pradeep@learnk8s$ kubectl create -f pv.yaml persistentvolume/pv created Verify the available PVs. The newly created PV should be in Available state. Currently, there are no Claims for this volume. pradeep@learnk8s$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv 512m RWX Retain Available standard 8s Describe the PV for addtional details. pradeep@learnk8s$ kubectl describe pv pv Name: pv Labels: <none> Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: standard Status: Available Claim: Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 512m Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /data/config HostPathType: Events: <none> Now, create a PVC with same accessModes: ReadWriteMany . pradeep@learnk8s$ cat pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 256m Create PVC from the YAML file. pradeep@learnk8s$ kubectl create -f pvc.yaml persistentvolumeclaim/pvc created Verify the newly created PVC. It should be in Bound state. Note the STORAGECLASS column. It is using the default one named standard . pradeep@learnk8s$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc Bound pv 512m RWX standard 4s Describe the PVC for addtional details. pradeep@learnk8s$ kubectl describe pvc Name: pvc Namespace: default StorageClass: standard Status: Bound Volume: pv Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 512m Access Modes: RWX VolumeMode: Filesystem Used By: <none> Events: <none> Now, create a Pod that uses the newly defined PVC. pradeep@learnk8s$ cat pod.yaml apiVersion: v1 kind: Pod metadata: name: app spec: containers: - image: nginx name: app volumeMounts: - mountPath: \"/data/app/config\" name: configpvc volumes: - name: configpvc persistentVolumeClaim: claimName: pvc restartPolicy: Never Create the Pod from the YAML file. pradeep@learnk8s$ kubectl create -f pod.yaml pod/app created Use the -o wide option to check the IP and NODE details. pradeep@learnk8s$ kubectl get pods app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES app 1/1 Running 0 18s 10.244.1.5 k8s-m02 <none> <none> Describe the Pod and look for Volumes section. pradeep@learnk8s$ kubectl describe pods app Name: app Namespace: default Priority: 0 Node: k8s-m02/192.168.177.30 Start Time: Mon, 28 Feb 2022 09:25:46 +0530 Labels: <none> Annotations: <none> Status: Running IP: 10.244.1.5 IPs: IP: 10.244.1.5 Containers: app: Container ID: docker://09871149e09a1da02a86a9e38ace82082def59aa38dffbf739e14f7694f0f79e Image: nginx Image ID: docker-pullable://nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: <none> Host Port: <none> State: Running Started: Mon, 28 Feb 2022 09:25:50 +0530 Ready: True Restart Count: 0 Environment: <none> Mounts: /data/app/config from configpvc (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vfx8b (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: configpvc: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: pvc ReadOnly: false kube-api-access-vfx8b: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 63s default-scheduler Successfully assigned default/app to k8s-m02 Normal Pulling 62s kubelet Pulling image \"nginx\" Normal Pulled 59s kubelet Successfully pulled image \"nginx\" in 3.044922832s Normal Created 59s kubelet Created container app Normal Started 59s kubelet Started container app Login to the Container and create a file in the /data/app/config folder, which is the container MountPath. pradeep@learnk8s$ kubectl exec -it app -- /bin/sh # cd /data/app/config # ls -l total 0 # touch hello.txt # echo \"Testing PV, PVC, and SC in K8S!\" > hello.txt # exit Login to the minikube node and check if you are able to view the hello.txt file in the host. On the control plane, there is no such file in the /data/config folder. pradeep@learnk8s$ minikube ssh -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ls /data/config/ $ ls -la /data/config/ total 8 drwxr-xr-x 2 root root 4096 Feb 28 03:33 . drwxr-xr-x 3 root root 4096 Feb 28 03:33 .. $ exit logout On the other node, k8s-m02 , there is the hello.txt file. This is because, the app pod got created in this node. pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ls /data/config hello.txt $ ls -la /data/config/ total 12 drwxr-xr-x 2 root root 4096 Feb 28 03:58 . drwxr-xr-x 3 root root 4096 Feb 28 03:55 .. -rw-r--r-- 1 root root 32 Feb 28 03:58 hello.txt $ cat /data/config/hello.txt Testing PV, PVC, and SC in K8S! $ exit logout We can also view the contents of the hello.txt file. Verify the PV, PVC, and SC details. pradeep@learnk8s$ kubectl get pv,pvc,sc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pv 512m RWX Retain Bound default/pvc standard 9m35s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/pvc Bound pv 512m RWX standard 8m17s NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/standard (default) k8s.io/minikube-hostpath Delete Immediate false 12d Describe the PV one more time, after the Pod creation, to see if there is any change. One change that we notice is that the Claim is showing default/pvc which was empty earlier. Also, Staus changed to Bound from Available . pradeep@learnk8s$ kubectl describe pv Name: pv Labels: <none> Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: standard Status: Bound Claim: default/pvc Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 512m Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /data/config HostPathType: Events: <none> Similarly describe the PVC and look for the changes. The Used By value has changed to app now, showing that it is used by this Pod. pradeep@learnk8s$ kubectl describe pvc Name: pvc Namespace: default StorageClass: standard Status: Bound Volume: pv Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 512m Access Modes: RWX VolumeMode: Filesystem Used By: app Events: <none> Also, look at the StorageClass description. Verify that, IsDefaultClass: Yes . pradeep@learnk8s$ kubectl describe sc Name: standard IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"EnsureExists\"},\"name\":\"standard\"},\"provisioner\":\"k8s.io/minikube-hostpath\"} ,storageclass.kubernetes.io/is-default-class=true Provisioner: k8s.io/minikube-hostpath Parameters: <none> AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Now delete the pod. pradeep@learnk8s$ kubectl delete pod app pod \"app\" deleted After deleting the pod, check if the data is persistent or not. You should still be able to view the contents of the hello.txt file on the k8s-m02 node, even after deleting the Pod, confirming that the storage is persistent now. pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ cd /data/config/ $ cat hello.txt Testing PV, PVC, and SC in K8S! $ exit logout","title":"Static Binding"},{"location":"Kubernetes/#dynamic-binding","text":"Define a new storage class named pradeep-sc-demo in the kube-system namespace using a sample YAML file. pradeep@learnk8s$ cat sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: namespace: kube-system name: pradeep-sc-demo annotations: storageclass.beta.kubernetes.io/is-default-class: \"false\" labels: addonmanager.kubernetes.io/mode: Reconcile provisioner: k8s.io/minikube-hostpath Create the storage class from this YAML file and verify that it was created. pradeep@learnk8s$ kubectl create -f sc.yaml storageclass.storage.k8s.io/pradeep-sc-demo created Now you should see two storageclasses, both with same PROVISIONER details. Also, not the default value next to the standard class, which is not present for the newly defined storage class. pradeep@learnk8s$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE pradeep-sc-demo k8s.io/minikube-hostpath Delete Immediate false 47s standard (default) k8s.io/minikube-hostpath Delete Immediate false 12d Let us modify the PVC definition file to specify this pradeep-sc-demo StorageClass, instead of the standard . First, let us delete the existing PVC. pradeep@learnk8s$ kubectl delete pvc pvc persistentvolumeclaim \"pvc\" deleted Modify the PVC definition, to include the new storage class. pradeep@learnk8s$ cat pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc spec: storageClassName: pradeep-sc-demo accessModes: - ReadWriteMany resources: requests: storage: 256m Create the PVC from the modified definition file. pradeep@learnk8s$ kubectl create -f pvc.yaml persistentvolumeclaim/pvc created Verify the Status of the new PVC. pradeep@learnk8s$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc Pending pradeep-sc-demo 64s It is still showing Pending , to find out why let us describe it. pradeep@learnk8s$ kubectl describe pvc Name: pvc Namespace: default StorageClass: pradeep-sc-demo Status: Pending Volume: Labels: <none> Annotations: volume.beta.kubernetes.io/storage-provisioner: k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner: k8s.io/minikube-hostpath Finalizers: [kubernetes.io/pvc-protection] Capacity: Access Modes: VolumeMode: Filesystem Used By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ExternalProvisioning 2s (x8 over 76s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator We can see that the PVC is waiting for a matching volume to be created. Our existing PV is using the standard storageclass which is not matching with this PVC definition. So let us delete the existing PV and modify it to specify the new SC. pradeep@learnk8s$ kubectl delete pv pv persistentvolume \"pv\" deleted Here is the modified PV definition. pradeep@learnk8s$ cat pv.yaml kind: PersistentVolume apiVersion: v1 metadata: name: pv spec: storageClassName: pradeep-sc-demo capacity: storage: 512m accessModes: - ReadWriteMany hostPath: path: /data/config pradeep@learnk8s$ kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pv 512m RWX Retain Bound default/pvc pradeep-sc-demo 15s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/pvc Bound pv 512m RWX pradeep-sc-demo 6m29s Now that the storageclass is matching, both the PV and PVC are in Bound State. Create the app pod again using the same manifest file that we used earlier. pradeep@learnk8s$ cat pod.yaml apiVersion: v1 kind: Pod metadata: name: app spec: containers: - image: nginx name: app volumeMounts: - mountPath: \"/data/app/config\" name: configpvc volumes: - name: configpvc persistentVolumeClaim: claimName: pvc restartPolicy: Never pradeep@learnk8s$ kubectl create -f pod.yaml pod/app created pradeep@learnk8s$ kubectl get pods app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES app 1/1 Running 0 2m19s 10.244.1.6 k8s-m02 <none> <none> Shell into the Pod and create a file in the mounted directory. pradeep@learnk8s$ kubectl exec app -it -- /bin/sh # cd /data/app/config # ls -l total 4 -rw-r--r-- 1 root root 32 Feb 28 03:58 hello.txt # touch HiAgain.txt # echo \"Hello again from K8s PV,PVC, and SC!\" > HiAgain.txt # cat HiAgain.txt Hello again from K8s PV,PVC, and SC! # exit Becuase of persistent nature, we still see the old hello.txt file in this new container. We have created another file called HiAgain.txt with some sample text. pradeep@learnk8s$ kubectl get pv,pvc,sc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pv 512m RWX Retain Bound default/pvc pradeep-sc-demo 6m43s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/pvc Bound pv 512m RWX pradeep-sc-demo 12m NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/pradeep-sc-demo k8s.io/minikube-hostpath Delete Immediate false 18m storageclass.storage.k8s.io/standard (default) k8s.io/minikube-hostpath Delete Immediate false 12d Let us delete the Pod again and verify the files. pradeep@learnk8s$ kubectl delete pod app pod \"app\" deleted pradeep@learnk8s$ minikube ssh -n k8s-m02 -p k8s _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ cd /data/config/ $ ls HiAgain.txt hello.txt $ ls -la total 16 drwxr-xr-x 2 root root 4096 Feb 28 04:50 . drwxr-xr-x 3 root root 4096 Feb 28 03:55 .. -rw-r--r-- 1 root root 37 Feb 28 04:50 HiAgain.txt -rw-r--r-- 1 root root 32 Feb 28 03:58 hello.txt $ cat HiAgain.txt Hello again from K8s PV,PVC, and SC! $ cat hello.txt Testing PV, PVC, and SC in K8S! $ exit logout This confirms that, the storage is persistent again (even after Pod deletion).","title":"Dynamic Binding"},{"location":"Kubernetes/#networking","text":"","title":"Networking"},{"location":"Kubernetes/#kindnet","text":"Welcome to Kubernetes Networking. Let us setup a fresh minikube cluster with default settings and with 3 nodes in it. pradeep@learnk8s$ minikube start --nodes=3 \ud83d\ude04 minikube v1.25.2 on Darwin 12.2.1 \u2728 Automatically selected the hyperkit driver \ud83d\udcbe Downloading driver docker-machine-driver-hyperkit: > docker-machine-driver-hyper...: 65 B / 65 B [----------] 100.00% ? p/s 0s > docker-machine-driver-hyper...: 8.35 MiB / 8.35 MiB 100.00% 9.87 MiB p/s \ud83d\udd11 The 'hyperkit' driver requires elevated permissions. The following commands will be executed: $ sudo chown root:wheel /Users/pradeep/.minikube/bin/docker-machine-driver-hyperkit $ sudo chmod u+s /Users/pradeep/.minikube/bin/docker-machine-driver-hyperkit Password: \ud83d\udcbf Downloading VM boot image ... > minikube-v1.25.2.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s > minikube-v1.25.2.iso: 237.06 MiB / 237.06 MiB [] 100.00% 6.12 MiB p/s 39s \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udcbe Downloading Kubernetes v1.23.3 preload ... > preloaded-images-k8s-v17-v1...: 505.68 MiB / 505.68 MiB 100.00% 9.37 MiB \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa kubelet.cni-conf-dir=/etc/cni/net.mk \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd17 Configuring CNI (Container Networking Interface) ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: storage-provisioner, default-storageclass \ud83d\udc4d Starting worker node minikube-m02 in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=172.16.30.3 \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa env NO_PROXY=172.16.30.3 \ud83d\udd0e Verifying Kubernetes components... \ud83d\udc4d Starting worker node minikube-m03 in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=172.16.30.3,172.16.30.4 \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa env NO_PROXY=172.16.30.3 \u25aa env NO_PROXY=172.16.30.3,172.16.30.4 \ud83d\udd0e Verifying Kubernetes components... \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default pradeep@learnk8s$ Our cluster nodes are assinged the IPs: 172.16.30.3 , 172.16.30.4 , and 172.16.30.5 respectively. Also, note that the CNI config directory kubelet.cni-conf-dir=/etc/cni/net.mk and minikube is configuring CNI (Container Networking Interface) during the start. pradeep@learnk8s$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane,master 6m54s v1.23.3 172.16.30.3 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 minikube-m02 Ready <none> 4m43s v1.23.3 172.16.30.4 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 minikube-m03 Ready <none> 74s v1.23.3 172.16.30.5 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 pradeep@learnk8s$ Let us take a look at all the pods in this newly deployed cluster. Currently, all the pods are in the kube-system namespace. The pods that are of interest to us in this post are the ones starting with the name kindnet . So what is Kindnet? Kindnet is a simple CNI plugin for Kubernetes with IPv4 and IPv6 support that provides the Cluster Networking. pradeep@learnk8s$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-64897985d-llrjh 1/1 Running 0 7m25s kube-system etcd-minikube 1/1 Running 0 7m40s kube-system kindnet-4mrvw 1/1 Running 0 2m2s kube-system kindnet-cpv4s 1/1 Running 0 5m31s kube-system kindnet-xqjlm 1/1 Running 0 7m26s kube-system kube-apiserver-minikube 1/1 Running 0 7m37s kube-system kube-controller-manager-minikube 1/1 Running 0 7m37s kube-system kube-proxy-b97w8 1/1 Running 0 7m26s kube-system kube-proxy-gtlw7 1/1 Running 0 2m2s kube-system kube-proxy-rts4b 1/1 Running 0 5m31s kube-system kube-scheduler-minikube 1/1 Running 0 7m37s kube-system storage-provisioner 1/1 Running 1 7m35s There are three kindnet pods, and if we check the IP address of these pods, all of them are having the same IP address as that of their node. pradeep@learnk8s$ kubectl get pods -A -o wide | grep kindnet kube-system kindnet-4mrvw 1/1 Running 0 6m17s 172.16.30.5 minikube-m03 <none> <none> kube-system kindnet-cpv4s 1/1 Running 0 9m46s 172.16.30.4 minikube-m02 <none> <none> kube-system kindnet-xqjlm 1/1 Running 0 11m 172.16.30.3 minikube <none> <none> pradeep@learnk8s$ Let us login to the first node, and check its routing table and all the interfaces currently configured on this node. pradeep@learnk8s$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether d6:df:bb:d6:c7:bc brd ff:ff:ff:ff:ff:ff inet 172.16.30.3/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 85395sec preferred_lft 85395sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:df:e0:80:59 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: veth2b322de6@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 8e:09:87:56:8c:50 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.0.1/32 brd 10.244.0.1 scope global veth2b322de6 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.3 metric 1024 10.244.0.2 dev veth2b322de6 scope host 10.244.1.0/24 via 172.16.30.4 dev eth0 10.244.2.0/24 via 172.16.30.5 dev eth0 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.3 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.3 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown This minikube node has the IP address of 172.16.30.3/24 on the eth0 interface and there is a static route to 10.244.1.0/24 with next-hop as the minikube-m02 node with IP address of 172.16.30.4 and similarly another static route to 10.244.2.0/24 with next-hop as 172.16.30.5 which is the IP address of the the minikube-m03 node. There is another virtual interface ( veth ) called veth2b322de6@if4 with an IP address of 10.244.0.1/32 . So what are these subnets 10.244.X.0/24 used for? If we look at the kindnet CNI configuration file (located at the path given in the minikube start output shown above), on this minikube node, we can see that there is a subnet 10.244.0.0/24 range defined. $ cat /etc/cni/net.mk/10-kindnet.conflist { \"cniVersion\": \"0.3.1\", \"name\": \"kindnet\", \"plugins\": [ { \"type\": \"ptp\", \"ipMasq\": false, \"ipam\": { \"type\": \"host-local\", \"dataDir\": \"/run/cni-ipam-state\", \"routes\": [ { \"dst\": \"0.0.0.0/0\" } ], \"ranges\": [ [ { \"subnet\": \"10.244.0.0/24\" } ] ] } , \"mtu\": 1500 }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } Similarly, let us verify the same details in other two nodes of the cluster. First, login to the minikube-m02 node. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 2e:6c:52:09:b4:bb brd ff:ff:ff:ff:ff:ff inet 172.16.30.4/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 84780sec preferred_lft 84780sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:b9:25:87:9f brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.4 metric 1024 10.244.0.0/24 via 172.16.30.3 dev eth0 10.244.2.0/24 via 172.16.30.5 dev eth0 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.4 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.4 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ cat /etc/cni/net.mk/10-kindnet.conflist { \"cniVersion\": \"0.3.1\", \"name\": \"kindnet\", \"plugins\": [ { \"type\": \"ptp\", \"ipMasq\": false, \"ipam\": { \"type\": \"host-local\", \"dataDir\": \"/run/cni-ipam-state\", \"routes\": [ { \"dst\": \"0.0.0.0/0\" } ], \"ranges\": [ [ { \"subnet\": \"10.244.1.0/24\" } ] ] } , \"mtu\": 1500 }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } $ We can see a similar setup. Local subnet range defined in the CNI (kindnet) configuration file is the 10.244.1.0/24 subnet and two static routes ( 10.244.0.0/24 via 172.16.30.3 and another static route 10.244.2.0/24 via 172.16.30.5 ). Finally, let us check the same in the minikube-m03 node. pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 84738sec preferred_lft 84738sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.5 metric 1024 10.244.0.0/24 via 172.16.30.3 dev eth0 10.244.1.0/24 via 172.16.30.4 dev eth0 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.5 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.5 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ cat /etc/cni/net.mk/10-kindnet.conflist { \"cniVersion\": \"0.3.1\", \"name\": \"kindnet\", \"plugins\": [ { \"type\": \"ptp\", \"ipMasq\": false, \"ipam\": { \"type\": \"host-local\", \"dataDir\": \"/run/cni-ipam-state\", \"routes\": [ { \"dst\": \"0.0.0.0/0\" } ], \"ranges\": [ [ { \"subnet\": \"10.244.2.0/24\" } ] ] } , \"mtu\": 1500 }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } $ exit logout Here also, very similar setup. 10.244.2.0/24 is the local subnet and remote subnets are cofnigured via static routes ( 10.244.0.0/24 via 172.16.30.3 and 10.244.1.0/24 via 172.16.30.4 ). One thing to note is that, in both of the worker nodes, there isn't any veth interface present yet, unlike the controleplane node. To understand, why let us get the IP addresses of all the running pods. pradeep@learnk8s$ kubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-64897985d-llrjh 1/1 Running 0 32m 10.244.0.2 minikube <none> <none> kube-system etcd-minikube 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kindnet-4mrvw 1/1 Running 0 27m 172.16.30.5 minikube-m03 <none> <none> kube-system kindnet-cpv4s 1/1 Running 0 31m 172.16.30.4 minikube-m02 <none> <none> kube-system kindnet-xqjlm 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kube-apiserver-minikube 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kube-controller-manager-minikube 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kube-proxy-b97w8 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system kube-proxy-gtlw7 1/1 Running 0 27m 172.16.30.5 minikube-m03 <none> <none> kube-system kube-proxy-rts4b 1/1 Running 0 31m 172.16.30.4 minikube-m02 <none> <none> kube-system kube-scheduler-minikube 1/1 Running 0 33m 172.16.30.3 minikube <none> <none> kube-system storage-provisioner 1/1 Running 1 33m 172.16.30.3 minikube <none> <none> pradeep@learnk8s$ There is one Pod named coredns-64897985d-llrjh in the kube-system namespace with an IP address of 10.244.0.2 which is in the same subnet range defined in the CNI config on the minikube node. Let us create our first pod (we can use any image, it does not matter). For test purposes, let us create a new pod using the busybox image. pradeep@learnk8s$ kubectl run busybox --image=busybox pod/busybox created Verify the IP address assigned and the node on which it is running. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES busybox 0/1 Completed 0 12s 10.244.2.2 minikube-m03 <none> <none> pradeep@learnk8s$ We can see that the scheduler has assigned the minikube-m03 node for this new pod and this busybox pod obtained its IP address 10.244.2.2 which is from the CNI assigned subnet ( 10.244.2.0/24 ) for this node. Now, let us go back to the minikube-m03 node and check the list of interfaces and see if there is anything new! pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 83749sec preferred_lft 83749sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: veth07b7de9e@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 6a:9a:f7:af:ac:0f brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.2.1/32 brd 10.244.2.1 scope global veth07b7de9e valid_lft forever preferred_lft forever $ Compared to the initial setup, there is one new interface (#5, with the name veth07b7de9e@if4 ). This is very similar to the controlplane node now. This veth interface has the first IP address ( 10.244.2.1/32 ) from the CNI assigned subnet. I tried to login to this container and check few things from inside the container, but before I do that the container crashed, so I have deleted it. pradeep@learnk8s$ kubectl exec -it busybox -- /bin/bash error: unable to upgrade connection: container not found (\"busybox\") pradeep@learnk8s$ kubectl get pods NAME READY STATUS RESTARTS AGE busybox 0/1 CrashLoopBackOff 6 (63s ago) 7m3s pradeep@learnk8s$ kubectl delete pod busybox pod \"busybox\" deleted Create another containter, this time using another image, nginx for test purposes. pradeep@learnk8s$ kubectl run nginx --image=nginx pod/nginx created Verify the IP address of this new pod and the node on which it is running. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 25s 10.244.2.3 minikube-m03 <none> <none> pradeep@learnk8s$ Ah!, this pod also got assigned to the same node, minikube-m03 and look at the IP address, the next IP address in the same range, 10.244.2.3 . pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ce:26:e4:eb:95:56 brd ff:ff:ff:ff:ff:ff inet 172.16.30.5/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 83069sec preferred_lft 83069sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:1a:9e:e4:0b brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 6: veth37e46d8f@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 7e:96:f9:98:5f:4a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.2.1/32 brd 10.244.2.1 scope global veth37e46d8f valid_lft forever preferred_lft forever $ From the routing table, we can see that the new Pod IP is routed via the veth interface 10.244.2.3 dev veth37e46d8f . $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.5 metric 1024 10.244.0.0/24 via 172.16.30.3 dev eth0 10.244.1.0/24 via 172.16.30.4 dev eth0 10.244.2.3 dev veth37e46d8f scope host 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.5 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.5 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ Let us manually schedule a Pod on the minikube-m02 node as well and observe. pradeep@learnk8s$ cat my-pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx-manual spec: nodeName: minikube-m02 containers: - image: nginx name: nginx pradeep@learnk8s$ kubectl create -f my-pod.yaml pod/nginx-manual created pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-manual 1/1 Running 0 73s 10.244.1.2 minikube-m02 <none> <none> pradeep@learnk8s$ We can see that, this pod has been given an IP ( 10.244.1.2 ) from the 10.244.1.0/24 subnet allocated to minikube-m02 node. Let us login to this node and verify the list of currently running pods with the docker ps command. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6397d143118 nginx \"/docker-entrypoint.\u2026\" 3 minutes ago Up 3 minutes k8s_nginx_nginx-manual_default_950195d8-bc1d-420f-aede-fcd4a273f0e0_0 d2fb47206b71 k8s.gcr.io/pause:3.6 \"/pause\" 4 minutes ago Up 4 minutes k8s_POD_nginx-manual_default_950195d8-bc1d-420f-aede-fcd4a273f0e0_0 ea4e0c180710 6de166512aa2 \"/bin/kindnetd\" 9 minutes ago Up 9 minutes k8s_kindnet-cni_kindnet-cpv4s_kube-system_5d3d3977-c5ff-4bea-917a-e0db52896da2_1 6ae7df286995 9b7cc9982109 \"/usr/local/bin/kube\u2026\" 9 minutes ago Up 9 minutes k8s_kube-proxy_kube-proxy-rts4b_kube-system_9773fb68-a469-4cb2-827b-546076677b3b_1 d77c3e7c06e4 k8s.gcr.io/pause:3.6 \"/pause\" 9 minutes ago Up 9 minutes k8s_POD_kindnet-cpv4s_kube-system_5d3d3977-c5ff-4bea-917a-e0db52896da2_1 5e18805db658 k8s.gcr.io/pause:3.6 \"/pause\" 9 minutes ago Up 9 minutes k8s_POD_kube-proxy-rts4b_kube-system_9773fb68-a469-4cb2-827b-546076677b3b_1 $ Obtain the container ID for the nginx-manual pod. In this case ,it is d6397d143118 . Use the docker inspect command to get the PID of this container. $ docker inspect d2fb47206b71 | grep Pid \"Pid\": 4200, \"PidMode\": \"\", \"PidsLimit\": null, $ Now, take the Pid and use the nsenter command to issue the ip a command from inside this Pod namespace. This requires root privileges, so use sudo command. $ sudo nsenter -t 4200 -n ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: eth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 52:a1:40:3b:54:18 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.1.2/24 brd 10.244.1.255 scope global eth0 valid_lft forever preferred_lft forever $ We can see that the Pod IP ( 10.244.1.2 ) is present on the interface `#4 named eth0@if5 . Look at all the interfaces from this minikube-m02 node. $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 2e:6c:52:09:b4:bb brd ff:ff:ff:ff:ff:ff inet 172.16.30.4/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 85770sec preferred_lft 85770sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:41:3c:16:fe brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: vethfa21a27c@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether b2:26:8c:88:bf:d3 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.1.1/32 brd 10.244.1.1 scope global vethfa21a27c valid_lft forever preferred_lft forever $ From the interface naming, we can see the link between the Pod namespace and the host. The 4: eth0@if5: interface with the IP address 10.244.1.2/24 is linked to the 5: vethfa21a27c@if4: interface with the IP address 10.244.1.1/32 . The name followed by the @ symbol tells us the interface number. For example, inside the Pod namespace, it is shown as @if5 for the interface number 4: , this corresponds to interface number 5: on the host, which is nothing but the 5: vethfa21a27c@if4: . From this 5: vethfa21a27c@if4: on the host, @if4 corresponds to the interface number 4 on the Pod, which is the eth0 interface indicated by 4: eth0@if5: . Now you can see the full picture of the internal connectivity. Just to confirm, this Pod hosted on minikube-m02 can communicate with a Pod with IP address 10.244.0.2 hosted on the minikube node. This can be confirmed by first looking at the Pod routing table, using the same nsenter command. There is a default route pointing to the gateway 10.244.1.1 . $ sudo nsenter -t 4200 -n ip route default via 10.244.1.1 dev eth0 10.244.1.0/24 via 10.244.1.1 dev eth0 src 10.244.1.2 10.244.1.1 dev eth0 scope link src 10.244.1.2 $ Ping to another Pod in the same cluster, but on another node. $ sudo nsenter -t 4200 -n ping 10.244.0.2 -c 3 PING 10.244.0.2 (10.244.0.2): 56 data bytes 64 bytes from 10.244.0.2: seq=0 ttl=62 time=1.313 ms 64 bytes from 10.244.0.2: seq=1 ttl=62 time=1.980 ms 64 bytes from 10.244.0.2: seq=2 ttl=62 time=1.139 ms --- 10.244.0.2 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 1.139/1.477/1.980 ms $ We can also see that using the same default route, this Pod can communicate with the outside cluster as well. $ sudo nsenter -t 4200 -n ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes 64 bytes from 8.8.8.8: seq=0 ttl=115 time=12.515 ms 64 bytes from 8.8.8.8: seq=1 ttl=115 time=12.238 ms 64 bytes from 8.8.8.8: seq=2 ttl=115 time=11.216 ms 64 bytes from 8.8.8.8: seq=3 ttl=115 time=11.303 ms ^C --- 8.8.8.8 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 11.216/11.818/12.515 ms $ The following diagram summarizes our discussion so far. This concludes our initial verification of the Kubernetes networking on the minikube with the default CNI (kindnet) which uses the simple static routes to facilitate communication across nodes. We will look at other CNIs later in other posts.","title":"KindNet"},{"location":"Kubernetes/#calico","text":"Kubernetes Networking with Calico CNI Let us build another minikube cluster, this time with the Calico CNI. Minikube supports many CNIs. From the minikube start -h output, we can see the supported CNI plug-ins list. --cni='': CNI plug-in to use. Valid options: auto, bridge, calico, cilium, flannel, kindnet, or path to a CNI manifest (default: auto) Start the 3-node minikube cluster with calico CNI plugin. pradeep@learnk8s$ minikube start --nodes=3 --cni=calico \ud83d\ude04 minikube v1.25.2 on Darwin 12.2.1 \u2728 Automatically selected the hyperkit driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa kubelet.housekeeping-interval=5m \u25aa Generating certificates and keys ... \u25aa Booting up control plane ... \u25aa Configuring RBAC rules ... \ud83d\udd17 Configuring Calico (Container Networking Interface) ... \ud83d\udd0e Verifying Kubernetes components... \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5 \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \ud83d\udc4d Starting worker node minikube-m02 in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=172.16.30.6 \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa env NO_PROXY=172.16.30.6 \ud83d\udd0e Verifying Kubernetes components... \ud83d\udc4d Starting worker node minikube-m03 in cluster minikube \ud83d\udd25 Creating hyperkit VM (CPUs=2, Memory=2200MB, Disk=20000MB) ... \ud83c\udf10 Found network options: \u25aa NO_PROXY=172.16.30.6,172.16.30.7 \ud83d\udc33 Preparing Kubernetes v1.23.3 on Docker 20.10.12 ... \u25aa env NO_PROXY=172.16.30.6 \u25aa env NO_PROXY=172.16.30.6,172.16.30.7 \ud83d\udd0e Verifying Kubernetes components... \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default pradeep@learnk8s$ Verify the cluster node IPs: 172.16.30.6 , 172.16.30.7 , and 172.16.30.8 . pradeep@learnk8s$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane,master 22m v1.23.3 172.16.30.6 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 minikube-m02 Ready <none> 20m v1.23.3 172.16.30.7 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 minikube-m03 Ready <none> 17m v1.23.3 172.16.30.8 <none> Buildroot 2021.02.4 4.19.202 docker://20.10.12 pradeep@learnk8s$ Verify the list of Pods in all namespaces. pradeep@learnk8s$ kubectl get pods -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system calico-kube-controllers-8594699699-dztlm 1/1 Running 0 6m50s 10.88.0.3 minikube <none> <none> kube-system calico-node-gqvw6 1/1 Running 1 (2m54s ago) 4m38s 172.16.30.7 minikube-m02 <none> <none> kube-system calico-node-qdbcf 1/1 Running 0 6m50s 172.16.30.6 minikube <none> <none> kube-system calico-node-sw74l 1/1 Running 0 114s 172.16.30.8 minikube-m03 <none> <none> kube-system coredns-64897985d-58btq 1/1 Running 0 6m50s 10.88.0.2 minikube <none> <none> kube-system etcd-minikube 1/1 Running 0 7m1s 172.16.30.6 minikube <none> <none> kube-system kube-apiserver-minikube 1/1 Running 0 7m1s 172.16.30.6 minikube <none> <none> kube-system kube-controller-manager-minikube 1/1 Running 0 7m1s 172.16.30.6 minikube <none> <none> kube-system kube-proxy-7k4lb 1/1 Running 0 114s 172.16.30.8 minikube-m03 <none> <none> kube-system kube-proxy-gm2dh 1/1 Running 0 4m38s 172.16.30.7 minikube-m02 <none> <none> kube-system kube-proxy-hvkqd 1/1 Running 0 6m50s 172.16.30.6 minikube <none> <none> kube-system kube-scheduler-minikube 1/1 Running 0 7m1s 172.16.30.6 minikube <none> <none> kube-system storage-provisioner 1/1 Running 1 (2m26s ago) 6m47s 172.16.30.6 minikube <none> <none> pradeep@learnk8s$ There are two Pods using the 10.88.0.X subnet ( calico-kube-controllers and coredns Pods ). Login to the controlplane node and check the CNI directory. pradeep@learnk8s$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ cat /etc/cni/net.d/ .keep 10-calico.conflist 87-podman-bridge.conflist calico-kubeconfig $ cat /etc/cni/net.d/10-calico.conflist { \"name\": \"k8s-pod-network\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"calico\", \"log_level\": \"info\", \"log_file_path\": \"/var/log/calico/cni/cni.log\", \"datastore_type\": \"kubernetes\", \"nodename\": \"minikube\", \"mtu\": 0, \"ipam\": { \"type\": \"calico-ipam\" }, \"policy\": { \"type\": \"k8s\" }, \"kubernetes\": { \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\" } }, { \"type\": \"portmap\", \"snat\": true, \"capabilities\": {\"portMappings\": true} }, { \"type\": \"bandwidth\", \"capabilities\": {\"bandwidth\": true} } ] }$ Apart from the Calico , there is a config file for Podman as well. Let us check that as well. $ cat /etc/cni/net.d/87-podman-bridge.conflist { \"cniVersion\": \"0.4.0\", \"name\": \"podman\", \"plugins\": [ { \"type\": \"bridge\", \"bridge\": \"cni-podman0\", \"isGateway\": true, \"ipMasq\": true, \"hairpinMode\": true, \"ipam\": { \"type\": \"host-local\", \"routes\": [{ \"dst\": \"0.0.0.0/0\" }], \"ranges\": [ [ { \"subnet\": \"10.88.0.0/16\", \"gateway\": \"10.88.0.1\" } ] ] } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } }, { \"type\": \"firewall\" }, { \"type\": \"tuning\" } ] } $ From this, it is clear that while starting the minikube cluster, those two Pods got the IP from the Podman CNI, which seems to compete with the Calico CNI. This issue is reported here: https://github.com/kubernetes/kubernetes/issues/107687 Let us create a new Pod in the default namespace and check which IP will be obtained. pradeep@learnk8s$ kubectl run nginx --image=nginx pod/nginx created Verify the IP of the new Pod and the node on which it got hosted. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 56s 10.244.205.193 minikube-m02 <none> <none> The new nginx pod got the IP address of 10.244.205.193 and is running on the minikube-m02 node. Let us log in to this minikube-m02 node and verify the routing table and list of interfaces. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 22:a7:1a:d9:be:42 brd ff:ff:ff:ff:ff:ff inet 172.16.30.7/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 84337sec preferred_lft 84337sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:d3:aa:60:ec brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 inet 10.244.205.192/32 scope global tunl0 valid_lft forever preferred_lft forever 8: calic440f455693@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0 $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.7 metric 1024 10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink 10.244.151.0/26 via 172.16.30.8 dev tunl0 proto bird onlink blackhole 10.244.205.192/26 proto bird 10.244.205.193 dev calic440f455693 scope link 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.7 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.7 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ There are two special interfaces, 5: tunl0@NONE: and 8: calic440f455693@if5: . We can see that it is an ipip tunnel and tunnel interface ( tunl0 ) has the IP address of 10.244.205.192/32 . From the routing table, there are two /26 routes pointing to the tunl0 interface. The 10.244.120.64/26 via 172.16.30.6 and 10.244.151.0/26 via 172.16.30.8 entries in the routing table, shows the Calico assigned subnets on the other two nodes, and the routes for those remote subnets via the IPIP tunnel interface. From within the minikube-m02 node, issue the docker ps command the retrieve the container ID of the nginx container. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0d1f5d390956 nginx \"/docker-entrypoint.\u2026\" 28 minutes ago Up 28 minutes k8s_nginx_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0 6b67d9586b86 k8s.gcr.io/pause:3.6 \"/pause\" 28 minutes ago Up 28 minutes k8s_POD_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0 6f65ec9ee321 5ef66b403f4f \"start_runit\" 39 minutes ago Up 39 minutes k8s_calico-node_calico-node-gqvw6_kube-system_6a835837-d36e-4366-aefe-cedc09d2f148_1 3d0be715b116 9b7cc9982109 \"/usr/local/bin/kube\u2026\" 41 minutes ago Up 41 minutes k8s_kube-proxy_kube-proxy-gm2dh_kube-system_770d601e-7b93-42fa-8019-5976ae95684e_0 9c077d1f8374 k8s.gcr.io/pause:3.6 \"/pause\" 41 minutes ago Up 41 minutes k8s_POD_calico-node-gqvw6_kube-system_6a835837-d36e-4366-aefe-cedc09d2f148_0 4b2360ad79a9 k8s.gcr.io/pause:3.6 \"/pause\" 41 minutes ago Up 41 minutes k8s_POD_kube-proxy-gm2dh_kube-system_770d601e-7b93-42fa-8019-5976ae95684e_0 $ Get the Pid of the nginx container with the docker inspect command. $ docker inspect 0d1f5d390956 | grep Pid \"Pid\": 11380, \"PidMode\": \"\", \"PidsLimit\": null, $ With the nsenter command, verify the list of interfaces inside the nginx container. $ sudo nsenter -t 11380 -n ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 3: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 5: eth0@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default link/ether e6:ea:af:be:ec:53 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.205.193/32 brd 10.244.205.193 scope global eth0 valid_lft forever preferred_lft forever $ We can see that the Pod IP is configured on the 5: eth0@if8: interface. From the @if8 , we can see the link to the 8: calic440f455693@if5: interface on the host (like the veth interfaces in the case of Kindnet CNI). Look at the routing table of the nginx pod. $ sudo nsenter -t 11380 -n ip route default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link $ Exit the minikube-m02 node and use the kubectl describe nodes command and filter for the PodCIDRs. pradeep@learnk8s$ kubectl describe nodes | grep -e Name -e PodCIDR Name: minikube PodCIDR: 10.244.0.0/24 PodCIDRs: 10.244.0.0/24 Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age Name: minikube-m02 PodCIDR: 10.244.1.0/24 PodCIDRs: 10.244.1.0/24 Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age Name: minikube-m03 PodCIDR: 10.244.2.0/24 PodCIDRs: 10.244.2.0/24 Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age pradeep@learnk8s$ Currently, there is only one pod ( nginx ) in the cluster. Similar to what we have verified on the minikube-m02 node, do the same on the other two nodes as well. From the minikube node: pradeep@learnk8s$ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 9e:55:51:7f:e1:98 brd ff:ff:ff:ff:ff:ff inet 172.16.30.6/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 82845sec preferred_lft 82845sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:92:37:51:98 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: cni-podman0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 36:85:e8:76:d2:1c brd ff:ff:ff:ff:ff:ff inet 10.88.0.1/16 brd 10.88.255.255 scope global cni-podman0 valid_lft forever preferred_lft forever 6: veth5a561323@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master cni-podman0 state UP group default link/ether f6:67:49:87:51:36 brd ff:ff:ff:ff:ff:ff link-netnsid 0 7: vethb6ed82ba@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master cni-podman0 state UP group default link/ether 22:b5:fe:8a:37:59 brd ff:ff:ff:ff:ff:ff link-netnsid 1 8: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 inet 10.244.120.64/32 scope global tunl0 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.6 metric 1024 10.88.0.0/16 dev cni-podman0 proto kernel scope link src 10.88.0.1 blackhole 10.244.120.64/26 proto bird 10.244.151.0/26 via 172.16.30.8 dev tunl0 proto bird onlink 10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.6 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.6 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ exit logout pradeep@learnk8s$ On the controlplane node, there are extra interfaces coming from the Podman CNI . From the minikube-m03 node: pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether de:b8:1d:5e:d9:c0 brd ff:ff:ff:ff:ff:ff inet 172.16.30.8/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 83040sec preferred_lft 83040sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:66:0b:71:73 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 inet 10.244.151.0/32 scope global tunl0 valid_lft forever preferred_lft forever $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.8 metric 1024 10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink blackhole 10.244.151.0/26 proto bird 10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.8 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.8 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ On other nodes also, we do see similar tunl0 interface and routes for Calico subnets (/26s) via the IPIP tunnel. For the local subnet, there is a blackhole route: for example on the minikube-m03 node blackhole 10.244.151.0/26 proto bird , and on minikube-m02 node blackhole 10.244.205.192/26 proto bird , on the minikube node blackhole 10.244.120.64/26 proto bird . Let us manually schedule a Pod on the minikube-m03 node. pradeep@learnk8s$ cat my-pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx-manual spec: nodeName: minikube-m03 containers: - image: nginx name: nginx pradeep@learnk8s$ kubectl create -f my-pod.yaml pod/nginx-manual created pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 53m 10.244.205.193 minikube-m02 <none> <none> nginx-manual 1/1 Running 0 39s 10.244.151.1 minikube-m03 <none> <none> pradeep@learnk8s$ As expected, the nginx-manual pod got an IP from the 10.244.151.0/26 subnet. Log back to the minikube-m03 node and check the routing table and list of interfaces. pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether de:b8:1d:5e:d9:c0 brd ff:ff:ff:ff:ff:ff inet 172.16.30.8/24 brd 172.16.30.255 scope global dynamic eth0 valid_lft 82414sec preferred_lft 82414sec 3: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:66:0b:71:73 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 5: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 inet 10.244.151.0/32 scope global tunl0 valid_lft forever preferred_lft forever 8: califba6dd09590@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0 $ ip route default via 172.16.30.1 dev eth0 proto dhcp src 172.16.30.8 metric 1024 10.244.120.64/26 via 172.16.30.6 dev tunl0 proto bird onlink blackhole 10.244.151.0/26 proto bird 10.244.151.1 dev califba6dd09590 scope link 10.244.205.192/26 via 172.16.30.7 dev tunl0 proto bird onlink 172.16.30.0/24 dev eth0 proto kernel scope link src 172.16.30.8 172.16.30.1 dev eth0 proto dhcp scope link src 172.16.30.8 metric 1024 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown $ Compared to the previous output (when there were no pods on this node), there is a new interface 8: califba6dd09590@if5: and new route entry for the Pod IP 10.244.151.1 dev califba6dd09590 scope link . Get the container ID and Pid of this nginx-manual container. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 244b50fad9d4 nginx \"/docker-entrypoint.\u2026\" 5 minutes ago Up 5 minutes k8s_nginx_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0 e7117e519923 k8s.gcr.io/pause:3.6 \"/pause\" 5 minutes ago Up 5 minutes k8s_POD_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0 c4c993bc1f2e calico/node \"start_runit\" About an hour ago Up About an hour k8s_calico-node_calico-node-sw74l_kube-system_0e30bbad-8370-4907-ab20-ce81450ad13c_0 a3482f106e3a 9b7cc9982109 \"/usr/local/bin/kube\u2026\" About an hour ago Up About an hour k8s_kube-proxy_kube-proxy-7k4lb_kube-system_f284e305-e71b-40b0-a715-796d0733bc03_0 25a914d158da k8s.gcr.io/pause:3.6 \"/pause\" About an hour ago Up About an hour k8s_POD_calico-node-sw74l_kube-system_0e30bbad-8370-4907-ab20-ce81450ad13c_0 abce35d2a0ae k8s.gcr.io/pause:3.6 \"/pause\" About an hour ago Up About an hour k8s_POD_kube-proxy-7k4lb_kube-system_f284e305-e71b-40b0-a715-796d0733bc03_0 $ $ docker inspect 244b50fad9d4 | grep Pid \"Pid\": 42125, \"PidMode\": \"\", \"PidsLimit\": null, $ Use the nsenter command and verify the Pod interfaces and route table. $ nsenter -t 42125 -n ip a nsenter: cannot open /proc/42125/ns/net: Permission denied $ sudo nsenter -t 42125 -n ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/sit 0.0.0.0 brd 0.0.0.0 3: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 5: eth0@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default link/ether 9a:93:7b:77:8f:c7 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.151.1/32 brd 10.244.151.1 scope global eth0 valid_lft forever preferred_lft forever $ The eth0 interface ( 5: eth0@if8 ) of the Pod has the IP address 10.244.151.1/32 and is mapped to the new cali interface 8: califba6dd09590@if5: on the host. $ sudo nsenter -t 42125 -n ip route default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link $ Verify communication between the two Pods. 10.244.205.193 is the IP address of the nginx pod running on the other node minikube-m02 . $ sudo nsenter -t 42125 -n ping 10.244.205.193 PING 10.244.205.193 (10.244.205.193): 56 data bytes 64 bytes from 10.244.205.193: seq=0 ttl=62 time=17.321 ms 64 bytes from 10.244.205.193: seq=1 ttl=62 time=0.963 ms 64 bytes from 10.244.205.193: seq=2 ttl=62 time=0.891 ms 64 bytes from 10.244.205.193: seq=3 ttl=62 time=1.529 ms ^C --- 10.244.205.193 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.891/5.176/17.321 ms $ Verify communication to the outside cluster hosts, like any internet host ( for example 8.8.8.8) $ sudo nsenter -t 42125 -n ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes 64 bytes from 8.8.8.8: seq=0 ttl=115 time=20.260 ms 64 bytes from 8.8.8.8: seq=1 ttl=115 time=15.457 ms ^C --- 8.8.8.8 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 15.457/17.858/20.260 ms $ Let us install calicoctl as a Pod itself. pradeep@learnk8s$ kubectl apply -f https://projectcalico.docs.tigera.io/manifests/calicoctl.yaml serviceaccount/calicoctl created pod/calicoctl created clusterrole.rbac.authorization.k8s.io/calicoctl created clusterrolebinding.rbac.authorization.k8s.io/calicoctl created pradeep@learnk8s$ Verify that the calicoctl pod is running. pradeep@learnk8s$ kubectl get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES calico-kube-controllers-8594699699-dztlm 1/1 Running 0 82m 10.88.0.3 minikube <none> <none> calico-node-gqvw6 1/1 Running 1 (78m ago) 80m 172.16.30.7 minikube-m02 <none> <none> calico-node-qdbcf 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> calico-node-sw74l 1/1 Running 0 77m 172.16.30.8 minikube-m03 <none> <none> calicoctl 1/1 Running 0 18s 172.16.30.7 minikube-m02 <none> <none> coredns-64897985d-58btq 1/1 Running 0 82m 10.88.0.2 minikube <none> <none> etcd-minikube 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> kube-apiserver-minikube 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> kube-controller-manager-minikube 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> kube-proxy-7k4lb 1/1 Running 0 77m 172.16.30.8 minikube-m03 <none> <none> kube-proxy-gm2dh 1/1 Running 0 80m 172.16.30.7 minikube-m02 <none> <none> kube-proxy-hvkqd 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> kube-scheduler-minikube 1/1 Running 0 82m 172.16.30.6 minikube <none> <none> storage-provisioner 1/1 Running 1 (77m ago) 82m 172.16.30.6 minikube <none> <none> pradeep@learnk8s$ Verify the calicoctl pradeep@learnk8s$ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get nodes -o wide Failed to get resources: Version mismatch. Client Version: v3.22.1 Cluster Version: v3.20.0 Use --allow-version-mismatch to override. command terminated with exit code 1 pradeep@learnk8s$ Repeat it with --allow-version-mismatch . pradeep@learnk8s$ kubectl exec -ti -n kube-system calicoctl -- /calicoctl get nodes -o wide --allow-version-mismatch NAME ASN IPV4 IPV6 minikube (64512) 172.16.30.6/24 minikube-m02 (64512) 172.16.30.7/24 minikube-m03 (64512) 172.16.30.8/24 pradeep@learnk8s$ We can confirm that the calicoctl is working and we can see some BGP Autonomous systems shown as well. Currently all three nodes are part of the same ASN ( 64512 ). Create an alias for the calicoctl pradeep@learnk8s$ alias calicoctl=\"kubectl exec -i -n kube-system calicoctl -- /calicoctl --allow-version-mismatch\" By default, calicoctl will attempt to read from the Kubernetes API using the default kubeconfig located at $(HOME)/.kube/config. If the default kubeconfig does not exist, or you would like to specify alternative API access information, you can do so using the following configuration options. pradeep@learnk8s$ DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes NAME minikube minikube-m02 minikube-m03 pradeep@learnk8s$ Verify all API-Resources installed by Calico CNI plugin. pradeep@learnk8s$ kubectl api-resources | grep calico bgpconfigurations crd.projectcalico.org/v1 false BGPConfiguration bgppeers crd.projectcalico.org/v1 false BGPPeer blockaffinities crd.projectcalico.org/v1 false BlockAffinity clusterinformations crd.projectcalico.org/v1 false ClusterInformation felixconfigurations crd.projectcalico.org/v1 false FelixConfiguration globalnetworkpolicies crd.projectcalico.org/v1 false GlobalNetworkPolicy globalnetworksets crd.projectcalico.org/v1 false GlobalNetworkSet hostendpoints crd.projectcalico.org/v1 false HostEndpoint ipamblocks crd.projectcalico.org/v1 false IPAMBlock ipamconfigs crd.projectcalico.org/v1 false IPAMConfig ipamhandles crd.projectcalico.org/v1 false IPAMHandle ippools crd.projectcalico.org/v1 false IPPool kubecontrollersconfigurations crd.projectcalico.org/v1 false KubeControllersConfiguration networkpolicies crd.projectcalico.org/v1 true NetworkPolicy networksets crd.projectcalico.org/v1 true NetworkSet pradeep@learnk8s$ Verify some of these resources with kubectl get and kubectl describe commands. pradeep@learnk8s$ kubectl get bgppeers -A No resources found pradeep@learnk8s$ kubectl get ippools NAME AGE default-ipv4-ippool 97m pradeep@learnk8s$ kubectl describe ippools Name: default-ipv4-ippool Namespace: Labels: <none> Annotations: projectcalico.org/metadata: {\"uid\":\"cf94cf43-8887-4528-a934-ca498f0422e1\",\"creationTimestamp\":\"2022-03-19T18:20:36Z\"} API Version: crd.projectcalico.org/v1 Kind: IPPool Metadata: Creation Timestamp: 2022-03-19T18:20:36Z Generation: 1 Managed Fields: API Version: crd.projectcalico.org/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:projectcalico.org/metadata: f:spec: .: f:blockSize: f:cidr: f:ipipMode: f:natOutgoing: f:nodeSelector: f:vxlanMode: Manager: Go-http-client Operation: Update Time: 2022-03-19T18:20:36Z Resource Version: 659 UID: 8d9e076a-dae6-4bbd-8f44-920da83472ba Spec: Block Size: 26 Cidr: 10.244.0.0/16 Ipip Mode: Always Nat Outgoing: true Node Selector: all() Vxlan Mode: Never Events: <none> pradeep@learnk8s$ pradeep@learnk8s$ kubectl get ipamblocks NAME AGE 10-244-120-64-26 98m 10-244-151-0-26 93m 10-244-205-192-26 95m pradeep@learnk8s$ We have seen these IPAM blocks already, when we verified the routing tables on each node. pradeep@learnk8s$ kubectl get networkpolicies No resources found in default namespace. Let us create a network policy now. pradeep@learnk8s$ cat network-policy.yaml --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: - Ingress pradeep@learnk8s$ pradeep@learnk8s$ kubectl create -f network-policy.yaml networkpolicy.networking.k8s.io/default-deny-ingress created Verify that the network policy is created. pradeep@learnk8s$ kubectl get netpol NAME POD-SELECTOR AGE default-deny-ingress <none> 2s Describe it for more details. pradeep@learnk8s$ kubectl describe netpol Name: default-deny-ingress Namespace: default Created on: 2022-03-20 01:45:29 +0530 IST Labels: <none> Annotations: <none> Spec: PodSelector: <none> (Allowing the specific traffic to all pods in this namespace) Allowing ingress traffic: <none> (Selected pods are isolated for ingress connectivity) Not affecting egress traffic Policy Types: Ingress pradeep@learnk8s$ cat network-policy.yaml --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: - Ingress pradeep@learnk8s$ If you recall, we have two pods in our cluster at the moment. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 105m 10.244.205.193 minikube-m02 <none> <none> nginx-manual 1/1 Running 0 52m 10.244.151.1 minikube-m03 <none> <none> Now that the network policy is applied, let us check the connectivity between these two pods again. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ docker ps | grep nginx 0d1f5d390956 nginx \"/docker-entrypoint.\u2026\" 2 hours ago Up 2 hours k8s_nginx_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0 6b67d9586b86 k8s.gcr.io/pause:3.6 \"/pause\" 2 hours ago Up 2 hours k8s_POD_nginx_default_5c5b022b-70d0-4e59-bbba-35a9bb43aa5c_0 $ docker inspect 0d1f5d390956 | grep Pid \"Pid\": 11380, \"PidMode\": \"\", \"PidsLimit\": null, $ sudo nsenter -t 11380 -n ping 10.244.151.1 PING 10.244.151.1 (10.244.151.1): 56 data bytes ^C --- 10.244.151.1 ping statistics --- 3 packets transmitted, 0 packets received, 100% packet loss $ From nginx pod, we are not able to ping to nginx-manual pod. Similarly, verify the other way, ping from nginx-manual to nginx Pod (which was working earlier, that we tested already!). pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ docker ps | grep nginx 244b50fad9d4 nginx \"/docker-entrypoint.\u2026\" 55 minutes ago Up 55 minutes k8s_nginx_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0 e7117e519923 k8s.gcr.io/pause:3.6 \"/pause\" 55 minutes ago Up 55 minutes k8s_POD_nginx-manual_default_ffbeaf88-b368-48af-b181-fc30cb49406a_0 $ docker inspect 244b50fad9d4 | grep Pid \"Pid\": 42125, \"PidMode\": \"\", \"PidsLimit\": null, $ sudo nsenter -t 42125 -n ping 10.244.205.193 PING 10.244.205.193 (10.244.205.193): 56 data bytes ^C --- 10.244.205.193 ping statistics --- 2 packets transmitted, 0 packets received, 100% packet loss $ exit 0 logout pradeep@learnk8s$ With network policy applied, the communication is blocked. Let us create another network policy, this time to allow all ingress. pradeep@learnk8s$ cat allow-ingress.yaml --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-ingress spec: podSelector: {} ingress: - {} policyTypes: - Ingress pradeep@learnk8s$ kubectl create -f allow-ingress.yaml networkpolicy.networking.k8s.io/allow-all-ingress created pradeep@learnk8s$ kubectl get netpol NAME POD-SELECTOR AGE allow-all-ingress <none> 5s default-deny-ingress <none> 11m Describe the new policy. pradeep@learnk8s$ kubectl describe netpol allow-all-ingress Name: allow-all-ingress Namespace: default Created on: 2022-03-20 01:56:32 +0530 IST Labels: <none> Annotations: <none> Spec: PodSelector: <none> (Allowing the specific traffic to all pods in this namespace) Allowing ingress traffic: To Port: <any> (traffic allowed to all ports) From: <any> (traffic not restricted by source) Not affecting egress traffic Policy Types: Ingress pradeep@learnk8s$ Now that we have allowed the ingress traffic to all pods/all ports. Verify again. Now that we know the Pid of the containers, those steps are not needed. pradeep@learnk8s$ minikube ssh -n minikube-m02 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ sudo nsenter -t 11380 -n ping 10.244.151.1 PING 10.244.151.1 (10.244.151.1): 56 data bytes 64 bytes from 10.244.151.1: seq=0 ttl=62 time=3.834 ms 64 bytes from 10.244.151.1: seq=1 ttl=62 time=1.409 ms 64 bytes from 10.244.151.1: seq=2 ttl=62 time=0.723 ms ^C --- 10.244.151.1 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.723/1.988/3.834 ms $ exit 0 logout Yay! Ping is working again. Similarly, verify in the other direction. pradeep@learnk8s$ minikube ssh -n minikube-m03 _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\\| |/' _ `\\| || , < ( ) ( )| '_`\\ /'__`\\ | ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )( ___/ (_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____) $ sudo nsenter -t 42125 -n ping 10.244.205.193 PING 10.244.205.193 (10.244.205.193): 56 data bytes 64 bytes from 10.244.205.193: seq=0 ttl=62 time=1.132 ms 64 bytes from 10.244.205.193: seq=1 ttl=62 time=1.672 ms 64 bytes from 10.244.205.193: seq=2 ttl=62 time=1.695 ms ^C --- 10.244.205.193 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 1.132/1.499/1.695 ms $ exit 0 logout This completes our initial discussion on the Calico CNI. We just scratched the surface of it, there are many more features, which will be discussed in other posts.","title":"Calico"},{"location":"Kubernetes/#kubernetes-ingress-controller","text":"","title":"Kubernetes Ingress Controller"},{"location":"Kubernetes/#enable-ingress-addon","text":"pradeep@learnk8s$ minikube addons list |-----------------------------|----------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|----------|--------------|--------------------------------| | ambassador | minikube | disabled | third-party (ambassador) | | auto-pause | minikube | disabled | google | | csi-hostpath-driver | minikube | disabled | kubernetes | | dashboard | minikube | disabled | kubernetes | | default-storageclass | minikube | enabled \u2705 | kubernetes | | efk | minikube | disabled | third-party (elastic) | | freshpod | minikube | disabled | google | | gcp-auth | minikube | disabled | google | | gvisor | minikube | disabled | google | | helm-tiller | minikube | disabled | third-party (helm) | | ingress | minikube | disabled | unknown (third-party) | | ingress-dns | minikube | disabled | google | | istio | minikube | disabled | third-party (istio) | | istio-provisioner | minikube | disabled | third-party (istio) | | kong | minikube | disabled | third-party (Kong HQ) | | kubevirt | minikube | disabled | third-party (kubevirt) | | logviewer | minikube | disabled | unknown (third-party) | | metallb | minikube | disabled | third-party (metallb) | | metrics-server | minikube | disabled | kubernetes | | nvidia-driver-installer | minikube | disabled | google | | nvidia-gpu-device-plugin | minikube | disabled | third-party (nvidia) | | olm | minikube | disabled | third-party (operator | | | | | framework) | | pod-security-policy | minikube | disabled | unknown (third-party) | | portainer | minikube | disabled | portainer.io | | registry | minikube | disabled | google | | registry-aliases | minikube | disabled | unknown (third-party) | | registry-creds | minikube | disabled | third-party (upmc enterprises) | | storage-provisioner | minikube | enabled \u2705 | google | | storage-provisioner-gluster | minikube | disabled | unknown (third-party) | | volumesnapshots | minikube | disabled | kubernetes | |-----------------------------|----------|--------------|--------------------------------| pradeep@learnk8s$ Enable the ingress addon which is currently disabled in this environment. pradeep@learnk8s$ minikube addons enable ingress \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 \u25aa Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 \u25aa Using image k8s.gcr.io/ingress-nginx/controller:v1.1.1 \ud83d\udd0e Verifying ingress addon... \ud83c\udf1f The 'ingress' addon is enabled pradeep@learnk8s$ Verify the addons list again, and confirm that the ingress is enabled. pradeep@learnk8s$ minikube addons list |-----------------------------|----------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|----------|--------------|--------------------------------| | ambassador | minikube | disabled | third-party (ambassador) | | auto-pause | minikube | disabled | google | | csi-hostpath-driver | minikube | disabled | kubernetes | | dashboard | minikube | disabled | kubernetes | | default-storageclass | minikube | enabled \u2705 | kubernetes | | efk | minikube | disabled | third-party (elastic) | | freshpod | minikube | disabled | google | | gcp-auth | minikube | disabled | google | | gvisor | minikube | disabled | google | | helm-tiller | minikube | disabled | third-party (helm) | | ingress | minikube | enabled \u2705 | unknown (third-party) | | ingress-dns | minikube | disabled | google | | istio | minikube | disabled | third-party (istio) | | istio-provisioner | minikube | disabled | third-party (istio) | | kong | minikube | disabled | third-party (Kong HQ) | | kubevirt | minikube | disabled | third-party (kubevirt) | | logviewer | minikube | disabled | unknown (third-party) | | metallb | minikube | disabled | third-party (metallb) | | metrics-server | minikube | disabled | kubernetes | | nvidia-driver-installer | minikube | disabled | google | | nvidia-gpu-device-plugin | minikube | disabled | third-party (nvidia) | | olm | minikube | disabled | third-party (operator | | | | | framework) | | pod-security-policy | minikube | disabled | unknown (third-party) | | portainer | minikube | disabled | portainer.io | | registry | minikube | disabled | google | | registry-aliases | minikube | disabled | unknown (third-party) | | registry-creds | minikube | disabled | third-party (upmc enterprises) | | storage-provisioner | minikube | enabled \u2705 | google | | storage-provisioner-gluster | minikube | disabled | unknown (third-party) | | volumesnapshots | minikube | disabled | kubernetes | |-----------------------------|----------|--------------|--------------------------------| pradeep@learnk8s$ Verify all resources that got created as part of this addon installation. pradeep@learnk8s$ kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-cvwv7 0/1 Completed 0 65s ingress-nginx-admission-patch-zsrq4 0/1 Completed 1 65s ingress-nginx-controller-cc8496874-6zr4h 1/1 Running 0 65s pradeep@learnk8s$ kubectl get all -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-admission-create-cvwv7 0/1 Completed 0 83s pod/ingress-nginx-admission-patch-zsrq4 0/1 Completed 1 83s pod/ingress-nginx-controller-cc8496874-6zr4h 1/1 Running 0 83s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller NodePort 10.101.85.4 <none> 80:30419/TCP,443:30336/TCP 83s service/ingress-nginx-controller-admission ClusterIP 10.104.163.136 <none> 443/TCP 83s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ingress-nginx-controller 1/1 1 1 83s NAME DESIRED CURRENT READY AGE replicaset.apps/ingress-nginx-controller-cc8496874 1 1 1 83s NAME COMPLETIONS DURATION AGE job.batch/ingress-nginx-admission-create 1/1 10s 83s job.batch/ingress-nginx-admission-patch 1/1 11s 83s pradeep@learnk8s$","title":"Enable Ingress Addon"},{"location":"Kubernetes/#deploy-a-hello-world-app","text":"pradeep@learnk8s$ kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0 deployment.apps/web created pradeep@learnk8s$ Expose the deployment as a NodePort Service on port number 8080. pradeep@learnk8s$ kubectl expose deployment web --type=NodePort - -port=8080 service/web exposed pradeep@learnk8s$ Verify that the service is created properly. pradeep@learnk8s$ kubectl get service web NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web NodePort 10.105.47.136 <none> 8080:31270/TCP 11s pradeep@learnk8s$ Get the URL for this newly created service. pradeep@learnk8s$ minikube service web --url http://172.16.30.6:31270 pradeep@learnk8s$ Access the URL pradeep@learnk8s$ curl http://172.16.30.6:31270 Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw pradeep@learnk8s$","title":"Deploy a Hello, World App!"},{"location":"Kubernetes/#create-an-ingress","text":"pradeep@learnk8s$ cat example-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: - host: hello-world.info http: paths: - path: / pathType: Prefix backend: service: name: web port: number: 8080 pradeep@learnk8s$ Create the Ingress using the above definition. pradeep@learnk8s$kubectl apply -f example-ingress.yaml ingress.networking.k8s.io/example-ingress created pradeep@learnk8s$ Verify that the Ingress is created. pradeep@learnk8s$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE example-ingress nginx hello-world.info 172.16.30.6 80 87s pradeep@learnk8s$ Add the 172.16.30.6 hello-world.info to the bottom of the /etc/hosts file on your computer (you will need administrator access): pradeep@learnk8s$ cat /etc/hosts ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127.0.0.1 localhost 255.255.255.255 broadcasthost ::1 localhost # Added by Docker Desktop # To allow the same kube context to work on the host and the container: 127.0.0.1 kubernetes.docker.internal # End of section 172.16.30.6 hello-world.info pradeep@learnk8s$ pradeep@learnk8s$ curl hello-world.info Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw pradeep@learnk8s$","title":"Create an Ingress"},{"location":"Kubernetes/#create-a-second-deployment","text":"pradeep@learnk8s$ kubectl create deployment web2 --image=gcr.io/google-samples/hello-app:2.0 deployment.apps/web2 created pradeep@learnk8s$ Expose the deployment. pradeep@learnk8s$ kubectl expose deployment web2 --port=8080 --type=NodePort service/web2 exposed pradeep@learnk8s$ Edit the existing example-ingress.yaml manifest, and add the following lines at the end: pradeep@learnk8s$ cat example-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: - host: hello-world.info http: paths: - path: / pathType: Prefix backend: service: name: web port: number: 8080 - path: /v2 pathType: Prefix backend: service: name: web2 port: number: 8080 pradeep@learnk8s$ Apply the changes. pradeep@learnk8s$ kubectl apply -f example-ingress.yaml ingress.networking.k8s.io/example-ingress configured pradeep@learnk8s$","title":"Create a second Deployment"},{"location":"Kubernetes/#test-your-ingress","text":"pradeep@learnk8s$ curl hello-world.info Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw pradeep@learnk8s$ Access the 2nd version of the Hello World app. pradeep@learnk8s$ curl hello-world.info/v2 Hello, world! Version: 2.0.0 Hostname: web2-5858b4c7c5-krt4l pradeep@learnk8s$ pradeep@learnk8s$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE example-ingress nginx hello-world.info 172.16.30.6 80 11m pradeep@learnk8s$ Let us describe the ingress to see how it looks like. pradeep@learnk8s$ kubectl describe ingress Name: example-ingress Namespace: default Address: 172.16.30.6 Default backend: default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>) Rules: Host Path Backends ---- ---- -------- hello-world.info / web:8080 (10.244.205.195:8080) /v2 web2:8080 (10.244.151.3:8080) Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 12m (x3 over 20m) nginx-ingress-controller Scheduled for sync pradeep@learnk8s$ We can see one host ( hello-world.info ) and two paths ( / and /v2 ), two backends ( web:8080 and web2:8080 ). These two services point to the respective Pods. pradeep@learnk8s$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES web-746c8679d4-nfkcw 1/1 Running 0 75m 10.244.205.195 minikube-m02 <none> <none> web2-5858b4c7c5-krt4l 1/1 Running 0 66m 10.244.151.3 minikube-m03 <none> <none> pradeep@learnk8s$ What happens to the requests sent to a path different from both of these ( / and /v2 )? Those requests are handled by the default backend. From the above description, we can see there is a default backend default-http-backend:80 which is not defined yet, so there is an error. Let us test few undefined paths like ( /v3 , /v5 etc) pradeep@learnk8s$ curl hello-world.info/v3 Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw pradeep@learnk8s$ curl hello-world.info/v5 Hello, world! Version: 1.0.0 Hostname: web-746c8679d4-nfkcw It looks like all these undefined paths are handled by the web:8080 backend only. According to Kubernetes documentation, If defaultBackend is not set, the handling of requests that do not match any of the rules will be up to the ingress controller (consult the documentation for your ingress controller to find out how it handles this case). If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend. Let us edit the ingress, to add a defaultBackend, in this case , change it to web2 . pradeep@learnk8s$ kubectl edit ingress example-ingress ingress.networking.k8s.io/example-ingress edited Describe it again, to see the changes. pradeep@learnk8s$ kubectl describe ingress Name: example-ingress Namespace: default Address: 172.16.30.6 Default backend: web2:8080 (10.244.151.3:8080) Rules: Host Path Backends ---- ---- -------- hello-world.info / web:8080 (10.244.205.195:8080) /v2 web2:8080 (10.244.151.3:8080) Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 11s (x4 over 78m) nginx-ingress-controller Scheduled for sync pradeep@learnk8s$ This line Default backend: web2:8080 (10.244.151.3:8080) confirms our change. List out all the endpoints. pradeep@learnk8s$ kubectl get ep NAME ENDPOINTS AGE kubernetes 172.16.30.6:8443 47h web 10.244.205.195:8080 87m web2 10.244.151.3:8080 78m pradeep@learnk8s$ Let us look at the ingress controller logs. pradeep@learnk8s$ kubectl -n ingress-nginx logs ingress-nginx-controller-cc8496874-6zr4h ------------------------------------------------------------------------------- NGINX Ingress controller Release: v1.1.1 Build: a17181e43ec85534a6fea968d95d019c5a4bc8cf Repository: https://github.com/kubernetes/ingress-nginx nginx version: nginx/1.19.9 ------------------------------------------------------------------------------- W0321 15:42:29.028539 9 client_config.go:615] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. I0321 15:42:29.028983 9 main.go:223] \"Creating API client\" host=\"https://10.96.0.1:443\" I0321 15:42:29.057278 9 main.go:267] \"Running in Kubernetes cluster\" major=\"1\" minor=\"23\" git=\"v1.23.3\" state=\"clean\" commit=\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\" platform=\"linux/amd64\" I0321 15:42:29.317187 9 main.go:104] \"SSL fake certificate created\" file=\"/etc/ingress-controller/ssl/default-fake-certificate.pem\" I0321 15:42:29.374201 9 ssl.go:531] \"loading tls certificate\" path=\"/usr/local/certificates/cert\" key=\"/usr/local/certificates/key\" I0321 15:42:29.419869 9 nginx.go:255] \"Starting NGINX Ingress controller\" I0321 15:42:29.437178 9 event.go:282] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller\", UID:\"d14f8541-76a0-4357-b9ad-38c7ce8f6586\", APIVersion:\"v1\", ResourceVersion:\"12223\", FieldPath:\"\"}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller I0321 15:42:29.442938 9 event.go:282] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"tcp-services\", UID:\"9cffa0f1-6754-4bfd-bf78-6cae4febc393\", APIVersion:\"v1\", ResourceVersion:\"12224\", FieldPath:\"\"}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services I0321 15:42:29.443183 9 event.go:282] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"udp-services\", UID:\"3ac9b32a-17b9-4824-bc0b-51a609648e9c\", APIVersion:\"v1\", ResourceVersion:\"12225\", FieldPath:\"\"}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services I0321 15:42:30.622870 9 nginx.go:297] \"Starting NGINX process\" I0321 15:42:30.623750 9 leaderelection.go:248] attempting to acquire leader lease ingress-nginx/ingress-controller-leader... I0321 15:42:30.624864 9 controller.go:155] \"Configuration changes detected, backend reload required\" I0321 15:42:30.623967 9 nginx.go:317] \"Starting validation webhook\" address=\":8443\" certPath=\"/usr/local/certificates/cert\" keyPath=\"/usr/local/certificates/key\" I0321 15:42:30.666454 9 leaderelection.go:258] successfully acquired lease ingress-nginx/ingress-controller-leader I0321 15:42:30.667675 9 status.go:84] \"New leader elected\" identity=\"ingress-nginx-controller-cc8496874-6zr4h\" I0321 15:42:30.700891 9 status.go:214] \"POD is not ready\" pod=\"ingress-nginx/ingress-nginx-controller-cc8496874-6zr4h\" node=\"minikube\" I0321 15:42:30.884747 9 controller.go:172] \"Backend successfully reloaded\" I0321 15:42:30.885032 9 controller.go:183] \"Initial sync, sleeping for 1 second\" I0321 15:42:30.885722 9 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-cc8496874-6zr4h\", UID:\"de6796ef-94a6-499c-b9b5-418c39cc62c0\", APIVersion:\"v1\", ResourceVersion:\"12337\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration I0321 15:56:40.753346 9 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.412s renderingIngressLength:1 renderingIngressTime:0.009s admissionTime:18.0kBs testedConfigurationSize:0.421} I0321 15:56:40.755944 9 main.go:101] \"successfully validated configuration, accepting\" ingress=\"default/example-ingress\" I0321 15:56:40.794396 9 store.go:424] \"Found valid IngressClass\" ingress=\"default/example-ingress\" ingressclass=\"nginx\" I0321 15:56:40.796827 9 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"example-ingress\", UID:\"51471ff6-1982-47da-85ee-a9356165df9f\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"13112\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync I0321 15:56:40.802072 9 controller.go:155] \"Configuration changes detected, backend reload required\" I0321 15:56:40.979390 9 controller.go:172] \"Backend successfully reloaded\" I0321 15:56:40.988286 9 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-cc8496874-6zr4h\", UID:\"de6796ef-94a6-499c-b9b5-418c39cc62c0\", APIVersion:\"v1\", ResourceVersion:\"12337\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration I0321 15:57:28.297662 9 status.go:299] \"updating Ingress status\" namespace=\"default\" ingress=\"example-ingress\" currentValue=[] newValue=[{IP:172.16.30.6 Hostname: Ports:[]}] I0321 15:57:28.322904 9 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"example-ingress\", UID:\"51471ff6-1982-47da-85ee-a9356165df9f\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"13164\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync 172.16.30.1 - - [21/Mar/2022:16:00:09 +0000] \"GET / HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 80 0.009 [default-web-8080] [] 10.244.205.195:8080 60 0.009 200 766ea45be48fc8dcb53c425eb3800ab2 I0321 16:04:49.720933 9 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.161s renderingIngressLength:1 renderingIngressTime:0s admissionTime:21.7kBs testedConfigurationSize:0.161} I0321 16:04:49.721038 9 main.go:101] \"successfully validated configuration, accepting\" ingress=\"default/example-ingress\" I0321 16:04:49.733433 9 controller.go:155] \"Configuration changes detected, backend reload required\" I0321 16:04:49.734529 9 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"example-ingress\", UID:\"51471ff6-1982-47da-85ee-a9356165df9f\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"13649\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync I0321 16:04:49.911585 9 controller.go:172] \"Backend successfully reloaded\" I0321 16:04:49.914092 9 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-cc8496874-6zr4h\", UID:\"de6796ef-94a6-499c-b9b5-418c39cc62c0\", APIVersion:\"v1\", ResourceVersion:\"12337\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration 172.16.30.1 - - [21/Mar/2022:16:05:30 +0000] \"GET / HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 80 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 c3d48047b7ef7ac70326098b25f1e6c1 172.16.30.1 - - [21/Mar/2022:16:05:33 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.013 [default-web2-8080] [] 10.244.151.3:8080 61 0.013 200 e90cf46a8385a64ac01c834734768488 172.16.30.1 - - [21/Mar/2022:16:32:44 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.114 [default-web2-8080] [] 10.244.151.3:8080 61 0.114 200 cb87a3a8abd12099f7db64a2ec83c681 172.16.30.1 - - [21/Mar/2022:17:03:17 +0000] \"GET /v3 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.040 [default-web-8080] [] 10.244.205.195:8080 60 0.041 200 f794e2a783715ac73ff7177f60e9dd68 172.16.30.1 - - [21/Mar/2022:17:03:21 +0000] \"GET /v5 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.016 [default-web-8080] [] 10.244.205.195:8080 60 0.017 200 095cbb53ca04bdb4359fa7b745e5a809 172.16.30.1 - - [21/Mar/2022:17:03:29 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.002 [default-web2-8080] [] 10.244.151.3:8080 61 0.002 200 cbc0c654a89c5f98f18a5102ddf61b0e 172.16.30.1 - - [21/Mar/2022:17:03:33 +0000] \"GET /1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 81 0.008 [default-web-8080] [] 10.244.205.195:8080 60 0.008 200 767955b2b4eed7791912b2a843363b28 172.16.30.1 - - [21/Mar/2022:17:05:26 +0000] \"GET /1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 81 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 0ec584df780661bccfa10731bcc70524 172.16.30.1 - - [21/Mar/2022:17:05:29 +0000] \"GET / HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 80 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 5ecb91527d77741daf16c17a8d79a462 172.16.30.1 - - [21/Mar/2022:17:05:31 +0000] \"GET /as HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.006 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 034643529f3104fef19915593168b200 I0321 17:14:53.079003 9 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.41s renderingIngressLength:1 renderingIngressTime:0.007s admissionTime:21.7kBs testedConfigurationSize:0.417} I0321 17:14:53.079746 9 main.go:101] \"successfully validated configuration, accepting\" ingress=\"default/example-ingress\" I0321 17:14:53.099157 9 controller.go:155] \"Configuration changes detected, backend reload required\" I0321 17:14:53.099689 9 event.go:282] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"example-ingress\", UID:\"51471ff6-1982-47da-85ee-a9356165df9f\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"15050\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync I0321 17:14:53.225350 9 controller.go:172] \"Backend successfully reloaded\" I0321 17:14:53.227604 9 event.go:282] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-cc8496874-6zr4h\", UID:\"de6796ef-94a6-499c-b9b5-418c39cc62c0\", APIVersion:\"v1\", ResourceVersion:\"12337\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration 172.16.30.1 - - [21/Mar/2022:17:16:53 +0000] \"GET /v5 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.009 [default-web-8080] [] 10.244.205.195:8080 60 0.009 200 2e8ee25ad012ea3d5ac6fb4646f37d61 172.16.30.1 - - [21/Mar/2022:17:16:57 +0000] \"GET /v3 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 70d9d27891c2b97a6bcda5e315fa3691 172.16.30.1 - - [21/Mar/2022:17:17:01 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.010 [default-web2-8080] [] 10.244.151.3:8080 61 0.009 200 297effa7825bef53d93e6dce4e5d9d62 172.16.30.1 - - [21/Mar/2022:17:17:02 +0000] \"GET /v1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 421c9cb655e07183d9a7d1a89562db97 172.16.30.1 - - [21/Mar/2022:17:17:29 +0000] \"GET /v1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 3bff305b8c1b782d08acf2de386076d9 172.16.30.1 - - [21/Mar/2022:17:17:31 +0000] \"GET /v3 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.001 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 e3dca522b7470345325f5dcda8adfc8e 172.16.30.1 - - [21/Mar/2022:17:17:34 +0000] \"GET /v5 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 207b1c9f4b52683061bb0807857c32f6 172.16.30.1 - - [21/Mar/2022:17:17:36 +0000] \"GET /v6 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 f2d0ae4264c26168246099ee90cc9f27 172.16.30.1 - - [21/Mar/2022:17:17:39 +0000] \"GET /vdfs HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.001 200 b08773ef6c074b4b10fee5626457df47 172.16.30.1 - - [21/Mar/2022:17:17:40 +0000] \"GET /vdfs HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 cd5a39f9d32b212bf97b41c57f7b429b 172.16.30.1 - - [21/Mar/2022:17:18:58 +0000] \"GET /vdfs HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.009 [default-web-8080] [] 10.244.205.195:8080 60 0.008 200 f413dd82a31c72df36114843c5054aad 172.16.30.1 - - [21/Mar/2022:17:18:59 +0000] \"GET /vdfs HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 09a92db2773acffcd23b31c856673a9b 172.16.30.1 - - [21/Mar/2022:17:19:12 +0000] \"GET /v1 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.001 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 c21e9af32944af9d9d70b39eaedbe11c 172.16.30.1 - - [21/Mar/2022:17:19:14 +0000] \"GET /v HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 81 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 5daf84fe1f8992ff8d9d0cdec4b751c1 172.16.30.1 - - [21/Mar/2022:17:19:17 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.007 [default-web2-8080] [] 10.244.151.3:8080 61 0.008 200 52df2d3780d65d11e34a545583958a17 172.16.30.1 - - [21/Mar/2022:17:20:32 +0000] \"GET /v2 HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 82 0.003 [default-web2-8080] [] 10.244.151.3:8080 61 0.003 200 d05940e016c2b36ce79996c04065b5a8 172.16.30.1 - - [21/Mar/2022:17:20:34 +0000] \"GET /v2f HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 83 0.003 [default-web2-8080] [] 10.244.151.3:8080 61 0.002 200 c59de52e577935709573bf0515e999e2 172.16.30.1 - - [21/Mar/2022:17:20:36 +0000] \"GET /v3 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 3354dc569894182a6ee5a5f69892ca66 172.16.30.1 - - [21/Mar/2022:17:20:40 +0000] \"GET /v35 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 83 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 75cccfe798f3a6f2ee072c9233c8e3b7 172.16.30.1 - - [21/Mar/2022:17:20:44 +0000] \"GET /v2a HTTP/1.1\" 200 61 \"-\" \"curl/7.77.0\" 83 0.004 [default-web2-8080] [] 10.244.151.3:8080 61 0.004 200 8ae6170f4a95446d0615bfc5d4a16403 172.16.30.1 - - [21/Mar/2022:17:20:48 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 c4cea1304f930170d7a7148fc2ac04a7 172.16.30.1 - - [21/Mar/2022:17:21:49 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 b860ca473d1b622514dd4ebf194311f2 172.16.30.1 - - [21/Mar/2022:17:21:50 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 1f8bf150a942fb5272d01d9a82c6fd77 172.16.30.1 - - [21/Mar/2022:17:21:51 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 2a2c3eec3ebe85dd546ea35cf92f1357 172.16.30.1 - - [21/Mar/2022:17:21:52 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.005 [default-web-8080] [] 10.244.205.195:8080 60 0.005 200 fca07770b107dfb25126f60a1f4dabcc 172.16.30.1 - - [21/Mar/2022:17:21:52 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 421ddaffd339aafd3ac912c195035b7e 172.16.30.1 - - [21/Mar/2022:17:22:41 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 58a2dc6300d892b640bd26ab007daba8 172.16.30.1 - - [21/Mar/2022:17:22:42 +0000] \"GET /dg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 402a0a5fd8037182ea8671d3d9347ee7 172.16.30.1 - - [21/Mar/2022:17:22:44 +0000] \"GET /df HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 4cb6f78cb734373510f98f669cdf253d 172.16.30.1 - - [21/Mar/2022:17:22:46 +0000] \"GET /dfdg HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 84 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.001 200 2fd8c843e0fac657bc80e2a6b23dcaf6 172.16.30.1 - - [21/Mar/2022:17:23:52 +0000] \"GET /v7 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 5aaabd3963132e95499b5be8b57368e6 172.16.30.1 - - [21/Mar/2022:17:23:54 +0000] \"GET /v9 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.002 [default-web-8080] [] 10.244.205.195:8080 60 0.002 200 452ce52c0ab19f2fdc37152f925887f7 172.16.30.1 - - [21/Mar/2022:17:24:28 +0000] \"GET /v9 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.004 200 faf0e7f8be1b66b38f6747c7df815525 172.16.30.1 - - [21/Mar/2022:17:24:30 +0000] \"GET /v9 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.004 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 c4526f5d5a9fab7245b1d77becf24bb7 172.16.30.1 - - [21/Mar/2022:17:24:31 +0000] \"GET /v9 HTTP/1.1\" 200 60 \"-\" \"curl/7.77.0\" 82 0.003 [default-web-8080] [] 10.244.205.195:8080 60 0.003 200 47fd2dd3ed81e5fa3762687343ed06c0 pradeep@learnk8s$","title":"Test your Ingress"},{"location":"Kubernetes/#kubernetes-dns","text":"When we setup our minikube cluster, it is configured to use the CoreDNS addon or its precursor, kube-dns. We can verify the same. pradeep@learnk8s$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-8594699699-dztlm 1/1 Running 2 (120m ago) 47h calico-node-gqvw6 1/1 Running 2 (119m ago) 47h calico-node-qdbcf 1/1 Running 1 (120m ago) 47h calico-node-sw74l 1/1 Running 1 (118m ago) 47h coredns-64897985d-58btq 1/1 Running 2 (120m ago) 47h etcd-minikube 1/1 Running 1 (120m ago) 47h kube-apiserver-minikube 1/1 Running 3 (120m ago) 47h kube-controller-manager-minikube 1/1 Running 3 (120m ago) 47h kube-proxy-7k4lb 1/1 Running 1 (118m ago) 47h kube-proxy-gm2dh 1/1 Running 1 (119m ago) 47h kube-proxy-hvkqd 1/1 Running 1 (120m ago) 47h kube-scheduler-minikube 1/1 Running 2 (120m ago) 47h storage-provisioner 1/1 Running 10 (107m ago) 47h Describe the coredns pod, to get additional information. pradeep@learnk8s$ kubectl describe pods coredns-64897985d-58btq -n kube-system Name: coredns-64897985d-58btq Namespace: kube-system Priority: 2000000000 Priority Class Name: system-cluster-critical Node: minikube/172.16.30.6 Start Time: Sat, 19 Mar 2022 23:48:56 +0530 Labels: k8s-app=kube-dns pod-template-hash=64897985d Annotations: <none> Status: Running IP: 10.88.0.4 IPs: IP: 10.88.0.4 Controlled By: ReplicaSet/coredns-64897985d Containers: coredns: Container ID: docker://c866d7b12f5f086407e04f3fd4e500889f860bfd55790e1e2a4870fad1052330 Image: k8s.gcr.io/coredns/coredns:v1.8.6 Image ID: docker-pullable://k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e Ports: 53/UDP, 53/TCP, 9153/TCP Host Ports: 0/UDP, 0/TCP, 0/TCP Args: -conf /etc/coredns/Corefile State: Running Started: Mon, 21 Mar 2022 21:07:03 +0530 Last State: Terminated Reason: Error Exit Code: 255 Started: Sun, 20 Mar 2022 19:25:02 +0530 Finished: Mon, 21 Mar 2022 21:06:40 +0530 Ready: True Restart Count: 2 Limits: memory: 170Mi Requests: cpu: 100m memory: 70Mi Liveness: http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5 Readiness: http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3 Environment: <none> Mounts: /etc/coredns from config-volume (ro) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rhcln (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: config-volume: Type: ConfigMap (a volume populated by a ConfigMap) Name: coredns Optional: false kube-api-access-rhcln: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: Burstable Node-Selectors: kubernetes.io/os=linux Tolerations: CriticalAddonsOnly op=Exists node-role.kubernetes.io/control-plane:NoSchedule node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 107m kubelet Liveness probe failed: Get \"http://10.88.0.4:8080/health\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 107m (x2 over 107m) kubelet Readiness probe failed: Get \"http://10.88.0.4:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) pradeep@learnk8s$","title":"Kubernetes DNS"},{"location":"Kubernetes/#create-a-simple-pod-to-use-as-a-test-environment","text":"pradeep@learnk8s$ cat dnsutils.yaml apiVersion: v1 kind: Pod metadata: name: dnsutils namespace: default spec: containers: - name: dnsutils image: k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always pradeep@learnk8s$ pradeep@learnk8s$ kubectl create -f dnsutils.yaml pod/dnsutils created pradeep@learnk8s$ Once that Pod is running, you can exec nslookup in that environment. pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup kubernetes.default ;; connection timed out; no servers could be reached command terminated with exit code 1 pradeep@learnk8s$ As the nslookup command failed, check the following: pradeep@learnk8s$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 47h web NodePort 10.105.47.136 <none> 8080:31270/TCP 111m web2 NodePort 10.106.73.151 <none> 8080:31691/TCP 102m kubernetes service is running the default namespace. Check the kube-dns service in the kube-system namespace. pradeep@learnk8s$ kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 47h pradeep@learnk8s$ It is running fine. Let us check what nameserver is being used by this Pod. pradeep@learnk8s$ kubectl exec -ti dnsutils -- cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 This confirms that the dnsutils pod is using the correct nameserver. Are DNS endpoints exposed? You can verify that DNS endpoints are exposed by using the kubectl get endpoints command. pradeep@learnk8s$ kubectl get ep kube-dns -n kube-system NAME ENDPOINTS AGE kube-dns 10.88.0.4:53,10.88.0.4:53,10.88.0.4:9153 47h pradeep@learnk8s$ Hmm, the endpoints are coming from the Podman CNI but not from the Calico CNI that we are using in this cluster. You remember the race condition issue between Calico and Podman CNIs?! Let us get the replicaset ID for the coredns service and delete it. pradeep@learnk8s$ kubectl get rs -n kube-system NAME DESIRED CURRENT READY AGE calico-kube-controllers-8594699699 1 1 1 47h coredns-64897985d 1 1 1 47h Deleting the replicaset pradeep@learnk8s$ kubectl delete rs coredns-64897985d -n kube-system replicaset.apps \"coredns-64897985d\" deleted pradeep@learnk8s$ Immediately, the replicaset got re-created. Check the AGE! pradeep@learnk8s$ kubectl get rs -n kube-system NAME DESIRED CURRENT READY AGE calico-kube-controllers-8594699699 1 1 1 47h coredns-64897985d 1 1 0 2s pradeep@learnk8s$ Let us check the endpoints again. pradeep@learnk8s$ kubectl get ep kube-dns -n kube-system NAME ENDPOINTS AGE kube-dns 10.244.205.196:53,10.244.205.196:53,10.244.205.196:9153 47h pradeep@learnk8s$ Now that the endpoints are correctly pointing to the Calico CNI IP addresses, verify the nslookup again. pradeep@learnk8s$ kubectl exec -ti dnsutils -- cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup kubernetes.default Server: 10.96.0.10 Address: 10.96.0.10#53 Name: kubernetes.default.svc.cluster.local Address: 10.96.0.1 pradeep@learnk8s$ Just to verify the coredns status. pradeep@learnk8s$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-8594699699-dztlm 1/1 Running 2 (132m ago) 47h calico-node-gqvw6 1/1 Running 2 (131m ago) 47h calico-node-qdbcf 1/1 Running 1 (132m ago) 47h calico-node-sw74l 1/1 Running 1 (130m ago) 47h coredns-64897985d-kwp95 1/1 Running 0 74s etcd-minikube 1/1 Running 1 (132m ago) 47h kube-apiserver-minikube 1/1 Running 3 (132m ago) 47h kube-controller-manager-minikube 1/1 Running 3 (132m ago) 47h kube-proxy-7k4lb 1/1 Running 1 (130m ago) 47h kube-proxy-gm2dh 1/1 Running 1 (131m ago) 47h kube-proxy-hvkqd 1/1 Running 1 (132m ago) 47h kube-scheduler-minikube 1/1 Running 2 (132m ago) 47h storage-provisioner 1/1 Running 10 (118m ago) 47h pradeep@learnk8s$ Are DNS queries being received/processed? You can verify if queries are being received by CoreDNS by adding the log plugin to the CoreDNS configuration (aka Corefile). The CoreDNS Corefile is held in a ConfigMap named coredns . pradeep@learnk8s$ kubectl get cm -n kube-system NAME DATA AGE calico-config 4 47h coredns 1 47h extension-apiserver-authentication 6 47h kube-proxy 2 47h kube-root-ca.crt 1 47h kubeadm-config 1 47h kubelet-config-1.23 1 47h pradeep@learnk8s$ Describe the config map. pradeep@learnk8s$ kubectl describe cm coredns -n kube-system Name: coredns Namespace: kube-system Labels: <none> Annotations: <none> Data ==== Corefile: ---- .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 hosts { 192.168.64.1 host.minikube.internal fallthrough } forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } BinaryData ==== Events: <none> pradeep@learnk8s$ Edit this configmap and add the log plugin. pradeep@learnk8s$ kubectl edit cm coredns -n kube-system configmap/coredns edited pradeep@learnk8s$ Describe it again and verify the change pradeep@learnk8s$ kubectl describe cm coredns -n kube-system Name: coredns Namespace: kube-system Labels: <none> Annotations: <none> Data ==== Corefile: ---- .:53 { log errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 hosts { 192.168.64.1 host.minikube.internal fallthrough } forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } BinaryData ==== Events: <none> pradeep@learnk8s$ After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods. Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs. pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95 .:53 [INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34 CoreDNS-1.8.6 linux/amd64, go1.17.1, 13a9191 [INFO] Reloading [INFO] plugin/health: Going into lameduck mode for 5s pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup kubernetes.default Server: 10.96.0.10 Address: 10.96.0.10#53 Name: kubernetes.default.svc.cluster.local Address: 10.96.0.1 pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95 .:53 [INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34 CoreDNS-1.8.6 linux/amd64, go1.17.1, 13a9191 [INFO] Reloading [INFO] plugin/health: Going into lameduck mode for 5s [INFO] plugin/reload: Running configuration MD5 = 07d7c5ad4525bf2c472eaef020d0184d [INFO] Reloading complete [INFO] 127.0.0.1:39552 - 1628 \"HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512\" NXDOMAIN qr,rd,ra 132 1.058936054s [INFO] 10.244.151.4:41947 - 42256 \"A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512\" NXDOMAIN qr,aa,rd 155 0.000469847s [INFO] 10.244.151.4:55247 - 43436 \"A IN kubernetes.default.svc.cluster.local. udp 54 false 512\" NOERROR qr,aa,rd 106 0.000357148s pradeep@learnk8s$ Let us make some more queries, pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup web.default Server: 10.96.0.10 Address: 10.96.0.10#53 Name: web.default.svc.cluster.local Address: 10.105.47.136 pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup web2.default Server: 10.96.0.10 Address: 10.96.0.10#53 Name: web2.default.svc.cluster.local Address: 10.106.73.151 pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup web3.default Server: 10.96.0.10 Address: 10.96.0.10#53 ** server can't find web3.default: NXDOMAIN command terminated with exit code 1 pradeep@learnk8s$ All the queries worked as expected. We did not have any service named web3 in the default namespace, so it failed. Let us review the coredns logs again. pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95 .:53 [INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34 CoreDNS-1.8.6 linux/amd64, go1.17.1, 13a9191 [INFO] Reloading [INFO] plugin/health: Going into lameduck mode for 5s [INFO] plugin/reload: Running configuration MD5 = 07d7c5ad4525bf2c472eaef020d0184d [INFO] Reloading complete [INFO] 127.0.0.1:39552 - 1628 \"HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512\" NXDOMAIN qr,rd,ra 132 1.058936054s [INFO] 10.244.151.4:41947 - 42256 \"A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512\" NXDOMAIN qr,aa,rd 155 0.000469847s [INFO] 10.244.151.4:55247 - 43436 \"A IN kubernetes.default.svc.cluster.local. udp 54 false 512\" NOERROR qr,aa,rd 106 0.000357148s [INFO] 10.244.151.4:46146 - 25028 \"A IN web.default.default.svc.cluster.local. udp 55 false 512\" NXDOMAIN qr,aa,rd 148 0.000302078s [INFO] 10.244.151.4:43513 - 15523 \"A IN web.default.svc.cluster.local. udp 47 false 512\" NOERROR qr,aa,rd 92 0.000412597s [INFO] 10.244.151.4:41650 - 22470 \"A IN web2.default.default.svc.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000398918s [INFO] 10.244.151.4:33214 - 54758 \"A IN web2.default.svc.cluster.local. udp 48 false 512\" NOERROR qr,aa,rd 94 0.00031088s [INFO] 10.244.151.4:34263 - 58079 \"A IN web3.default.default.svc.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000433005s [INFO] 10.244.151.4:52587 - 56008 \"A IN web3.default.svc.cluster.local. udp 48 false 512\" NXDOMAIN qr,aa,rd 141 0.000420513s [INFO] 10.244.151.4:49172 - 35844 \"A IN web3.default.cluster.local. udp 44 false 512\" NXDOMAIN qr,aa,rd 137 0.000302094s [INFO] 10.244.151.4:44833 - 3203 \"A IN web3.default. udp 30 false 512\" NXDOMAIN qr,rd,ra 105 0.18944251s pradeep@learnk8s$ Are you in the right namespace for the service? DNS queries that don't specify a namespace are limited to the pod's namespace. If the namespace of the pod and service differ, the DNS query must include the namespace of the service. This query is limited to the pod's namespace: pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup ingress-nginx-controller Server: 10.96.0.10 Address: 10.96.0.10#53 ** server can't find ingress-nginx-controller: NXDOMAIN command terminated with exit code 1 pradeep@learnk8s$ This failed because, ingress-nginx-controller svc is not present in the default namespace. pradeep@learnk8s$ kubectl -n kube-system logs coredns-64897985d-kwp95 .:53 [INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34 CoreDNS-1.8.6 linux/amd64, go1.17.1, 13a9191 [INFO] Reloading [INFO] plugin/health: Going into lameduck mode for 5s [INFO] plugin/reload: Running configuration MD5 = 07d7c5ad4525bf2c472eaef020d0184d [INFO] Reloading complete [INFO] 127.0.0.1:39552 - 1628 \"HINFO IN 1824978633801001221.7972855182251791512. udp 57 false 512\" NXDOMAIN qr,rd,ra 132 1.058936054s [INFO] 10.244.151.4:41947 - 42256 \"A IN kubernetes.default.default.svc.cluster.local. udp 62 false 512\" NXDOMAIN qr,aa,rd 155 0.000469847s [INFO] 10.244.151.4:55247 - 43436 \"A IN kubernetes.default.svc.cluster.local. udp 54 false 512\" NOERROR qr,aa,rd 106 0.000357148s [INFO] 10.244.151.4:46146 - 25028 \"A IN web.default.default.svc.cluster.local. udp 55 false 512\" NXDOMAIN qr,aa,rd 148 0.000302078s [INFO] 10.244.151.4:43513 - 15523 \"A IN web.default.svc.cluster.local. udp 47 false 512\" NOERROR qr,aa,rd 92 0.000412597s [INFO] 10.244.151.4:41650 - 22470 \"A IN web2.default.default.svc.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000398918s [INFO] 10.244.151.4:33214 - 54758 \"A IN web2.default.svc.cluster.local. udp 48 false 512\" NOERROR qr,aa,rd 94 0.00031088s [INFO] 10.244.151.4:34263 - 58079 \"A IN web3.default.default.svc.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000433005s [INFO] 10.244.151.4:52587 - 56008 \"A IN web3.default.svc.cluster.local. udp 48 false 512\" NXDOMAIN qr,aa,rd 141 0.000420513s [INFO] 10.244.151.4:49172 - 35844 \"A IN web3.default.cluster.local. udp 44 false 512\" NXDOMAIN qr,aa,rd 137 0.000302094s [INFO] 10.244.151.4:44833 - 3203 \"A IN web3.default. udp 30 false 512\" NXDOMAIN qr,rd,ra 105 0.18944251s [INFO] 10.244.151.4:50251 - 50181 \"A IN ingress-nginx-controller.default.svc.cluster.local. udp 68 false 512\" NXDOMAIN qr,aa,rd 161 0.000381672s [INFO] 10.244.151.4:52033 - 12007 \"A IN ingress-nginx-controller.svc.cluster.local. udp 60 false 512\" NXDOMAIN qr,aa,rd 153 0.000844354s [INFO] 10.244.151.4:50003 - 30556 \"A IN ingress-nginx-controller.cluster.local. udp 56 false 512\" NXDOMAIN qr,aa,rd 149 0.000453492s [INFO] 10.244.151.4:41810 - 19431 \"A IN ingress-nginx-controller. udp 42 false 512\" NXDOMAIN qr,rd,ra 117 0.057777871s pradeep@learnk8s$ We can see that the query is done with the default namespace: ingress-nginx-controller.default.svc.cluster.local. Let us list all active services in all namespaces. pradeep@learnk8s$ kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 47h default web NodePort 10.105.47.136 <none> 8080:31270/TCP 137m default web2 NodePort 10.106.73.151 <none> 8080:31691/TCP 127m ingress-nginx ingress-nginx-controller NodePort 10.101.85.4 <none> 80:30419/TCP,443:30336/TCP 147m ingress-nginx ingress-nginx-controller-admission ClusterIP 10.104.163.136 <none> 443/TCP 147m kube-system kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 47h pradeep@learnk8s$ Now that we see ingress-nginx-controller in the ingress-nginx namespace, let us query for the correct name. pradeep@learnk8s$ kubectl exec -i -t dnsutils -- nslookup ingress-nginx-controller.ingress-nginx Server: 10.96.0.10 Address: 10.96.0.10#53 Name: ingress-nginx-controller.ingress-nginx.svc.cluster.local Address: 10.101.85.4 We can see the name resolution is successful this time. :end: Thank you for reading my :book: :warning: Warning: Do not push the big red button. :memo: Note: Sunrises are beautiful. :bulb: Tip: Remember to appreciate the little things in life. :building_construction: :building_construction::building_construction: Learning continues and new topics will be updated here regularly. Keep learning :rocket: Best regards, Pradeep","title":"Create a simple Pod to use as a test environment"},{"location":"about/","text":"Welcome Welcome to my blog! Thanks for reading. Best regards, Pradeep","title":"About"},{"location":"about/#welcome","text":"Welcome to my blog! Thanks for reading. Best regards, Pradeep","title":"Welcome"}]}